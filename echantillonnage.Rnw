\chapter{Distributions d'échantillonnage}
\label{chap:C}

\Opensolutionfile{reponses}[reponses-echantillon]
\Opensolutionfile{solutions}[solutions-echantillon]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:C}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:C}}

\end{Filesave}

<<echo=FALSE>>=
options(width = 55)
@


%%%
%%% Début des exercices
%%%

% concept de statistique et distribution d'échantillonnage

% Distributions liées à la loi normale

% moyenne avec échantillon normal
\begin{exercice}
  Soit $\bar{X}_{5}$ la moyenne d'un échantillon de taille $5$ tiré d'une
  distribution normale avec moyenne $10$ et variance $125$. Trouver la constante $c$
  telle que $\prob{\bar{X}_{5} < c} = 0,90$.
  \begin{rep}
    $16,41$
  \end{rep}
  \begin{sol}
    On sait que $\bar{X}_{5} \sim \mathcal{N}(\esp{\bar{X}_{5}}, \var{\bar{X}_{5}})$ avec
      $\esp{\bar{X}_{5}} = \esp{X} = 10$ et $\var{\bar{X}_{5}} = \var{X}/n =
      125/5 = 5$. Par conséquent,
    \begin{align*}
      \prob{\bar{X}_{5} < c}
      &= \Prob{\frac{\bar{X}_{5} - 10}{\sqrt{25}} < %
        \frac{c - 10}{\sqrt{25}}} \\
      &= \prob{Z < z_\alpha} \\
      &= 1 - \alpha
    \end{align*}
    avec $Z \sim \mathcal{N}(0, 1)$ et $z_\alpha = (c - 10)/5$. Ici, on a $1 -
    \alpha = 0,90$. On trouve dans une table de quantiles de la loi
    normale que $z_{0,10} = 1,282$, d'où $c = 16,41$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Si $\bar{X}_n$ est la moyenne d'un échantillon de taille $n$ tiré
  d'une distribution normale de moyenne $\mu$ et de variance $100$,
  trouver la valeur de $n$ telle que $\Prob{\mu - 5 < \bar{X}_n < \mu +
    5} = 0,954$.
  \begin{rep}
    $16$
  \end{rep}
  \begin{sol}
    On a $\esp{\bar{X}_n} = \esp{X} = \mu$, $\var{\bar{X}_n} = \var{X}/n =
    100/n$ et $\bar{X}_n\sim \mathcal{N}(\mu, 100/n)$. Ainsi, on cherche $n$ tel
    que
    \begin{align*}
      \prob{\mu - 5 < \bar{X}_n< \mu + 5}
      &= \Prob{-\frac{5}{10/\sqrt{n}} < %
        \frac{\bar{X}_n- \mu}{10/\sqrt{n}} < %
        \frac{5}{10/\sqrt{n}}} \\
      &= \Phi\left( \frac{5 \sqrt{n}}{10} \right)
      - \Phi\left( -\frac{5 \sqrt{n}}{10} \right) \\
      &= 2 \Phi\left( \frac{5\sqrt{n}}{10} \right) - 1 \\
      &= 0,954,
    \end{align*}
    soit
    \begin{equation*}
     \Phi \left( \frac{5\sqrt{n}}{10} \right) = 0,977.
    \end{equation*}
    On trouve dans une table de loi normale que $5 \sqrt{n}/10 = 2$,
    d'où $n = 16$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_{25}$ un échantillon aléatoire issu d'une
  distribution $\mathcal{N}(0, 16)$ et $Y_1, \dots, Y_{25}$ un échantillon
  aléatoire issu d'une distribution $\mathcal{N}(1, 9)$. Les deux échantillons
  sont indépendants. Soient $\bar{X}_{25}$ et $\bar{Y}_{25}$ les moyennes des
  deux échantillons. Calculer la probabilité $\prob{\bar{X}_{25} > \bar{Y}_{25}}$.
  \begin{rep}
    $0,159$
  \end{rep}
  \begin{sol}
    On a $\bar{X}_{25} \sim \mathcal{N}(0, 16/25)$, $\bar{Y}_{25} \sim \mathcal{N}(1, 9/25)$ et, par
    conséquent, $\bar{X}_{25} - \bar{Y}_{25} \sim \mathcal{N}(-1, 1)$. On a donc
    \begin{align*}
      \prob{\bar{X}_{25} > \bar{Y}_{25}}
      &= \prob{\bar{X}_{25} - \bar{Y}_{25} > 0} \\
      &= \Prob{\frac{\bar{X}_{25}-\bar{Y}_{25} - (-1)}{\sqrt{1}} > %
        \frac{0 - (-1)}{\sqrt{1}}} \\
      &= 1 - \Phi(1)\\
      &= 0,159.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice} 
Soit $Y_1,\ldots,Y_8$ un échantillon aléatoire issu d'une distribution $\mathcal{N}(0,1)$ et 
$$
\bar Y= (Y_1+\cdots+Y_7)/7.
$$ 
Quelle est la distribution des statistiques suivantes? Justifiez vos réponses.
%Let $Y_1,\ldots,Y_8$ be a random sample from a $\mathcal{N}(0,1)$ and $\bar Y= (Y_1+\cdots+Y_7)/7$. What is the distribution of the following statistics? (Justify your answers.)

\begin{enumerate}
\item $W = \sum_{i=1}^7 Y_i^2$ 
\item $U = \sum_{i=1}^7 (Y_i-\bar Y)^2$ 
\item $Y_8^2+U$
\item $\sqrt{7} Y_8/\sqrt{W}$
\item $\sqrt{6}Y_8/\sqrt{U}$
\item $3(7\bar Y^2+Y_8^2)/U$
\end{enumerate}
\begin{sol}
\begin{enumerate}
\item La statistique $W = Y_1^2 + \cdots + Y^2_7$ suit une loi khi carré avec $7$ degrés de liberté, $\chi^2_{(7)}$, puisque $W$ est une somme de sept variables \textbf{indépendantes}, chacune d'entre elle étant le carré d'une variable aléatoire normale centrée réduite.

\item Si $Y_1, \ldots , Y_n$ forme un échantillon aléatoire tiré d'une $\mathcal{N}(\mu, \sigma^2)$, alors
$$
\frac{(n-1)S^2_n}{\sigma^2} = \frac{1}{\sigma^2} \sum_{i=1}^n (Y_i - {\bar Y}_n)^2
$$
a une distribution khi carré avec $n-1$ degrés de liberté, $\chi^2_{(n-1)}$. Si on pose $n=7$ et $\sigma^2 =1$, on trouve que
$$
U = \sum_{i=1}^7 (Y_i - {\bar Y})^2 \sim \chi^2_{(6)}.
$$

\item Selon (b), $U$ a une distribution $\chi^2_{(6)}$. Cela signifie que la somme a la même distribution que $Z_1^2 + \cdots + Z_6^2$, où $Z_1, \ldots , Z_6$ sont iid $\mathcal{N}(0,1)$ et indépendantes de $Y_8$, qui suit aussi une $\mathcal{N}(0,1)$. Ainsi, 
$$
Y_8^2 +U \sim \chi^2_{(7)}.
$$

\item 
D'abord, on observe que $\sqrt{7} \, Y_8 /\sqrt{W} = Y_8/\sqrt{W/7}$, où $Y_8 \sim \mathcal{N}(0,1)$ et $W \sim \chi^2_{(7)}$ selon a). De plus, $W$ et $Y_8$ sont des variables aléatoires indépendantes. Ainsi, $Y_8/\sqrt{W/7}$ suit une loi Student $t_{(7)}$.

\item On note que $U \sim \chi^2_{(6)}$ selon b). Ensuite, $U$ est indépendant de $Y_8 \sim \mathcal{N}(0,1)$. Ainsi,
$$
\frac{\sqrt{6}Y_8}{\sqrt{U}} = \frac{Y_8}{\sqrt{U/6}} \sim t_{(6)}.
$$

\item On sait que la moyenne échantillonnale ${\bar Y}$ est indépendante de la variance échantillonnale $U/6$. Donc, ${\bar Y}$ et $Y_8$ sont toutes deux indépendantes de $U$. Puisque ${\bar Y} \sim \mathcal{N}(0,1/7)$, on a $\sqrt{7} \, {\bar Y} \sim \mathcal{N}(0,1)$ et ainsi
$$
7{\bar Y}^2 + Y^2_8 = (\sqrt{7} \, {\bar Y})^2 + Y^2_8 \sim \chi^2_{(2)}.
$$
De plus, $U \sim \chi^2_{(6)}$ avec (b). Il en découle que
$$
\frac{3(7{\bar Y}^2 + Y^2_8)}{U} = \frac{\displaystyle \frac{(\sqrt{7} \, {\bar Y})^2+Y^2_8}{2}}{U/6} \sim \mathcal{F}_{(2,6)}.
$$
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire d'une loi normale de moyenne $\mu$ et variance~$\sigma^2$. Trouver la moyenne et la variance de la statistique
  \begin{displaymath}
    S^2_n = \frac{1}{(n-1)}\sum_{i=1}^n (X_i - \bar{X})^2.
  \end{displaymath}

  \begin{rep}
    $\esp{S^2_n} = \sigma^2$, $\var{S^2_n} = 2 \sigma^4/(n-1)$
  \end{rep}
  \begin{sol}
    On sait que $(n-1)S^2_n/\sigma^2 \sim \chi^2(n - 1)$, que l'espérance
    d'une loi $\chi^2$ est égale à son nombre de degrés de liberté et
    que sa variance est égale à deux fois son nombre de degrés de
    liberté. On a donc
    \begin{align*}
      \esp{S^2_n}
      &= \frac{\sigma^2}{(n-1)}\, \Esp{\frac{(n-1)S^2_n}{\sigma^2}}\\\
      &= \frac{(n - 1) \sigma^2}{(n-1)}=\sigma^2 \\
      \intertext{et}
      \var{S^2_n}
      &= \frac{\sigma^4}{(n-1)^2}\, \Var{\frac{(n-1)S^2_n}{\sigma^2}}\\
      &= \frac{2 (n - 1) \sigma^4}{(n-1)^2}= \frac{2 \sigma^4}{(n-1)}
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $S^2_6$ la variance d'un échantillon de taille $6$ d'une
  distribution normale de moyenne $\mu$ et de variance $10$. Calculer
  $\prob{2,30 < S^2_6 < 22,2}$.
  \begin{rep}
    $0,90$
  \end{rep}
  \begin{sol}
    On sait que $5S^2_6/\sigma^2 \sim \chi^2(5)$. Soit $Y \sim
    \chi^2(5)$. On a donc,
    \begin{align*}
      \prob{2,30 < S^2_6 < 22,2}
      &= \Prob{\frac{5 (2,30)}{10} <
        \frac{5 S^2_6}{10} <
        \frac{5 (22,2)}{10}} \\
      &= \prob{1,15 < Y < 11,1}\\
      &= \prob{Y < 11,1} - \prob{Y <  1,15}.
    \end{align*}
    On trouve dans une table de quantiles de la loi khi~carré (ou avec
    la fonction \texttt{qchisq} dans \textsf{R}, par exemple) que
    $\prob{Y < 11,1} = 0,95$ et $\prob{Y < 1,15} = 0,05$. Par
    conséquent, $\prob{2,30 < S^2_6 < 22,2} = 0,90$.
  \end{sol}
\end{exercice}

\begin{exercice} 
Trouvez la fonction de densité de probabilité de $S_n^2$, la variance échantillonale d'un échantillon de taille $n$ d'une distribution $\mathcal{N}(0,\sigma^2)$. Est-ce que la distribution échantillonale de $S_n^2$ fait partie d'une famille de distributions connues~? Utilisez ce résultat pour trouver la moyenne et la variance de $S_n^2$. [\emph{Truc: Considérez $(n-1)S_n^2/\sigma^2\sim \chi^2_{(n-1)}$ et utilisez la méthode de la fonction de répartition pour obtenir la densité de la variable aléatoire transformée.}]
\begin{sol}
On sait que $W=(n-1)S_n^2/\sigma^2\sim \chi^2_{(n-1)}$, et on cherche à trouver la fonction de densité de $S_n^2=\sigma^2 W /(n-1)$. Puisque c'est une transformation d'échelle, on peut utiliser la méthode de la fonction de répartition. D'abord, on exprime la fonction de répartition de $S_n^2$ en termes de la fonction de répartition de $W$:
\begin{align*}
F_{S_n^2}(s)&=\Pr[S_n^2\leq s]=\Pr\left[\frac{\sigma^2 W}{n-1}\leq s\right]=\Pr[W\leq s(n-1)/\sigma^2]=F_W\left(\frac{s(n-1)}{\sigma^2}\right).
\end{align*}
Ensuite, on dérive $F_{S_n^2}(s)$ en fonction de $s$ pour trouver la densité:
\begin{align*}
f_{S_n^2}(s) &=\frac{\d}{\d s}F_W\left(\frac{s(n-1)}{\sigma^2}\right) = f_W\left(\frac{s(n-1)}{\sigma^2}\right)\frac{n-1}{\sigma^2}.
\end{align*}
Finalement, la densité de $W$ est la distribution d'une $\chi^2_{n-1}$. Donc, pour $s>0$,
\begin{align*}
f_{S_n^2}(s) &= \frac{n-1}{\sigma^2}\frac{1}{2^{(n-1)/2}\Gamma(\frac{n-1}{2})}\left(\frac{(n-1)s}{\sigma^2}\right)^{(n-1)/2-1}\exp\left(-\frac{(n-1)s}{2\sigma^2}\right)\\
&=\left(\frac{n-1}{2\sigma^2}\right)^{(n-1)/2}\frac{1}{\Gamma(\frac{n-1}{2})}s^{(n-1)/2-1}\exp\left(-\frac{s}{2\sigma^2/(n-1)}\right).
\end{align*}
Par conséquent, $S_n^2$ suit une distribution gamma avec paramètres $\alpha=(n-1)/2$ et $\beta=2\sigma^2/(n-1)$. La moyenne et la variance sont:
\begin{align*}
\ex[S_n^2]&= \alpha\beta= \frac{n-1}{2}\frac{2\sigma^2}{n-1}=\sigma^2\\
\vr(S_n^2)&=\alpha\beta^2=\frac{n-1}{2}\frac{4\sigma^4}{(n-1)^2}=\frac{2\sigma^4}{n-1}.
\end{align*}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $Z \sim \mathcal{N}(0, 1)$ avec fonction de densité de
  probabilité
  \begin{align*}
    \phi(z) &= \frac{1}{\sqrt{2 \pi}}\, e^{-\frac{1}{2}\, z^2}, \quad
    - \infty < z < \infty, \\
    \intertext{et fonction de répartition}
    \Phi(z) &= \int_{-\infty}^z \phi(x)\, dx.
  \end{align*}
  Exprimer les fonctions de densité et de répartition de $X = \mu +
  \sigma Z$ en fonction de $\phi(\cdot)$ et $\Phi(\cdot)$.
  \begin{rep}
    $f_X(x) = \sigma^{-1} \phi((x - \mu)/\sigma)$, %
    $F_X(x) = \Phi((x - \mu)/\sigma)$
  \end{rep}
  \begin{sol}
    On a $X = \mu + \sigma Z$. Par la technique de la fonction de
    répartition, on obtient:
    \begin{align*}
      F_X(x) &= \prob{X \leq x}\\
      &= \prob{\mu + \sigma Z \leq x}\\
      &= \Prob{Z \leq \frac{x - \mu}{\sigma}}\\
      &= \Phi\left(\frac{x-\mu}{\sigma}\right).
    \end{align*}
    Ainsi, la fonction de densité de probabilité de $X$ est
    \begin{displaymath}
      f_X(x) = \frac{d}{dx}\, \Phi\left(\frac{x - \mu}{\sigma}\right) =
      \frac{1}{\sigma}\, \phi\left(\frac{x-\mu}{\sigma}\right).
    \end{displaymath}
  \end{sol}
\end{exercice}

% La loi khi carré
\begin{exercice}
  \label{ex:transformations:khi2}
  Soit la variable aléatoire $Z \sim \mathcal{N}(0, 1)$. Démontrer que $Z^2 \sim
  \chi^2(1)$ avec la technique de la fonction génératrice des moments.
  (\emph{Note}: il faut intégrer pour trouver la fonction génératrice
  des moments de $Z^2$.)
  \begin{sol}
    Soit $Z \sim \mathcal{N}(0, 1)$. La fonction génératrice des moments de la
    variable aléatoire $Z^2$ est
    \begin{align*}
      M_{Z^2}(t) &= \Esp{e^{Z^2 t}} \\
      &= \int_{-\infty}^\infty e^{z^2 t} \phi(z)\, dz \\
      &= \int_{-\infty}^\infty
      \frac{1}{\sqrt{2\pi}}\, e^{z^2 t} e^{-z^2/2}\, dz\\
      &= \int_{-\infty}^\infty
      \frac{1}{\sqrt{2\pi}}\, e^{-z^2 (1 - 2t)/2}\, dz.
    \end{align*}
    En posant $\sigma^2 = (1 - 2t)^{-1}$, on voit que l'on peut écrire
    l'expression ci-dessus sous la forme
    \begin{equation*}
      M_{Z^2}(t) = \sigma \int_{-\infty}^\infty
      \frac{1}{\sigma \sqrt{2\pi}}\, e^{-z^2/(2 \sigma^2)}\,dz.
    \end{equation*}
    On reconnaît alors sous l'intégrale la densité d'une loi normale
    de moyenne $0$ et de variance $\sigma^2$. Par conséquent,
    \begin{equation*}
      M_{Z^2}(t) = \sigma = \left( \frac{1}{1 - 2t}\right)^{1/2},
    \end{equation*}
    soit la fonction génératrice des moments d'une loi gamma de
    paramètres $\alpha = 1/2 \text{ et } \beta = 2$ ou, de manière
    équivalente, d'une distribution $\chi^2(1)$.
  \end{sol}
\end{exercice}

\begin{exercice}
  \begin{enumerate}
  \item Soit $X \sim \mathcal{N}(0, \sigma^2)$. Trouver la distribution de $Y =
    X^2$.
  \item Soient $X_1$ et $X_2$ deux variables aléatoires indépendantes
    chacune distribuée selon une loi normale centrée réduite. Trouver
    la distribution de
    \begin{displaymath}
      Y = \frac{(X_1 - X_2)^2}{2}.
    \end{displaymath}
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item Gamma$(1/2, 2\sigma^2)$
    \item $\chi^2(1)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a $X \sim \mathcal{N}(0, \sigma^2)$ et $Y = X^2$. Il faut voir que
      $Y = X^2$ n'est pas une transformation bijective. On pose donc
      d'abord $Z = \abs{X}$ et on trouve la densité de $Z$ à l'aide de
      la technique de la fonction de répartition:
      \begin{align*}
        F_Z(z) &= \prob{\abs{X}\leq z}\\
        &= \prob{-z \leq X \leq z}\\
        &= F_X(z) - F_X(-z) \\
        \intertext{d'où}
        f_Z(z) &= f_X(z) + f_X(-z)\\
        &= \frac{2}{\sigma \sqrt{2\pi}}\, e^{-x^2/(2 \sigma^2)},
        \quad z > 0.
      \end{align*}
      Ensuite, on pose la transformation bijective $Y = Z^2 =
      \abs{X}^2 = X^2$. Par la technique du changement de variable, on
      a
      \begin{align*}
        f_Y(y) &= f_Z(y^{1/2}) \left| \frac{1}{2\sqrt{y}} \right| \\
        &= \frac{2}{\sigma \sqrt{2\pi}}\,
        e^{-y/(2 \sigma^2)} \left( \frac{1}{2\sqrt{y}} \right) \\
        &= \frac{(2 \sigma^2)^{-1/2}}{\Gamma(\frac{1}{2})}\,
        y^{1/2 - 1} e^{-y/(2 \sigma^2)}, \quad y > 0
      \end{align*}
      puisque $\Gamma(\frac{1}{2}) = \sqrt{\pi}$. On voit donc que
      \begin{equation*}
        Y \sim \text{Gamma}\left(\frac{1}{2}, 2\sigma^2\right).
      \end{equation*}
      
		Plus directement, on peut aussi voir que $$ X = \pm \sqrt{y}.$$ 
		Par la méthode du changement de variable, on développe
	  \begin{align*}
	        F_Y(y) &= \prob{Y\leq y}\\
		           &= \prob{X^2 \leq y}\\
		           &= \prob{-\sqrt{y} \leq X \leq \sqrt{y}}\\
		           &= F_X(\sqrt{y}) - F_X(-\sqrt{y}).
      \end{align*}

       On dérive ensuite pour trouver la fonction de densité:
      \begin{align*}
           \frac{\text{d}}{\text{d}y} F_Y(y) &= \frac{1}{2\sqrt{y}} f_x(\sqrt{y})- \frac{-1}{2\sqrt{y}} f_x(\sqrt{y})\\
	                           &= \frac{f_x(\sqrt{Y})}{\sqrt{y}}\\
	                           &= \frac{1}{\sqrt{2 \pi \sigma^2 y}} e^{-y\frac{1}{2 \sigma^2}} \\
	                           &= \frac{(2 \sigma^2)^{-1/2}}{\Gamma(\frac{1}{2})}\,
        y^{1/2 - 1} e^{-y/(2 \sigma^2)}, \quad y > 0.
      \end{align*}

    \item On sait que $Z = X_1 - X_2 \sim \mathcal{N}(0, 2)$ et que $Z/\sqrt{2}
      \sim \mathcal{N}(0, 1)$. En utilisant le résultat de la partie a), on a
      immédiatement que $Y = Z^2/2 \sim \chi^2(1)$.
    \end{enumerate}
  \end{sol}
\end{exercice}

% La loi T

\begin{exercice}
  Soit $T$ une variable aléatoire distribuée selon une loi \emph{t}
  avec $10$ degrés de liberté.
  \begin{enumerate}
  \item Trouver $\prob{\abs{T} > 2,228}$ à l'aide d'une table de la loi
    \emph{t}.
  \item Répéter la partie a) à l'aide de \textsf{R}. La fonction
    \texttt{pt(x, n)} donne la valeur de la fonction de répartition en
    \texttt{x} d'une loi \emph{t} avec \texttt{n} degrés de liberté.
  \end{enumerate}
  \begin{rep}
    $0,05$
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Puisque la loi $t$ est symétrique autour de zéro, on a
      \begin{align*}
        \prob{\abs{T} > 2,228}
        &= \prob{T > 2,228} + \prob{T < -2,228} \\
        &= 2 \prob{T > 2,228}.
      \end{align*}
      Or, on trouve dans la table de la loi $t$ de
      l'annexe~\ref{chap:t} que $\prob{T \leq 2,228} = 0,975$ si $T
      \sim t(10)$. Par conséquent, $\prob{\abs{T} > 2,228} = 2 (1 -
      0,975) = 0,05$.
    \item Toutes les fonction \textsf{R} servant à évaluer des
      fonctions de répartition ont un argument \texttt{lower.tail}. Ce
      argument est \texttt{TRUE} par défaut, mais lorsque qu'il est
      \texttt{FALSE}, la fonction retourne la probabilité
      \emph{au-dessus} du point \texttt{x}. Ainsi, la probabilité
      cherchée ici est
<<echo=TRUE>>=
2 * pt(2.228, 10, lower.tail = FALSE)
@
      Il est recommandé d'utiliser cette approche parce qu'elle est,
      de manière générale, plus précise que le calcul du type
      \verb|1 - pt(x, n)|, surtout loin dans les queues des
      distributions.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $T$ une variable aléatoire distribuée selon une loi \emph{t}
  avec $14$ degrés de liberté.
  \begin{enumerate}
  \item Trouver la valeur de $b$ tel que $\prob{-b < T < b} = 0,90$ à
    l'aide d'une table de la loi \emph{t}.
  \item Répéter la partie a) à l'aide \textsf{R}. La fonction
    \texttt{qt(p, n)} retourne le \texttt{p}{\ieme} quantile d'une loi
    \emph{t} avec \texttt{n} degrés de liberté, c'est-à-dire la valeur
    de $x$ où la fonction de répartition vaut \texttt{p}.
  \end{enumerate}
  \begin{rep}
    $1,761$
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Par symétrie de la loi $t$,
      \begin{align*}
        \prob{-b < T < b}
        &= \prob{T < b} - \prob{T < -b} \\
        &= \prob{T < b} - (1 - \prob{T < b}) \\
        &= 2 \prob{T < b} - 1 \\
        &= 0,90.
      \end{align*}
      On cherche donc la valeur de $b$ tel que $\prob{T < b} = (1 +
      0,90)/2 = 0,95$, où $T \sim t(14)$. Dans la table de la loi $t$
      de l'annexe~\ref{chap:t} on trouve que $b = 1,761$.
    \item En définitive, on cherche le 95{\ieme} centile d'une loi
      $t(14)$. Avec \textsf{R}, on obtient
<<echo=TRUE>>=
qt(0.95, 14)
@
    \end{enumerate}
  \end{sol}
\end{exercice}

% La loi F

\begin{exercice}
  Soit $U \sim \chi^2(r_1)$ et $V \sim \chi^2(r_2)$, deux variables
  aléatoires indépendantes.
  \begin{enumerate}
  \item Démontrer que la densité de
    \begin{displaymath}
      F = \frac{U/r_1}{V/r_2}
    \end{displaymath}
    est
    \begin{displaymath}
      f(x) =
      \frac{\Gamma((r_1 + r_2)/2) (r_1/r_2)^{r_1/2} x^{r_1/2 - 1}}{%
        \Gamma(r_1/2) \Gamma(r_2/2) (1 + r_1 x/r_2)^{(r_1 + r_2)/2}}.
    \end{displaymath}
  \item Calculer $\esp{F}$.
  \item Calculer $\var{F}$.
  \end{enumerate}
  \begin{rep}
   \begin{enumerate}
     \stepcounter{enumi}
    \item $r_2/(r_2 - 2)$
    \item $2 [r_2^2 (r_2 + r_1 - 2)]/[r_1 (r_2 - 2)^2 (r_2 - 4)]$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a $U \sim \chi^2(r_1)$ et $V \sim \chi^2(r_2)$. On pose
      $F = (U/r_1)/(V/r_2)$ et, disons, $G = V$. Pour trouver la
      densité (marginale) de $F$, il faudra passer par la densité
      conjointe de $F$ et $G$.

      Les équations régissant la transformation de variables
      aléatoires sont
      \begin{align*}
        x &= \frac{r_2}{r_1}\left(\frac{u}{v}\right) &
        u &= \frac{r_1}{r_2}\, x y \\
        y &= v &
        v &= y.
      \end{align*}
      Ainsi, le jacobien de la transformation est
      \begin{displaymath}
        J =
        \begin{vmatrix}
          r_1 y/r_2 & r_1 x/r_2 \\
          0         & 1
        \end{vmatrix}
        = \frac{r_1}{r_2}\, y
      \end{displaymath}
      et la densité conjointe de $F$ et $G$ est
      \begin{align*}
        f_{FG}(x, y)
        &= f_{UV}\left(\frac{r_1}{r_2}\, xy, y\right)
        \left|\frac{r_1}{r_2}\, y\right| \\
        &= \left(\frac{r_1}{r_2}\right) y
        f_U\left(\frac{r_1}{r_2}\, xy\right) f_V(y) \\
        &= \frac{%
          (r_1/r_2)
          (1/2)^{(r_1 + r_2)/2}
          (r_1 xy/r_2)^{r_1/2 - 1}
          y^{r_2/2}
          e^{-(r_1 x/r_2 + 1) y/2}}{%
          \Gamma(r_1/2) \Gamma(r_2/2)}
      \end{align*}
      pour $x > 0$ et $y > 0$. En intégrant, on trouve la densité
      marginale de $F$:
      \begin{align*}
        f_F(x) &= \int_0^{\infty} f_{FG}(x, y)\, dy \\
        &= \frac{(r_1/r_2)^{(r_1 + r_2)/2} x^{r_1/2 - 1}}{%
          \Gamma(r_1/2) \Gamma(r_2/2)} \\
        &\phantom{=} \times
        \int_0^\infty
        \left(\frac{1}{2}\right)^{(r_1 + r_2)/2}
        y^{(r_1 + r_2)/2 - 1}
        e^{-(r_1 x/r_2 + 1) y/2}\, dy \\
        &= \frac{\Gamma((r_1 + r_2)/2) (r_1/r_2)^{r_1/2} x^{r_1/2 - 1}}{%
          \Gamma(r_1/2) \Gamma(r_2/2) (r_1 x/r_2 + 1)^{(r_1 + r_2)/2}} \\
        &\phantom{=} \times
        \int_0^\infty
        \frac{(1/2)^{(r_1 + r_2)/2} (r_1 x/r_2 + 1)^{(r_1 + r_2)/2}}{%
          \Gamma((r_1 + r_2)/2)}\,
        y^{(r_1 + r_2)/2 - 1}
        e^{-(r_1 x/r_2 + 1) y/2}\, dy \\
        &= \frac{\Gamma((r_1 + r_2)/2) (r_1/r_2)^{r_1/2} x^{r_1/2 - 1}}{%
        \Gamma(r_1/2) \Gamma(r_2/2) (1 + r_1 x/r_2)^{(r_1 + r_2)/2}},
      \end{align*}
      puisque l'intégrande ci-dessus est la densité d'une loi gamma.
      La loi de la variable aléatoire $F$ est appelée loi $F$ avec
      $r_1$ et $r_2$ degrés de liberté.
    \item Par indépendance entre les variables aléatoires $U$ et $V$,
      on a
      \begin{align*}
        \esp{F} &= \Esp{\frac{U/r_1}{V/r_2}} \\
        &= \frac{r_2}{r_1}\, \Esp{\frac{U}{V}} \\
        &= \frac{r_2}{r_1}\, \esp{U} \Esp{\frac{1}{V}}.
      \end{align*}
      Or, $\esp{U} = r_1$ et
      \begin{align*}
        \Esp{\frac{1}{V}}
        &= \int_0^\infty \frac{1}{v}\, f_V(v)\, dv \\
        &= \int_0^\infty
        \frac{1}{2^{r_2/2} \Gamma(r_2/2)}\,
        v^{r_2/2 - 1 - 1} e^{-v/2}\, dv \\
        &= \frac{2^{r_2/2 - 1} \Gamma(r_2/2 - 1)}{%
          2^{r_2/2} \Gamma(r_2/2)}, \quad
        \frac{r_2}{2} - 1 > 0.
      \end{align*}
      Avec la propriété de la fonction gamma $\Gamma(x) = (x - 1)
      \Gamma(x - 1)$, cette expression se simplifie en
      \begin{equation*}
        \Esp{\frac{1}{V}} = \frac{1}{r_2 - 2},
      \end{equation*}
      d'où, enfin,
      \begin{displaymath}
        \esp{F} = \frac{r_2}{r_2 - 2}
      \end{displaymath}
      pour $r_2 > 2$.
    \item En procédant comme en b), on trouve que $\esp{U^2} = \var{U}
      + \esp{U}^2 = 2 r_1 + r_1^2$, que
      \begin{displaymath}
        \Esp{\frac{1}{V^2}} = \frac{1}{(r_2 - 2)(r_2 - 4)}
      \end{displaymath}
      et donc que
      \begin{align*}
        \esp{F^2}
        &= \frac{r_2^2}{r_1^2}\, \esp{U^2} \Esp{\frac{1}{V^2}} \\
        &= \frac{r_2^2 (r_1 + 2)}{r_1 (r_2 - 2)(r_2 - 4)}.
      \end{align*}
      Par conséquent,
      \begin{align*}
        \var{F}
        &= \frac{r_2^2 (r_1 + 2)}{r_1 (r_2 - 2)(r_2 - 4)} -
        \left(\frac{r_2}{r_2 - 2}\right)^2 \\
        &= 2 \left(\frac{r_2}{r_2 - 2}\right)^2
        \left(\frac{r_2 + r_1 - 2}{r_1 (r_2 - 4)}\right),
      \end{align*}
      pour $r_2 > 4$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:transformations:Finverse}
  Soit $F$ une variable aléatoire distribuée selon une loi \emph{F}
  avec $\nu_1$ et $\nu_2$ degrés de liberté (dans l'ordre). Démontrer que
  $1/F$ est aussi distribuée selon une loi \emph{F}, mais avec $\nu_2$
  et $\nu_1$ degrés de liberté.
  \begin{sol}
    On a
    \begin{align*}
      \frac{1}{F} &=
      \left(\frac{U/\nu_1}{V/\nu_2}\right)^{-1} \\
      &=\frac{V/\nu_2}{U/\nu_1}
    \end{align*}
    où $U \sim \chi^2(\nu_1)$ et $V \sim \chi^2(\nu_2)$. Puisqu'il s'agit
    d'un ratio de deux variables aléatoires $\chi^2$ divisées chacune
    par son nombre de degrés de liberté, on a donc que
    \begin{displaymath}
      \frac{1}{F} \sim F(\nu_2, \nu_1).
    \end{displaymath}
  \end{sol}
\end{exercice}

\begin{exercice}
  Si $F$ a une distribution \emph{F} avec paramètres $\nu_1 = 5$ et $\nu_2
  = 10$, trouver $a$ et $b$ de sorte que $\prob{F \leq a} = 0,05$ et
  $\prob{F \leq b} = 0,95$. Les quantiles de la loi \emph{F} peuvent
  être trouvés soit dans une table, soit à l'aide de la fonction
  \texttt{qf(x, v1, v2)} de \textsf{R}. (\emph{Astuce}: en travaillant
  avec une table, utiliser le fait que $\prob{F \leq a} = \prob{F^{-1}
    \geq a^{-1}} = 1 - \prob{F^{-1} \leq a^{-1}}$.)
  \begin{rep}
    $a = 0,211$ et $b = 3,33$
  \end{rep}
  \begin{sol}
    On a $F \sim F(5, 10)$ et l'on cherche $a$ et $b$ tel que $\prob{F
      \leq a} = 0,05$ et $\prob{F \leq b} = 0,95$. Dans une table de
    loi $F$, on trouve que $\prob{F \leq 3,326} = 0,95$ et donc que $b
    = 3,33$. Puisque les quantiles inférieurs ne sont pas inclus dans
    la table de l'annexe~\ref{chap:F}, on doit utiliser pour trouver
    $a$ la relation $\prob{F \leq a} = 1 - \prob{F^{-1} \leq a^{-1}}$
    où, tel que démontré à
    l'exercice~\ref{chap:echantillon}.\ref{ex:transformations:Finverse},
    $F^{-1} \sim F(10, 5)$. Dans une table, on trouve que $a^{-1} =
    4,74$, d'où $a = 0,211$.

    Avec \textsf{R}, on obtient les mêmes résultats encore plus
    simplement:
<<echo=TRUE>>=
qf(c(0.05, 0.95), 5, 10)
@
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $T = W/\sqrt{V/r}$, où $W$ et $V$ sont des variables aléatoires
  indépendantes avec une distribution, respectivement, normale centrée
  réduite et khi carré avec $r$ degrés de liberté. Démontrer que la
  distribution de $T^2$ est $F$ avec $1$ et $r$ degrés de liberté.
  \begin{sol}
    On sait que $W^2 \sim \chi^2(1)$. Ainsi,
    \begin{displaymath}
      T^2 = \frac{W^2/1}{V/r},
    \end{displaymath}
    qui est un ratio de deux variables aléatoires $\chi^2$ divisées
    par leur nombre de degrés de liberté. Par définition de la loi
    $F$, on a donc que $T^2 \sim F(1, r)$.
  \end{sol}
\end{exercice}

% TCL 

\begin{exercice}
  Démontrer à l'aide du Théorème central limite que la distribution
  gamma avec paramètre de forme $\alpha$ entier et paramètre d'échelle
  $\beta$ tend vers la distribution normale avec moyenne
  $\alpha\beta$ et variance $\alpha\beta^2$ lorsque $\alpha$
  tend vers l'infini. (\emph{Astuce}: définir $Y = X_1 + \dots +
  X_\alpha$ où $X_i \sim \text{Exponentielle}(\beta)$ et trouver la
  distribution asymptotique de $Y$.)
  \begin{sol}
    Soit
    \begin{equation*}
      Y = \sum_{i = 1}^\alpha X_i
    \end{equation*}
    avec $X_i \sim \text{Exponentielle}(\beta)$ et $X_1, \dots,
    X_\alpha$ indépendantes. Par le Théorème central limite,
    \begin{align*}
      \lim_{\alpha \rightarrow \infty} Y =
      \lim_{\alpha  \rightarrow \infty} \sum_{i = 1}^\alpha X_i \sim
      \mathcal{N}(\alpha \esp{X_i}, \alpha \var{X_i}).
    \end{align*}
    Par conséquent,
    \begin{equation*}
      \lim_{\alpha \rightarrow \infty} Y \sim
      N\left(\alpha\beta, \alpha\beta^2\right).
    \end{equation*}
    On trouve à la figure~\ref{fig:echantillon:gammas} les graphiques
    de densités gamma pour quelques valeurs du paramètre $\alpha$. On
    observe, en effet, que la distribution tend vers une normale
    lorsque $\alpha$ augmente.
    \begin{figure}
      \centering
<<echo=FALSE, fig=TRUE, out.width = "0.6\\textwidth">>=
curve(dgamma(x, 10, 1), from = 0, to = 100,
      ylab = expression(paste("dgamma(x, ", alpha, ", 1)")),
      lty = 3, lwd = 2)
curve(dgamma(x, 20, 1), add = TRUE, lty = 2, lwd = 2)
curve(dgamma(x, 50, 1), add = TRUE, lty = 1, lwd = 2)
legend("topright", c(expression(paste(alpha, " = 10")),
                     expression(paste(alpha, " = 20")),
                     expression(paste(alpha, " = 50"))),
       lty = 3:1, lwd = 2)
@
      \caption{Densités de lois gamma pour quelques valeurs du
        paramètre de forme $\alpha$.}
      \label{fig:echantillon:gammas}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\bar{X}_{100}$ la moyenne d'un échantillon aléatoire de taille $100$
  tiré d'une loi $\chi^2(50)$.
  \begin{enumerate}
  \item Trouver la distribution exacte de $\bar{X}_{100}$.
  \item Calculer à l'aide d'un logiciel statistique la valeur exacte
    de $\prob{49 < \bar{X}_{100} < 51}$.
  \item Calculer une valeur approximative de la probabilité en b).
  \end{enumerate}
  \begin{rep}
<<echo=FALSE>>=
q <- format(pgamma(51, 2500, 50) - pgamma(49, 2500, 50),
            digits = options()$digits, dec = ",")
@
    \begin{inparaenum}
    \item Gamma$(\nombre{2500}, 1/50)$
    \item $\Sexpr{q}$
    \item $0,682$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a l'échantillon aléatoire $X_1, \dots, X_{100}$, où
      \begin{equation*}
        X_i \sim \chi^2(50) \equiv
        \text{Gamma}\left( \frac{50}{2},  2 \right), \quad
        i = 1, \dots, 100.
      \end{equation*}
      Or, on sait que $Y = X_1 + \dots + X_{100} \sim \text{Gamma}(100
      (25), 2)$ et que $\bar{X}_{100} = Y/100 \sim
      \text{Gamma}(\nombre{2500}, 2/100)$. 

Ce résultat tient parce qu'une somme de $k$ observations d'un échantillon aléatoire tiré d'une loi gamma est telle que $ S = \sum_{i = 1}^{k} X_i \sim Gamma(\alpha k, \beta)$. La distribution de $\bar{X}_{100}$ suit directement de la transformation d'échelle d'une loi gamma, qu'on peut démontrer en utilisant la fonction génératrice de moments. Ainsi,
      \begin{align*}
      \mathcal{M}_{\bar{X}_{100}}(t) &= \esp{e^{t \frac{Y} {100}}} \\
                            &= \esp{e^{Y \frac{t} {100}}} \\
                            &= \mathcal{M}_Y(\frac{t}{100}).
      \end{align*}
      
    \item On peut, par exemple, obtenir la probabilité demandée avec
      \textsf{R} ainsi:
<<echo=TRUE>>=
pgamma(51, 2500, 50) - pgamma(49, 2500, 50)
@
    \item On a $\esp{\bar{X}_{100}} = \nombre{2500}/50 = 50$ et
      $\var{\bar{X}_{100}} = \nombre{2500}/50^2 = 1$. En utilisant
      l'approximation normale, on trouve
      \begin{align*}
        \prob{49 < \bar{X}_{100} < 51}
        &= \Prob{\frac{49 - 50}{1} <
          \frac{\bar{X}_{100} - 50}{1} <
          \frac{51 - 50}{1}} \\
        &\approx \Phi(1) - \Phi(-1) \\
        &= 2\Phi(1) - 1 \\
        &= 0,682.
      \end{align*}
      On trouve la valeur de $\Phi(1)$ dans une table de quantiles de
      la loi normale ou à l'aide d'un logiciel statistique.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\bar{X}$ la moyenne d'un échantillon de taille $128$ d'une loi
  Gamma$(2, 4)$. Trouver une approximation pour $\prob{7 < \bar{X} <
    9}$.
  \begin{rep}
    $0,954$
  \end{rep}
  \begin{sol}
    Puique l'on ne demande qu'une valeur approximative pour $\prob{7 <
      \bar{X} < 9}$, on va utiliser l'approximation normale. La taille
    de l'échantillon étant relativement grande, l'approximation sera
    très bonne. Soit $\bar{X} = (X_1 + \dots + X_{128})/128$, où $X_i
    \sim \text{Gamma}(2, 4)$, $i = 1, \dots, 128$. On a
    $\esp{\bar{X}} = \esp{X_i} = 8$ et $\var{\bar{X}} = \var{X_i}/128
    = 1/4$. Par conséquent,
    \begin{align*}
      \prob{7 < \bar{X} < 9}
        &= \Prob{\frac{7 - 8}{\sqrt{1/4}} <
          \frac{\bar{X} - 8}{\sqrt{1/4}} <
          \frac{9 - 8}{\sqrt{1/4}}} \\
      &\approx \Phi(2) - \Phi(-2) \\
      &= 2 \Phi(2) - 1 \\
      &= 0,954.
    \end{align*}
    On trouve la valeur de $\Phi(2)$ dans une table de quantiles de
    la loi normale ou à l'aide d'un logiciel statistique.
  \end{sol}
\end{exercice}

\begin{exercice}
  Trouver une valeur approximative de la probabilité que la moyenne
  d'un échantillon de taille $15$ d'une loi avec densité $f(x) = 3
  x^2$, $0 < x < 1$, soit entre $3/5$ et $4/5$.
  \begin{rep}
    $0,840$
  \end{rep}
  \begin{sol}
    On souhaitera utiliser l'approximation normale. Cela requiert de
    connaître les valeurs de l'espérance et de la variance de la
    moyenne de l'échantillon, $\bar{X}$, et, par ricochet, celles de
    la variable aléatoire avec densité $f(x)$. Or,
    \begin{align*}
      \esp{X} &= \int_0^1 3x^3\,dx = \frac{3}{4} \\
      \esp{X^2} &= \int_0^1 3x^4\,dx = \frac{3}{5}
    \end{align*}
    et donc $\var{X} = 3/80$. Ainsi, $\esp{\bar{X}} = \esp{X} = 3/4$
    et $\var{\bar{X}} = \var{X}/15 = 1/400$ et
    \begin{align*}
      \Prob{\frac{3}{5} < \bar{X} < \frac{4}{5}}
      &= \Prob{\frac{3/5 - 3/4}{\sqrt{1/400}} <
        \frac{\bar{X} - 3/4}{\sqrt{1/400}} <
        \frac{4/5 - 3/4}{\sqrt{1/400}}} \\
      &\approx \Phi(1) - \Phi(-3) \\
      &= 0,840.
    \end{align*}
  \end{sol}
\end{exercice}


\begin{exercice} 
On suppose que $X_1,\ldots,X_n$ et $Y_1,\ldots,Y_n$ sont des échantillons aléatoires indépendants de populations avec moyennes $\mu_1$ et $\mu_2$ et variances $\sigma_1^2>0$ et $\sigma_2^2>0$, respectivement. Démontrer que, quand $n\to\infty$,
$$
U_n = \frac{(\bar X_n-\bar Y_n)-(\mu_1-\mu_2)}{\sqrt{(\sigma_1^2+\sigma_2^2)/n}}
$$
est asymptotiquement $\mathcal{N}(0,1)$.
\begin{sol}
Pour chaque $i \in \{1,\dots, n \}$, on pose $D_i = X_i - Y_i$. $D_1, \ldots, D_n$ sont mutuellement indépendants et identiquement distribués avec moyenne
$$
\ex(D_1) = \ex(X_1) - \ex(Y_1) = \mu_1 - \mu_2
$$
et variance
$$
\vr(D_1) = \vr(X_1) + \vr(Y_1) = \sigma_1^2 + \sigma_2^2.
$$
Par le Théorème central limite, on a donc que, quand $n \to \infty$,
$$
\sqrt{n} \, \frac{\bar D_n - \ex[D_1]}{\sqrt{\vr(D_1)}} = \frac{(\bar X_n - \bar Y_n) - (\mu_1 -\mu_2)}{\sqrt{(\sigma_1^2 + \sigma_2^2)/n}} = Z_n
$$
converge en distribution vers $\mathcal{N}(0,1)$.
\end{sol}
\end{exercice}

\begin{exercice} 
Une machine d'embouteillage peut être réglée de sorte qu'elle remplisse en moyenne les bouteilles de $\mu$ onces de liquide par bouteille. Il a été observé que la quantité de liquide distribuée par la machine suit une loi normale avec $\sigma= 2,5$ onces.
\begin{enumerate}
\item Si $n=9$ bouteilles sont sélectionnées aléatoirement à la sortie de la machine, quelle est la probabilité que la moyenne échantillonnale diffère de $\mu$ d'au plus 0,2 once?
\item Trouver la probabilité que la moyenne échantillonale diffère de $\mu$ d'au plus 0,2 once lorsque la taille de l'échantillon est $n = 25, 36,$ et $64$. Que remarquez-vous lorsque $n$ augmente? Pouvez-vous expliquer cette remarque?
\item Quelle est la taille d'échantillon requise pour s'assurer que la probabilité que la moyenne échantillonnale diffère de $\mu$ d'au plus 0,2 once est d'au moins 95\%?
\item Comment la probabilité obtenue en a) change-t-elle lorsque $\sigma$ est inconnu et que la variance échantillonnale est égale à $s_n^2= 5,5$?
\end{enumerate}
\begin{rep}
a) 0,1896 \,\, b) 0,3108~; 0,3688~; 0,4778 \,\, c) 601 \,\, d) 0,1955
\end{rep}
\begin{sol}
\begin{enumerate}
\item La probabilité peut être calculée comme suit:
\begin{align*}
\Pr(|\bar X_n - \mu| \le 0,2) & = \Pr(-0,2 \le \bar X_n - \mu \le 0,2) \\
& = \Pr\left(-\frac{0,2}{\sqrt{2,5^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \le \frac{0,2}{\sqrt{2,5^2/9}} \right).
\end{align*}
On sait que 
$$
\frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \sim \mathcal{N}(0,1),
$$
la probabilité est alors
\begin{align*}
\Pr\left(-\frac{0,2}{\sqrt{2,5^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \le \frac{0,2}{\sqrt{2,5^2/9}} \right) &= \Phi\left(\frac{0,2}{\sqrt{2,5^2/9}}\right)-\Phi\left(-\frac{0,2}{\sqrt{2,5^2/9}}\right)\\
& = 2\Phi\left(\frac{0,2}{\sqrt{2,5^2/9}}\right)-1,
\end{align*}
où $\Phi$ représente la fonction de répartition de la loi normale centrée réduite. Cette probabilité peut être évaluée avec une table de la loi normale ou avec \textsf{R}; ce dernier donne 
<<>>=
2*pnorm(0.2/sqrt(2.5^2/9))-1
@

\item De la même façon qu'en a), on trouve
\begin{align*}
\Pr(|\bar X_n - \mu| \le 0,2)& =\Pr\left(-\frac{0,2}{\sqrt{2,5^2/n}} \le \frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \le \frac{0,2}{\sqrt{2,5^2/n}} \right)\\
& = 2\Phi\left(\frac{0,2}{\sqrt{2,5^2/n}}\right)-1.
\end{align*}
On considère $n=25$, $n=36$ et $n=64$ dans la formule, ce qui donne 
<<echo=1:2>>=
n <- c(25,36,64)
2*pnorm(0.2/sqrt(2.5^2/n))-1
out <-2*pnorm(0.2/sqrt(2.5/n))-1
@
On remarque que la probabilité que la moyenne échantillonnale diffère de la vraie moyenne d'au plus 0,2 once augmente avec $n$. C'est le résultat qu'on attend selon la Loi faible des grands nombres, qui indique que cette probabilité tend vers $1$ quand $n\to \infty$.

\item Si
$$
\Pr ( | {\bar X}_n - \mu | \le 0,2) = 2\Phi\left(\frac{0,2}{\sqrt{2,5^2/n}}\right)-1 \geq 0,95,
$$
alors
$\Phi \left(0,2\sqrt{n}/2,5\right) \geq 0,975$ et ainsi $0,2\sqrt{n}/2,5 \geq 1,96$, ce qui implique que $\sqrt{n} \geq 24,5$ ou $n\geq 600,25$. La taille d'échantillon doit donc être de $n=601$ pour que la probabilité soit \emph{le plus près possible, mais pas plus petite que} 0,95.

\item Dans ce cas, la probabilité à calculer est 
$$
 \Pr\left(-\frac{0,2}{\sqrt{s_n^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{S_n^2/9}} \le \frac{0,2}{\sqrt{s_n^2/9}} \right).
$$
On sait que 
$$
\frac{\bar X_n - \mu}{\sqrt{S_n^2/9}} \sim t_{(8)},
$$
on trouve que
\begin{align*}
 \Pr\left(-\frac{0,2}{\sqrt{s_n^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{S_n^2/9}} \le \frac{0,2}{\sqrt{s_n^2/9}} \right) & =  \Pr\left(-\frac{0,2}{\sqrt{5,5/9}} \le \frac{\bar X_n - \mu}{\sqrt{5,5/9}} \le \frac{0,2}{\sqrt{5,5/9}} \right) \\
 &= T_{(8)}\left(\frac{0,2}{\sqrt{5,5/9}} \right) - T_{(8)} \left(-\frac{0,2}{\sqrt{5,5/9}} \right),
\end{align*}
où $T_{(8)}$ est la fonction de répartition d'une loi Student $t$ avec $8$ degrés de liberté. Encore une fois, cette expression peut être évaluée avec une table de la loi normale ou avec \textsf{R}; ce dernier donne
<<>>=
pt(0.2/sqrt(5.5/9),df=8) -pt(-0.2/sqrt(5.5/9),df=8)
@
La probabilité obtenue est plus grande qu'en a) étant donné la variabilité ajoutée avec l'estimation de la variance.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
L'\emph{Agence de Protection de l'Environnement} est responsable d'établir les critères pour la quantité autorisée de certains produits chimiques en eau douce. Une mesure commune de la toxicité pour les polluants est la concentration d'un produit chimique causant la mort de la moitié d'une population animale donnée dans un laps de temps connu. Cette mesure est appelée CL50 (concentration létale médiane). Dans plusieurs études, les valeurs observées du logarithme naturel de l'indicateur CL50 sont normalement distribuées. L'analyse est donc basée sur les données de ln(CL50).

Soit $S^2_n$ la variance échantillonnale d'un échantillon de $n=10$ valeurs de ln(CL50) pour le cuivre, et $S^2_m$ la variance échantillonnale d'un échantillon de $m=6$ valeurs de ln(CL50) pour le plomb, tous les deux utilisant la même espèce de poisson. La variance de la population des mesures sur le cuivre est supposée être le double de la variance de la population des mesures sur le plomb. Supposer que $S^2_n$ est indépendante de $S^2_m$.
\begin{enumerate}
\item Expliquer comment il est possible, en référant à une table statistique appropriée, de trouver les nombres $a$ et $b$ tels que
$$
\Pr\left(\frac{S_n^2}{S_m^2}\leq b\right)=0,95, \quad \Pr\left(\frac{S_n^2}{S_m^2}\geq a\right)=0,95.
$$
\emph{Astuce: Observer que $\Pr[U_1/U_2\leq k]=\Pr[U_2/U_1\geq 1/k]$.}

\item Si $a$ et $b$ sont les mêmes qu'en a), calculer
$$
\Pr\left(a\leq\frac{S_n^2}{S_m^2}\leq b\right).
$$
\end{enumerate}
\begin{rep}
a) 0,5744 \,\, b) 0,9
\end{rep}
\begin{sol}
\begin{enumerate}
\item On peut utiliser le fait que
$$
\frac{S_n^2/\sigma_1^2}{S_m^2/\sigma_2^2} \sim F_{(n-1,m-1)},
$$
où $\sigma^2_1$ et $\sigma_2^2$ représentent la variance du premier et du second échantillon, respectivement. Dans ce contexte, $n=10$, $m=6$ et $\sigma_1^2 = 2\sigma_2^2$. Donc,
$$
\frac{S_n^2}{2 S_m^2} \sim F(9,5).
$$
Si $W$ représente une variable aléatoire $F_{(9,5)}$, on a
$$
\Pr\left(  \frac{S_n^2}{ S_m^2} \le b\right) = \Pr(W \le b/2)
$$
et donc $b/2$ est le $95$e quantile de la distribution $F_{(9,5)}$.
<<>>=
qf(0.95,df1=9,df2=5)
@
Par conséquent,
$$
b = 2 \times \Sexpr{qf(0.95,df1=9,df2=5)} =  \Sexpr{2*qf(0.95,df1=9,df2=5)}. 
$$
Pour trouver $a$, on note que 
$$
\frac{S_m^2/\sigma_2^2}{S_n^2/\sigma_1^2} = 2 \frac{S_m^2}{ S_n^2} \sim F_{(5,9)}.
$$
Alors,
$$
\Pr\left(  \frac{S_n^2}{ S_m^2} \ge a\right) = \Pr\left(  \frac{S_m^2}{ S_n^2} \le \frac{1}{a}\right) = \Pr\left(  2\frac{S_m^2}{ S_n^2} \le \frac{2}{a}\right). 
$$
Donc, $2/a$ est le $95$e quantile de la distribution $F_{(5,9)}$.
<<>>=
qf(0.95,df1=5,df2=9)
@
et ainsi
$$
a = \frac{2}{\Sexpr{qf(0.95,df1=5,df2=9)}} = \Sexpr{2/qf(0.95,df1=5,df2=9)}.
$$
\item Cette probabilité égale
\begin{align*}
\Pr\left(a \le \frac{S_n^2}{S_m^2}\le b\right) & = \Pr\left(\frac{S_n^2}{S_m^2}\le b\right) - \Pr\left(\frac{S_n^2}{S_m^2}\le a\right)\\
& = \Pr\left(\frac{S_n^2}{S_m^2}\le b\right) + \Pr\left(\frac{S_n^2}{S_m^2}\ge a\right) -1 = 2\times 0,95 -1 = 0,9.
\end{align*}
\end{enumerate}
\end{sol}
\end{exercice}



%\begin{exercice} 
%The following \textsf{R} function returns the sample variances of $N$ random samples of size $n$ drawn from the Normal distribution with mean $\mu$ and variance $\sigma^2$:
%<< >>=
%sample.var <- function(n=10, N=100, mu=0, sd=1){
%    data <- rnorm(n*N, mean=mu, sd=sd)
%    data.mat <- matrix(data, ncol=N)
%    apply(data.mat, 2, var)
%}
%@
%For example, to compute sample variances of $N = 1000$ samples of size $n = 28$ from the $\mathcal{N}(0,1)$, call
%<<eval=FALSE>>=
%sample.var(n=28, N=1000, mu=0, sd=1)
%@
%\begin{enumerate}
%\item Generate $N = 1000$ samples of size $n = 10$ from the $\mathcal{N}(0, 1)$ distribution. Compute $S_n^2$ for each of these samples and summarize the results in a histogram. What is the mean of these sample variances? Is it close to $\sigma^2$? What is the sample variance of your sample of sample variances? Is this what you expect? Overlay the histogram with a plot of the theoretical density of the sampling distribution of the sample variance from Q8. Does this density provide a good approximation to the histogram?
%
%\item Repeat part a) for samples of size $n = 200$. How do the results compare? What does the theoretical density of the sample variance remind you of?
%
%\item Adapt the function \texttt{sample.var} so that it draws samples from the Exponential distribution with mean 1. Redo parts (a) and (b) when the random samples are drawn from this distribution. Although not justified theoretically, you can still overlay the resulting histograms with the theoretical sampling density of the sample variance of a Normal sample with variance 1. What do you see now?
%\end{enumerate}
%\begin{sol}
%\begin{enumerate}
%\item The sample of sample variances can be drawn as follows; the result is stored in the vector \texttt{out}:
%<<>>=
%sample.var <- function(n=10,N=100){
%	data <- rnorm(n*N,mean=0,sd=1)
%	data.mat <- matrix(data,ncol=N)
%	apply(data.mat,2,var)
%	}
%set.seed(28)	
%out <- sample.var(n=10,N=1000)	
%@
%
%The mean and variance of the sample of sample variances are then
%<<>>=
%mean(out)
%var(out)
%@
%The sample mean is indeed close to the theoretical value, i.e., $\sigma^2=1$. The sample variance is also closed to the theoretical value found in Q8, i.e., $2\sigma^4/(n-1)=0.22$. The histogram of the sample variance values along with the theoretical density of the Gamma with shape parameter $\alpha=(n-1)/2=9/2$ and scale parameter $\beta=2\sigma^2/(n-1)=2/9$ can then be plotted as follows:
%<<fig.height=4>>=
%hist(out, freq=FALSE, ylim=c(0,1),
%     main="Histogramme des variances \u{E9}chantillonnales")
%curve(dgamma(x, shape=9/2, scale=2/9), col="red", lwd=2, add=TRUE)
%@
%
%The fit seems to be very good.
%
%\item First, we draw 1000 samples from the Normal distribution of size $n=200$ and then we compute the sample variances, as well as the mean and variance of the resulting sample of sample variances:
%<<>>=
%set.seed(17)
%out <- sample.var(n=200, N=1000)
%mean(out)
%var(out)
%@
%Again, the mean is close to the theoretical variance, i.e., $\sigma^2=1$. Furthermore, compared to part (a), the sample variance is much smaller, indicating that the observed values of $S_n^2$ are now much closer to one. The theoretical density of $S_n^2$ is given by $\mathcal{G}(199/2,2/199)$. The histogram of the sample variances can be produced as follows:
%<<fig.height=4>>=
%hist(out, freq=FALSE, ylim=c(0,5),
%     main="Histogramme des variances \u{E9}chantillonnales")
%curve(dgamma(x, shape=199/2, scale=2/199), col="red", lwd=2, add=TRUE)
%@
%The fit is again very good. In addition, the true density of $S_n^2$ resembles the Normal density. This is not a coincidence, because the $\chi^2_\nu$ distribution can be well approximated by the Normal distribution as $\nu \to \infty$.
% 
%\item The function \texttt{sample.var} can be adapted as follows: 
% <<>>=
% sample.var.exp <- function(n=10,N=100){
%	data <- rexp(n*N,rate=1)
%	data.mat <- matrix(data,ncol=N)
%	apply(data.mat,2,var)
%	}
% @
%First, we compute sample variances  corresponding to $1000$ samples of size $n=5$ and $n=500$ from the Exponential distribution with mean $1$. We also compute the means and variances of the samples of sample variances:
%<<>>=
%set.seed(123)
%out1 <- sample.var.exp(n=10, N=1000)
%mean(out1)
%var(out1)
%out2 <- sample.var.exp(n=200, N=1000)
%mean(out2)
%var(out2)
%@
%We can see that the mean is quite close to the theoretical variance of the Exponential distribution with mean $1$. Also, as $n$ increases from $10$ to $200$, the sample variance decreases. However, the sample variance is considerably higher when compared to the case of Normal samples investigated in parts (a) and (b).
% 
%The histograms and densities of $S_n^2$ computed pretending the samples are drawn from the Normal distribution can be plotted as follows:
%<<fig.height=4>>=
%par(mfrow=c(1,2)) # this draws two pictures next to one another
%hist(out1, freq=FALSE,main="Histogram of sample variances",ylim=c(0,1))
%curve(dgamma(x,shape=9/2,scale=2/9),col="red",lwd=2,add=TRUE)
%hist(out2, freq=FALSE,main="Histogram of sample variances",ylim=c(0,6.5))
%curve(dgamma(x,shape=199/2,scale=2/199),col="red",lwd=2,add=TRUE)
%@
%We can clearly see, especially when $n=200$, that the sampling distribution of the sample variance for exponential samples is not the same as the sampling distribution of the sample variance for Normal samples. It can be shown that, as $n\to \infty$, the sampling distribution of the sample variance converges to the Normal distribution. However, the variance of the limiting Normal distribution depends on the distribution from which the original samples were drawn.
%\end{enumerate}
%\end{sol}
%\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}


%\section*{Exercices proposés dans \cite{Wackerly:mathstat:7e:2008}}
%
%\begin{trivlist}
%\item 3.145--3.151, 3.153, 3.155, 3.158, 3.159, 3.161, 3.162, 3.163
%\end{trivlist}
%

%%%
%%% Insérer les réponses
%%%
\input{reponses-echantillon}

