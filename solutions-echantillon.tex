\section*{Chapitre \ref{chap:C}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:C}}

\begin{solution}{2.1}
    On sait que $\bar{X}_{5} \sim \mathcal{N}(\esp{\bar{X}_{5}}, \var{\bar{X}_{5}})$ avec
      $\esp{\bar{X}_{5}} = \esp{X} = 10$ et $\var{\bar{X}_{5}} = \var{X}/n =
      125/5 = 5$. Par conséquent,
    \begin{align*}
      \prob{\bar{X}_{5} < c}
      &= \Prob{\frac{\bar{X}_{5} - 10}{\sqrt{25}} < %
        \frac{c - 10}{\sqrt{25}}} \\
      &= \prob{Z < z_\alpha} \\
      &= 1 - \alpha
    \end{align*}
    avec $Z \sim \mathcal{N}(0, 1)$ et $z_\alpha = (c - 10)/5$. Ici, on a $1 -
    \alpha = 0,90$. On trouve dans une table de quantiles de la loi
    normale que $z_{0,10} = 1,282$, d'où $c = 16,41$.
  
\end{solution}
\begin{solution}{2.2}
    On a $\esp{\bar{X}_n} = \esp{X} = \mu$, $\var{\bar{X}_n} = \var{X}/n =
    100/n$ et $\bar{X}_n\sim \mathcal{N}(\mu, 100/n)$. Ainsi, on cherche $n$ tel
    que
    \begin{align*}
      \prob{\mu - 5 < \bar{X}_n< \mu + 5}
      &= \Prob{-\frac{5}{10/\sqrt{n}} < %
        \frac{\bar{X}_n- \mu}{10/\sqrt{n}} < %
        \frac{5}{10/\sqrt{n}}} \\
      &= \Phi\left( \frac{5 \sqrt{n}}{10} \right)
      - \Phi\left( -\frac{5 \sqrt{n}}{10} \right) \\
      &= 2 \Phi\left( \frac{5\sqrt{n}}{10} \right) - 1 \\
      &= 0,954,
    \end{align*}
    soit
    \begin{equation*}
     \Phi \left( \frac{5\sqrt{n}}{10} \right) = 0,977.
    \end{equation*}
    On trouve dans une table de loi normale que $5 \sqrt{n}/10 = 2$,
    d'où $n = 16$.
  
\end{solution}
\begin{solution}{2.3}
    On a $\bar{X}_{25} \sim \mathcal{N}(0, 16/25)$, $\bar{Y}_{25} \sim \mathcal{N}(1, 9/25)$ et, par
    conséquent, $\bar{X}_{25} - \bar{Y}_{25} \sim \mathcal{N}(-1, 1)$. On a donc
    \begin{align*}
      \prob{\bar{X}_{25} > \bar{Y}_{25}}
      &= \prob{\bar{X}_{25} - \bar{Y}_{25} > 0} \\
      &= \Prob{\frac{\bar{X}_{25}-\bar{Y}_{25} - (-1)}{\sqrt{1}} > %
        \frac{0 - (-1)}{\sqrt{1}}} \\
      &= 1 - \Phi(1)\\
      &= 0,159.
    \end{align*}
  
\end{solution}
\begin{solution}{2.4}
\begin{enumerate}
\item La statistique $W = Y_1^2 + \cdots + Y^2_7$ suit une loi khi carré avec $7$ degrés de liberté, $\chi^2_{(7)}$, puisque $W$ est une somme de sept variables \textbf{indépendantes}, chacune d'entre elle étant le carré d'une variable aléatoire normale centrée réduite.

\item Si $Y_1, \ldots , Y_n$ forme un échantillon aléatoire tiré d'une $\mathcal{N}(\mu, \sigma^2)$, alors
$$
\frac{(n-1)S^2_n}{\sigma^2} = \frac{1}{\sigma^2} \sum_{i=1}^n (Y_i - {\bar Y}_n)^2
$$
a une distribution khi carré avec $n-1$ degrés de liberté, $\chi^2_{(n-1)}$. Si on pose $n=7$ et $\sigma^2 =1$, on trouve que
$$
U = \sum_{i=1}^7 (Y_i - {\bar Y})^2 \sim \chi^2_{(6)}.
$$

\item Selon (b), $U$ a une distribution $\chi^2_{(6)}$. Cela signifie que la somme a la même distribution que $Z_1^2 + \cdots + Z_6^2$, où $Z_1, \ldots , Z_6$ sont iid $\mathcal{N}(0,1)$ et indépendantes de $Y_8$, qui suit aussi une $\mathcal{N}(0,1)$. Ainsi,
$$
Y_8^2 +U \sim \chi^2_{(7)}.
$$

\item
D'abord, on observe que $\sqrt{7} \, Y_8 /\sqrt{W} = Y_8/\sqrt{W/7}$, où $Y_8 \sim \mathcal{N}(0,1)$ et $W \sim \chi^2_{(7)}$ selon a). De plus, $W$ et $Y_8$ sont des variables aléatoires indépendantes. Ainsi, $Y_8/\sqrt{W/7}$ suit une loi Student $t_{(7)}$.

\item On note que $U \sim \chi^2_{(6)}$ selon b). Ensuite, $U$ est indépendant de $Y_8 \sim \mathcal{N}(0,1)$. Ainsi,
$$
\frac{\sqrt{6}Y_8}{\sqrt{U}} = \frac{Y_8}{\sqrt{U/6}} \sim t_{(6)}.
$$

\item On sait que la moyenne échantillonnale ${\bar Y}$ est indépendante de la variance échantillonnale $U/6$. Donc, ${\bar Y}$ et $Y_8$ sont toutes deux indépendantes de $U$. Puisque ${\bar Y} \sim \mathcal{N}(0,1/7)$, on a $\sqrt{7} \, {\bar Y} \sim \mathcal{N}(0,1)$ et ainsi
$$
7{\bar Y}^2 + Y^2_8 = (\sqrt{7} \, {\bar Y})^2 + Y^2_8 \sim \chi^2_{(2)}.
$$
De plus, $U \sim \chi^2_{(6)}$ avec (b). Il en découle que
$$
\frac{3(7{\bar Y}^2 + Y^2_8)}{U} = \frac{\displaystyle \frac{(\sqrt{7} \, {\bar Y})^2+Y^2_8}{2}}{U/6} \sim \mathcal{F}_{(2,6)}.
$$
\end{enumerate}
\end{solution}
\begin{solution}{2.5}
    On sait que $(n-1)S^2_n/\sigma^2 \sim \chi^2(n - 1)$, que l'espérance
    d'une loi $\chi^2$ est égale à son nombre de degrés de liberté et
    que sa variance est égale à deux fois son nombre de degrés de
    liberté. On a donc
    \begin{align*}
      \esp{S^2_n}
      &= \frac{\sigma^2}{(n-1)}\, \Esp{\frac{(n-1)S^2_n}{\sigma^2}}\\\
      &= \frac{(n - 1) \sigma^2}{(n-1)}=\sigma^2 \\
      \intertext{et}
      \var{S^2_n}
      &= \frac{\sigma^4}{(n-1)^2}\, \Var{\frac{(n-1)S^2_n}{\sigma^2}}\\
      &= \frac{2 (n - 1) \sigma^4}{(n-1)^2}= \frac{2 \sigma^4}{(n-1)}
    \end{align*}
  
\end{solution}
\begin{solution}{2.6}
    On sait que $5S^2_6/\sigma^2 \sim \chi^2(5)$. Soit $Y \sim
    \chi^2(5)$. On a donc,
    \begin{align*}
      \prob{2,30 < S^2_6 < 22,2}
      &= \Prob{\frac{5 (2,30)}{10} <
        \frac{5 S^2_6}{10} <
        \frac{5 (22,2)}{10}} \\
      &= \prob{1,15 < Y < 11,1}\\
      &= \prob{Y < 11,1} - \prob{Y <  1,15}.
    \end{align*}
    On trouve dans une table de quantiles de la loi khi~carré (ou avec
    la fonction \texttt{qchisq} dans \textsf{R}, par exemple) que
    $\prob{Y < 11,1} = 0,95$ et $\prob{Y < 1,15} = 0,05$. Par
    conséquent, $\prob{2,30 < S^2_6 < 22,2} = 0,90$.
  
\end{solution}
\begin{solution}{2.7}
On sait que $W=(n-1)S_n^2/\sigma^2\sim \chi^2_{(n-1)}$, et on cherche à trouver la fonction de densité de $S_n^2=\sigma^2 W /(n-1)$. Puisque c'est une transformation d'échelle, on peut utiliser la méthode de la fonction de répartition. D'abord, on exprime la fonction de répartition de $S_n^2$ en termes de la fonction de répartition de $W$:
\begin{align*}
F_{S_n^2}(s)&=\Pr[S_n^2\leq s]=\Pr\left[\frac{\sigma^2 W}{n-1}\leq s\right]=\Pr[W\leq s(n-1)/\sigma^2]=F_W\left(\frac{s(n-1)}{\sigma^2}\right).
\end{align*}
Ensuite, on dérive $F_{S_n^2}(s)$ en fonction de $s$ pour trouver la densité:
\begin{align*}
f_{S_n^2}(s) &=\frac{\d}{\d s}F_W\left(\frac{s(n-1)}{\sigma^2}\right) = f_W\left(\frac{s(n-1)}{\sigma^2}\right)\frac{n-1}{\sigma^2}.
\end{align*}
Finalement, la densité de $W$ est la distribution d'une $\chi^2_{n-1}$. Donc, pour $s>0$,
\begin{align*}
f_{S_n^2}(s) &= \frac{n-1}{\sigma^2}\frac{1}{2^{(n-1)/2}\Gamma(\frac{n-1}{2})}\left(\frac{(n-1)s}{\sigma^2}\right)^{(n-1)/2-1}\exp\left(-\frac{(n-1)s}{2\sigma^2}\right)\\
&=\left(\frac{n-1}{2\sigma^2}\right)^{(n-1)/2}\frac{1}{\Gamma(\frac{n-1}{2})}s^{(n-1)/2-1}\exp\left(-\frac{s}{2\sigma^2/(n-1)}\right).
\end{align*}
Par conséquent, $S_n^2$ suit une distribution gamma avec paramètres $\alpha=(n-1)/2$ et $\beta=2\sigma^2/(n-1)$. La moyenne et la variance sont:
\begin{align*}
\ex[S_n^2]&= \alpha\beta= \frac{n-1}{2}\frac{2\sigma^2}{n-1}=\sigma^2\\
\vr(S_n^2)&=\alpha\beta^2=\frac{n-1}{2}\frac{4\sigma^4}{(n-1)^2}=\frac{2\sigma^4}{n-1}.
\end{align*}
\end{solution}
\begin{solution}{2.8}
    On a $X = \mu + \sigma Z$. Par la technique de la fonction de
    répartition, on obtient:
    \begin{align*}
      F_X(x) &= \prob{X \leq x}\\
      &= \prob{\mu + \sigma Z \leq x}\\
      &= \Prob{Z \leq \frac{x - \mu}{\sigma}}\\
      &= \Phi\left(\frac{x-\mu}{\sigma}\right).
    \end{align*}
    Ainsi, la fonction de densité de probabilité de $X$ est
    \begin{displaymath}
      f_X(x) = \frac{d}{dx}\, \Phi\left(\frac{x - \mu}{\sigma}\right) =
      \frac{1}{\sigma}\, \phi\left(\frac{x-\mu}{\sigma}\right).
    \end{displaymath}
  
\end{solution}
\begin{solution}{2.9}
    Soit $Z \sim \mathcal{N}(0, 1)$. La fonction génératrice des moments de la
    variable aléatoire $Z^2$ est
    \begin{align*}
      M_{Z^2}(t) &= \Esp{e^{Z^2 t}} \\
      &= \int_{-\infty}^\infty e^{z^2 t} \phi(z)\, dz \\
      &= \int_{-\infty}^\infty
      \frac{1}{\sqrt{2\pi}}\, e^{z^2 t} e^{-z^2/2}\, dz\\
      &= \int_{-\infty}^\infty
      \frac{1}{\sqrt{2\pi}}\, e^{-z^2 (1 - 2t)/2}\, dz.
    \end{align*}
    En posant $\sigma^2 = (1 - 2t)^{-1}$, on voit que l'on peut écrire
    l'expression ci-dessus sous la forme
    \begin{equation*}
      M_{Z^2}(t) = \sigma \int_{-\infty}^\infty
      \frac{1}{\sigma \sqrt{2\pi}}\, e^{-z^2/(2 \sigma^2)}\,dz.
    \end{equation*}
    On reconnaît alors sous l'intégrale la densité d'une loi normale
    de moyenne $0$ et de variance $\sigma^2$. Par conséquent,
    \begin{equation*}
      M_{Z^2}(t) = \sigma = \left( \frac{1}{1 - 2t}\right)^{1/2},
    \end{equation*}
    soit la fonction génératrice des moments d'une loi gamma de
    paramètres $\alpha = 1/2 \text{ et } \beta = 2$ ou, de manière
    équivalente, d'une distribution $\chi^2(1)$.
  
\end{solution}
\begin{solution}{2.10}
    \begin{enumerate}
    \item On a $X \sim \mathcal{N}(0, \sigma^2)$ et $Y = X^2$. Il faut voir que
      $Y = X^2$ n'est pas une transformation bijective. On pose donc
      d'abord $Z = \abs{X}$ et on trouve la densité de $Z$ à l'aide de
      la technique de la fonction de répartition:
      \begin{align*}
        F_Z(z) &= \prob{\abs{X}\leq z}\\
        &= \prob{-z \leq X \leq z}\\
        &= F_X(z) - F_X(-z) \\
        \intertext{d'où}
        f_Z(z) &= f_X(z) + f_X(-z)\\
        &= \frac{2}{\sigma \sqrt{2\pi}}\, e^{-x^2/(2 \sigma^2)},
        \quad z > 0.
      \end{align*}
      Ensuite, on pose la transformation bijective $Y = Z^2 =
      \abs{X}^2 = X^2$. Par la technique du changement de variable, on
      a
      \begin{align*}
        f_Y(y) &= f_Z(y^{1/2}) \left| \frac{1}{2\sqrt{y}} \right| \\
        &= \frac{2}{\sigma \sqrt{2\pi}}\,
        e^{-y/(2 \sigma^2)} \left( \frac{1}{2\sqrt{y}} \right) \\
        &= \frac{(2 \sigma^2)^{-1/2}}{\Gamma(\frac{1}{2})}\,
        y^{1/2 - 1} e^{-y/(2 \sigma^2)}, \quad y > 0
      \end{align*}
      puisque $\Gamma(\frac{1}{2}) = \sqrt{\pi}$. On voit donc que
      \begin{equation*}
        Y \sim \text{Gamma}\left(\frac{1}{2}, 2\sigma^2\right).
      \end{equation*}
    \item On sait que $Z = X_1 - X_2 \sim \mathcal{N}(0, 2)$ et que $Z/\sqrt{2}
      \sim \mathcal{N}(0, 1)$. En utilisant le résultat de la partie a), on a
      immédiatement que $Y = Z^2/2 \sim \chi^2(1)$.
    \end{enumerate}
  
\end{solution}
\begin{solution}{2.11}
    \begin{enumerate}
    \item Puisque la loi $t$ est symétrique autour de zéro, on a
      \begin{align*}
        \prob{\abs{T} > 2,228}
        &= \prob{T > 2,228} + \prob{T < -2,228} \\
        &= 2 \prob{T > 2,228}.
      \end{align*}
      Or, on trouve dans la table de la loi $t$ de
      l'annexe~\ref{chap:t} que $\prob{T \leq 2,228} = 0,975$ si $T
      \sim t(10)$. Par conséquent, $\prob{\abs{T} > 2,228} = 2 (1 -
      0,975) = 0,05$.
    \item Toutes les fonction \textsf{R} servant à évaluer des
      fonctions de répartition ont un argument \texttt{lower.tail}. Ce
      argument est \texttt{TRUE} par défaut, mais lorsque qu'il est
      \texttt{FALSE}, la fonction retourne la probabilité
      \emph{au-dessus} du point \texttt{x}. Ainsi, la probabilité
      cherchée ici est
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlnum{2} \hlopt{*} \hlkwd{pt}\hlstd{(}\hlnum{2.228}\hlstd{,} \hlnum{10}\hlstd{,} \hlkwc{lower.tail} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.05001177
\end{verbatim}
\end{kframe}
\end{knitrout}
      Il est recommandé d'utiliser cette approche parce qu'elle est,
      de manière générale, plus précise que le calcul du type
      \verb|1 - pt(x, n)|, surtout loin dans les queues des
      distributions.
    \end{enumerate}
  
\end{solution}
\begin{solution}{2.12}
    \begin{enumerate}
    \item Par symétrie de la loi $t$,
      \begin{align*}
        \prob{-b < T < b}
        &= \prob{T < b} - \prob{T < -b} \\
        &= \prob{T < b} - (1 - \prob{T < b}) \\
        &= 2 \prob{T < b} - 1 \\
        &= 0,90.
      \end{align*}
      On cherche donc la valeur de $b$ tel que $\prob{T < b} = (1 +
      0,90)/2 = 0,95$, où $T \sim t(14)$. Dans la table de la loi $t$
      de l'annexe~\ref{chap:t} on trouve que $b = 1,761$.
    \item En définitive, on cherche le 95{\ieme} centile d'une loi
      $t(14)$. Avec \textsf{R}, on obtient
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qt}\hlstd{(}\hlnum{0.95}\hlstd{,} \hlnum{14}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 1.76131
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{enumerate}
  
\end{solution}
\begin{solution}{2.13}
    \begin{enumerate}
    \item On a $U \sim \chi^2(r_1)$ et $V \sim \chi^2(r_2)$. On pose
      $F = (U/r_1)/(V/r_2)$ et, disons, $G = V$. Pour trouver la
      densité (marginale) de $F$, il faudra passer par la densité
      conjointe de $F$ et $G$.

      Les équations régissant la transformation de variables
      aléatoires sont
      \begin{align*}
        x &= \frac{r_2}{r_1}\left(\frac{u}{v}\right) &
        u &= \frac{r_1}{r_2}\, x y \\
        y &= v &
        v &= y.
      \end{align*}
      Ainsi, le jacobien de la transformation est
      \begin{displaymath}
        J =
        \begin{vmatrix}
          r_1 y/r_2 & r_1 x/r_2 \\
          0         & 1
        \end{vmatrix}
        = \frac{r_1}{r_2}\, y
      \end{displaymath}
      et la densité conjointe de $F$ et $G$ est
      \begin{align*}
        f_{FG}(x, y)
        &= f_{UV}\left(\frac{r_1}{r_2}\, xy, y\right)
        \left|\frac{r_1}{r_2}\, y\right| \\
        &= \left(\frac{r_1}{r_2}\right) y
        f_U\left(\frac{r_1}{r_2}\, xy\right) f_V(y) \\
        &= \frac{%
          (r_1/r_2)
          (1/2)^{(r_1 + r_2)/2}
          (r_1 xy/r_2)^{r_1/2 - 1}
          y^{r_2/2}
          e^{-(r_1 x/r_2 + 1) y/2}}{%
          \Gamma(r_1/2) \Gamma(r_2/2)}
      \end{align*}
      pour $x > 0$ et $y > 0$. En intégrant, on trouve la densité
      marginale de $F$:
      \begin{align*}
        f_F(x) &= \int_0^{\infty} f_{FG}(x, y)\, dy \\
        &= \frac{(r_1/r_2)^{(r_1 + r_2)/2} x^{r_1/2 - 1}}{%
          \Gamma(r_1/2) \Gamma(r_2/2)} \\
        &\phantom{=} \times
        \int_0^\infty
        \left(\frac{1}{2}\right)^{(r_1 + r_2)/2}
        y^{(r_1 + r_2)/2 - 1}
        e^{-(r_1 x/r_2 + 1) y/2}\, dy \\
        &= \frac{\Gamma((r_1 + r_2)/2) (r_1/r_2)^{r_1/2} x^{r_1/2 - 1}}{%
          \Gamma(r_1/2) \Gamma(r_2/2) (r_1 x/r_2 + 1)^{(r_1 + r_2)/2}} \\
        &\phantom{=} \times
        \int_0^\infty
        \frac{(1/2)^{(r_1 + r_2)/2} (r_1 x/r_2 + 1)^{(r_1 + r_2)/2}}{%
          \Gamma((r_1 + r_2)/2)}\,
        y^{(r_1 + r_2)/2 - 1}
        e^{-(r_1 x/r_2 + 1) y/2}\, dy \\
        &= \frac{\Gamma((r_1 + r_2)/2) (r_1/r_2)^{r_1/2} x^{r_1/2 - 1}}{%
        \Gamma(r_1/2) \Gamma(r_2/2) (1 + r_1 x/r_2)^{(r_1 + r_2)/2}},
      \end{align*}
      puisque l'intégrande ci-dessus est la densité d'une loi gamma.
      La loi de la variable aléatoire $F$ est appelée loi $F$ avec
      $r_1$ et $r_2$ degrés de liberté.
    \item Par indépendance entre les variables aléatoires $U$ et $V$,
      on a
      \begin{align*}
        \esp{F} &= \Esp{\frac{U/r_1}{V/r_2}} \\
        &= \frac{r_2}{r_1}\, \Esp{\frac{U}{V}} \\
        &= \frac{r_2}{r_1}\, \esp{U} \Esp{\frac{1}{V}}.
      \end{align*}
      Or, $\esp{U} = r_1$ et
      \begin{align*}
        \Esp{\frac{1}{V}}
        &= \int_0^\infty \frac{1}{v}\, f_V(v)\, dv \\
        &= \int_0^\infty
        \frac{1}{2^{r_2/2} \Gamma(r_2/2)}\,
        v^{r_2/2 - 1 - 1} e^{-v/2}\, dv \\
        &= \frac{2^{r_2/2 - 1} \Gamma(r_2/2 - 1)}{%
          2^{r_2/2} \Gamma(r_2/2)}, \quad
        \frac{r_2}{2} - 1 > 0.
      \end{align*}
      Avec la propriété de la fonction gamma $\Gamma(x) = (x - 1)
      \Gamma(x - 1)$, cette expression se simplifie en
      \begin{equation*}
        \Esp{\frac{1}{V}} = \frac{1}{r_2 - 2},
      \end{equation*}
      d'où, enfin,
      \begin{displaymath}
        \esp{F} = \frac{r_2}{r_2 - 2}
      \end{displaymath}
      pour $r_2 > 2$.
    \item En procédant comme en b), on trouve que $\esp{U^2} = \var{U}
      + \esp{U}^2 = 2 r_1 + r_1^2$, que
      \begin{displaymath}
        \Esp{\frac{1}{V^2}} = \frac{1}{(r_2 - 2)(r_2 - 4)}
      \end{displaymath}
      et donc que
      \begin{align*}
        \esp{F^2}
        &= \frac{r_2^2}{r_1^2}\, \esp{U^2} \Esp{\frac{1}{V^2}} \\
        &= \frac{r_2^2 (r_1 + 2)}{r_1 (r_2 - 2)(r_2 - 4)}.
      \end{align*}
      Par conséquent,
      \begin{align*}
        \var{F}
        &= \frac{r_2^2 (r_1 + 2)}{r_1 (r_2 - 2)(r_2 - 4)} -
        \left(\frac{r_2}{r_2 - 2}\right)^2 \\
        &= 2 \left(\frac{r_2}{r_2 - 2}\right)^2
        \left(\frac{r_2 + r_1 - 2}{r_1 (r_2 - 4)}\right),
      \end{align*}
      pour $r_2 > 4$.
    \end{enumerate}
  
\end{solution}
\begin{solution}{2.14}
    On a
    \begin{align*}
      \frac{1}{F} &=
      \left(\frac{U/\nu_1}{V/\nu_2}\right)^{-1} \\
      &=\frac{V/\nu_2}{U/\nu_1}
    \end{align*}
    où $U \sim \chi^2(\nu_1)$ et $V \sim \chi^2(\nu_2)$. Puisqu'il s'agit
    d'un ratio de deux variables aléatoires $\chi^2$ divisées chacune
    par son nombre de degrés de liberté, on a donc que
    \begin{displaymath}
      \frac{1}{F} \sim F(\nu_2, \nu_1).
    \end{displaymath}
  
\end{solution}
\begin{solution}{2.15}
    On a $F \sim F(5, 10)$ et l'on cherche $a$ et $b$ tel que $\prob{F
      \leq a} = 0,05$ et $\prob{F \leq b} = 0,95$. Dans une table de
    loi $F$, on trouve que $\prob{F \leq 3,326} = 0,95$ et donc que $b
    = 3,33$. Puisque les quantiles inférieurs ne sont pas inclus dans
    la table de l'annexe~\ref{chap:F}, on doit utiliser pour trouver
    $a$ la relation $\prob{F \leq a} = 1 - \prob{F^{-1} \leq a^{-1}}$
    où, tel que démontré à
    l'exercice~\ref{chap:echantillon}.\ref{ex:transformations:Finverse},
    $F^{-1} \sim F(10, 5)$. Dans une table, on trouve que $a^{-1} =
    4,74$, d'où $a = 0,211$.

    Avec \textsf{R}, on obtient les mêmes résultats encore plus
    simplement:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qf}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0.05}\hlstd{,} \hlnum{0.95}\hlstd{),} \hlnum{5}\hlstd{,} \hlnum{10}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.2111904 3.3258345
\end{verbatim}
\end{kframe}
\end{knitrout}
  
\end{solution}
\begin{solution}{2.16}
    On sait que $W^2 \sim \chi^2(1)$. Ainsi,
    \begin{displaymath}
      T^2 = \frac{W^2/1}{V/r},
    \end{displaymath}
    qui est un ratio de deux variables aléatoires $\chi^2$ divisées
    par leur nombre de degrés de liberté. Par définition de la loi
    $F$, on a donc que $T^2 \sim F(1, r)$.
  
\end{solution}
\begin{solution}{2.17}
    Soit
    \begin{equation*}
      Y = \sum_{i = 1}^\alpha X_i
    \end{equation*}
    avec $X_i \sim \text{Exponentielle}(\beta)$ et $X_1, \dots,
    X_\alpha$ indépendantes. Par le Théorème central limite,
    \begin{align*}
      \lim_{\alpha \rightarrow \infty} Y =
      \lim_{\alpha  \rightarrow \infty} \sum_{i = 1}^\alpha X_i \sim
      \mathcal{N}(\alpha \esp{X_i}, \alpha \var{X_i}).
    \end{align*}
    Par conséquent,
    \begin{equation*}
      \lim_{\alpha \rightarrow \infty} Y \sim
      N\left(\alpha\beta, \alpha\beta^2\right).
    \end{equation*}
    On trouve à la figure~\ref{fig:echantillon:gammas} les graphiques
    de densités gamma pour quelques valeurs du paramètre $\alpha$. On
    observe, en effet, que la distribution tend vers une normale
    lorsque $\alpha$ augmente.
    \begin{figure}
      \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=0.6\textwidth]{figure/unnamed-chunk-13-1}

\end{knitrout}
      \caption{Densités de lois gamma pour quelques valeurs du
        paramètre de forme $\alpha$.}
      \label{fig:echantillon:gammas}
    \end{figure}
  
\end{solution}
\begin{solution}{2.18}
    \begin{enumerate}
    \item On a l'échantillon aléatoire $X_1, \dots, X_{100}$, où
      \begin{equation*}
        X_i \sim \chi^2(50) \equiv
        \text{Gamma}\left( \frac{50}{2},  2 \right), \quad
        i = 1, \dots, 100.
      \end{equation*}
      Or, on sait que $Y = X_1 + \dots + X_{100} \sim \text{Gamma}(100
      (25), 2)$ et que $\bar{X}_{100} = Y/100 \sim
      \text{Gamma}(\nombre{2500}, 2/100)$.
    \item On peut, par exemple, obtenir la probabilité demandée avec
      \textsf{R} ainsi:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pgamma}\hlstd{(}\hlnum{51}\hlstd{,} \hlnum{2500}\hlstd{,} \hlnum{50}\hlstd{)} \hlopt{-} \hlkwd{pgamma}\hlstd{(}\hlnum{49}\hlstd{,} \hlnum{2500}\hlstd{,} \hlnum{50}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.6827218
\end{verbatim}
\end{kframe}
\end{knitrout}
    \item On a $\esp{\bar{X}_{100}} = \nombre{2500}/50 = 50$ et
      $\var{\bar{X}_{100}} = \nombre{2500}/50^2 = 1$. En utilisant
      l'approximation normale, on trouve
      \begin{align*}
        \prob{49 < \bar{X}_{100} < 51}
        &= \Prob{\frac{49 - 50}{1} <
          \frac{\bar{X}_{100} - 50}{1} <
          \frac{51 - 50}{1}} \\
        &\approx \Phi(1) - \Phi(-1) \\
        &= 2\Phi(1) - 1 \\
        &= 0,682.
      \end{align*}
      On trouve la valeur de $\Phi(1)$ dans une table de quantiles de
      la loi normale ou à l'aide d'un logiciel statistique.
    \end{enumerate}
  
\end{solution}
\begin{solution}{2.19}
    Puique l'on ne demande qu'une valeur approximative pour $\prob{7 <
      \bar{X} < 9}$, on va utiliser l'approximation normale. La taille
    de l'échantillon étant relativement grande, l'approximation sera
    très bonne. Soit $\bar{X} = (X_1 + \dots + X_{128})/128$, où $X_i
    \sim \text{Gamma}(2, 4)$, $i = 1, \dots, 128$. On a
    $\esp{\bar{X}} = \esp{X_i} = 8$ et $\var{\bar{X}} = \var{X_i}/128
    = 1/4$. Par conséquent,
    \begin{align*}
      \prob{7 < \bar{X} < 9}
        &= \Prob{\frac{7 - 8}{\sqrt{1/4}} <
          \frac{\bar{X} - 8}{\sqrt{1/4}} <
          \frac{9 - 8}{\sqrt{1/4}}} \\
      &\approx \Phi(2) - \Phi(-2) \\
      &= 2 \Phi(2) - 1 \\
      &= 0,954.
    \end{align*}
    On trouve la valeur de $\Phi(2)$ dans une table de quantiles de
    la loi normale ou à l'aide d'un logiciel statistique.
  
\end{solution}
\begin{solution}{2.20}
    On souhaitera utiliser l'approximation normale. Cela requiert de
    connaître les valeurs de l'espérance et de la variance de la
    moyenne de l'échantillon, $\bar{X}$, et, par ricochet, celles de
    la variable aléatoire avec densité $f(x)$. Or,
    \begin{align*}
      \esp{X} &= \int_0^1 3x^3\,dx = \frac{3}{4} \\
      \esp{X^2} &= \int_0^1 3x^4\,dx = \frac{3}{5}
    \end{align*}
    et donc $\var{X} = 3/80$. Ainsi, $\esp{\bar{X}} = \esp{X} = 3/4$
    et $\var{\bar{X}} = \var{X}/15 = 1/400$ et
    \begin{align*}
      \Prob{\frac{3}{5} < \bar{X} < \frac{4}{5}}
      &= \Prob{\frac{3/5 - 3/4}{\sqrt{1/400}} <
        \frac{\bar{X} - 3/4}{\sqrt{1/400}} <
        \frac{4/5 - 3/4}{\sqrt{1/400}}} \\
      &\approx \Phi(1) - \Phi(-3) \\
      &= 0,840.
    \end{align*}
  
\end{solution}
\begin{solution}{2.21}
Pour chaque $i \in \{1,\dots, n \}$, on pose $D_i = X_i - Y_i$. $D_1, \ldots, D_n$ sont mutuellement indépendants et identiquement distribués avec moyenne
$$
\ex(D_1) = \ex(X_1) - \ex(Y_1) = \mu_1 - \mu_2
$$
et variance
$$
\vr(D_1) = \vr(X_1) + \vr(Y_1) = \sigma_1^2 + \sigma_2^2.
$$
Par le Théorème central limite, on a donc que, quand $n \to \infty$,
$$
\sqrt{n} \, \frac{\bar D_n - \ex[D_1]}{\sqrt{\vr(D_1)}} = \frac{(\bar X_n - \bar Y_n) - (\mu_1 -\mu_2)}{\sqrt{(\sigma_1^2 + \sigma_2^2)/n}} = Z_n
$$
converge en distribution vers $\mathcal{N}(0,1)$.
\end{solution}
\begin{solution}{2.22}
\begin{enumerate}
\item La probabilité peut être calculée comme suit:
\begin{align*}
\Pr(|\bar X_n - \mu| \le 0,2) & = \Pr(-0,2 \le \bar X_n - \mu \le 0,2) \\
& = \Pr\left(-\frac{0,2}{\sqrt{2,5^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \le \frac{0,2}{\sqrt{2,5^2/9}} \right).
\end{align*}
On sait que
$$
\frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \sim \mathcal{N}(0,1),
$$
la probabilité est alors
\begin{align*}
\Pr\left(-\frac{0,2}{\sqrt{2,5^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \le \frac{0,2}{\sqrt{2,5^2/9}} \right) &= \Phi\left(\frac{0,2}{\sqrt{2,5^2/9}}\right)-\Phi\left(-\frac{0,2}{\sqrt{2,5^2/9}}\right)\\
& = 2\Phi\left(\frac{0,2}{\sqrt{2,5^2/9}}\right)-1,
\end{align*}
où $\Phi$ représente la fonction de répartition de la loi normale centrée réduite. Cette probabilité peut être évaluée avec une table de la loi normale ou avec \textsf{R}; ce dernier donne
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlnum{2}\hlopt{*}\hlkwd{pnorm}\hlstd{(}\hlnum{0.2}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{2.5}\hlopt{^}\hlnum{2}\hlopt{/}\hlnum{9}\hlstd{))}\hlopt{-}\hlnum{1}
\end{alltt}
\begin{verbatim}
## [1] 0.1896697
\end{verbatim}
\end{kframe}
\end{knitrout}

\item De la même façon qu'en a), on trouve
\begin{align*}
\Pr(|\bar X_n - \mu| \le 0,2)& =\Pr\left(-\frac{0,2}{\sqrt{2,5^2/n}} \le \frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \le \frac{0,2}{\sqrt{2,5^2/n}} \right)\\
& = 2\Phi\left(\frac{0,2}{\sqrt{2,5^2/n}}\right)-1.
\end{align*}
On considère $n=25$, $n=36$ et $n=64$ dans la formule, ce qui donne
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{25}\hlstd{,}\hlnum{36}\hlstd{,}\hlnum{64}\hlstd{)}
\hlnum{2}\hlopt{*}\hlkwd{pnorm}\hlstd{(}\hlnum{0.2}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{2.5}\hlopt{^}\hlnum{2}\hlopt{/}\hlstd{n))}\hlopt{-}\hlnum{1}
\end{alltt}
\begin{verbatim}
## [1] 0.3108435 0.3687726 0.4778274
\end{verbatim}
\end{kframe}
\end{knitrout}
On remarque que la probabilité que la moyenne échantillonnale diffère de la vraie moyenne d'au plus 0,2 once augmente avec $n$. C'est le résultat qu'on attend selon la Loi faible des grands nombres, qui indique que cette probabilité tend vers $1$ quand $n\to \infty$.

\item Si
$$
\Pr ( | {\bar X}_n - \mu | \le 0,2) = 2\Phi\left(\frac{0,2}{\sqrt{2,5^2/n}}\right)-1 \geq 0,95,
$$
alors
$\Phi \left(0,2\sqrt{n}/2,5\right) \geq 0,975$ et ainsi $0,2\sqrt{n}/2,5 \geq 1,96$, ce qui implique que $\sqrt{n} \geq 24,5$ ou $n\geq 600,25$. La taille d'échantillon doit donc être de $n=601$ pour que la probabilité soit \emph{le plus près possible, mais pas plus petite que} 0,95.

\item Dans ce cas, la probabilité à calculer est
$$
 \Pr\left(-\frac{0,2}{\sqrt{s_n^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{S_n^2/9}} \le \frac{0,2}{\sqrt{s_n^2/9}} \right).
$$
On sait que
$$
\frac{\bar X_n - \mu}{\sqrt{S_n^2/9}} \sim t_{(8)},
$$
on trouve que
\begin{align*}
 \Pr\left(-\frac{0,2}{\sqrt{s_n^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{S_n^2/9}} \le \frac{0,2}{\sqrt{s_n^2/9}} \right) & =  \Pr\left(-\frac{0,2}{\sqrt{5,5/9}} \le \frac{\bar X_n - \mu}{\sqrt{5,5/9}} \le \frac{0,2}{\sqrt{5,5/9}} \right) \\
 &= T_{(8)}\left(\frac{0,2}{\sqrt{5,5/9}} \right) - T_{(8)} \left(-\frac{0,2}{\sqrt{5,5/9}} \right),
\end{align*}
où $T_{(8)}$ est la fonction de répartition d'une loi Student $t$ avec $8$ degrés de liberté. Encore une fois, cette expression peut être évaluée avec une table de la loi normale ou avec \textsf{R}; ce dernier donne
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pt}\hlstd{(}\hlnum{0.2}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{5.5}\hlopt{/}\hlnum{9}\hlstd{),}\hlkwc{df}\hlstd{=}\hlnum{8}\hlstd{)} \hlopt{-}\hlkwd{pt}\hlstd{(}\hlopt{-}\hlnum{0.2}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{5.5}\hlopt{/}\hlnum{9}\hlstd{),}\hlkwc{df}\hlstd{=}\hlnum{8}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.1954708
\end{verbatim}
\end{kframe}
\end{knitrout}
La probabilité obtenue est plus grande qu'en a) étant donné la variabilité ajoutée avec l'estimation de la variance.
\end{enumerate}
\end{solution}
\begin{solution}{2.23}
\begin{enumerate}
\item On peut utiliser le fait que
$$
\frac{S_n^2/\sigma_1^2}{S_m^2/\sigma_2^2} \sim F_{(n-1,m-1)},
$$
où $\sigma^2_1$ et $\sigma_2^2$ représentent la variance du premier et du second échantillon, respectivement. Dans ce contexte, $n=10$, $m=6$ et $\sigma_1^2 = 2\sigma_2^2$. Donc,
$$
\frac{S_n^2}{2 S_m^2} \sim F(9,5).
$$
Si $W$ représente une variable aléatoire $F_{(9,5)}$, on a
$$
\Pr\left(  \frac{S_n^2}{ S_m^2} \le b\right) = \Pr(W \le b/2)
$$
et donc $b/2$ est le $95$e quantile de la distribution $F_{(9,5)}$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qf}\hlstd{(}\hlnum{0.95}\hlstd{,}\hlkwc{df1}\hlstd{=}\hlnum{9}\hlstd{,}\hlkwc{df2}\hlstd{=}\hlnum{5}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 4.772466
\end{verbatim}
\end{kframe}
\end{knitrout}
Par conséquent,
$$
b = 2 \times 4.7724656 =  9.5449312.
$$
Pour trouver $a$, on note que
$$
\frac{S_m^2/\sigma_2^2}{S_n^2/\sigma_1^2} = 2 \frac{S_m^2}{ S_n^2} \sim F_{(5,9)}.
$$
Alors,
$$
\Pr\left(  \frac{S_n^2}{ S_m^2} \ge a\right) = \Pr\left(  \frac{S_m^2}{ S_n^2} \le \frac{1}{a}\right) = \Pr\left(  2\frac{S_m^2}{ S_n^2} \le \frac{2}{a}\right).
$$
Donc, $2/a$ est le $95$e quantile de la distribution $F_{(5,9)}$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qf}\hlstd{(}\hlnum{0.95}\hlstd{,}\hlkwc{df1}\hlstd{=}\hlnum{5}\hlstd{,}\hlkwc{df2}\hlstd{=}\hlnum{9}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 3.481659
\end{verbatim}
\end{kframe}
\end{knitrout}
et ainsi
$$
a = \frac{2}{3.4816587} = 0.5744389.
$$
\item Cette probabilité égale
\begin{align*}
\Pr\left(a \le \frac{S_n^2}{S_m^2}\le b\right) & = \Pr\left(\frac{S_n^2}{S_m^2}\le b\right) - \Pr\left(\frac{S_n^2}{S_m^2}\le a\right)\\
& = \Pr\left(\frac{S_n^2}{S_m^2}\le b\right) + \Pr\left(\frac{S_n^2}{S_m^2}\ge a\right) -1 = 2\times 0,95 -1 = 0,9.
\end{align*}
\end{enumerate}
\end{solution}
