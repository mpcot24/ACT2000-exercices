\documentclass[letterpaper,11pt]{memoir}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{natbib,url}
  \usepackage[english,francais]{babel}
  \usepackage[autolanguage]{numprint}

  \usepackage{vgmath,vgsets,amsmath,icomma}
  \usepackage{lucidabr,pslatex}
  \usepackage[sc]{mathpazo}
  \usepackage{graphicx,color}
  \usepackage[absolute]{textpos}
  \usepackage{answers}
  \usepackage[alwaysadjust,defblank]{paralist}

  \usepackage{rotating,lscape} % table de loi F

  %%% Hyperliens
  \usepackage{hyperref}
  \definecolor{link}{rgb}{0,0,0.3}
  \hypersetup{
    pdftex,
    colorlinks,%
    citecolor=link,%
    filecolor=link,%
    linkcolor=link,%
    urlcolor=link}

  %%% Page titre
  \title{\HUGE
    \fontseries{ub}\selectfont Analyse statistique des risques actuariels \\[0.5\baselineskip]
    \huge\fontseries{m}\selectfont Exercices et solutions}
  \author{\LARGE Marie-Pier C\^ot\'e \\[3mm]
    \large École d'actuariat \\ Université Laval}
  \date{Première édition préliminaire}
  \newcommand{\ISBN}{}

  %%% Sous-figures
%  \newsubfloat{figure}

%% MP: marges plus larges
\setlrmarginsandblock{3.5cm}{3cm}{*}
\setulmarginsandblock{3.5cm}{3cm}{*}
\checkandfixthelayout


  %%% Style des entêtes de chapitres
  \chapterstyle{hangnum}

  %%% Styles des entêtes et pieds de page 
  \setlength{\marginparsep}{1mm}
  \setlength{\marginparwidth}{1mm}
  \setlength{\headwidth}{\textwidth}
  \addtolength{\headwidth}{\marginparsep}
  \addtolength{\headwidth}{\marginparwidth}

  %%% Style de la bibliographie
  %\bibliographystyle{francais}
   \bibliographystyle{plain}

  %%% Options de babel
  \frenchbsetup{CompactItemize=false,%
    ThinSpaceInFrenchNumbers=true}
  \addto\captionsfrench{\def\tablename{{\scshape Tab.}}}
  \addto\captionsfrench{\def\figurename{{\scshape Fig.}}}

  %%% Associations entre les environnements et les fichiers
  \Newassociation{sol}{solution}{solutions}
  \Newassociation{rep}{reponse}{reponses}

  %%% Environnement pour les exercices
  \newcounter{exercice}[chapter]
  \newenvironment{exercice}{%
     \begin{list}{\bfseries \arabic{chapter}.\arabic{exercice}}{%
         \refstepcounter{exercice}
         \settowidth{\labelwidth}{\bfseries \arabic{chapter}.\arabic{exercice}}
         \setlength{\leftmargin}{\labelwidth}
         \addtolength{\leftmargin}{\labelsep}
         \setdefaultenum{a)}{i)}{}{}}\item}
     {\end{list}}

  %%% Environnement pour les réponses
  \renewenvironment{reponse}[1]{%
    \begin{list}{\bfseries #1}{%
        \settowidth{\labelwidth}{#1}
        \setlength{\leftmargin}{\labelwidth}
        \addtolength{\leftmargin}{\labelsep}
        \setdefaultenum{a)}{i)}{}{}}\item}
    {\end{list}}
  \renewcommand{\reponseparams}{{\thechapter.\theexercice}}

  %%% Environnement pour les solutions
  \renewenvironment{solution}[1]{%
    \begin{list}{\bfseries #1}{%
        \settowidth{\labelwidth}{#1}
        \setlength{\leftmargin}{\labelwidth}
        \addtolength{\leftmargin}{\labelsep}
        \setdefaultenum{a)}{i)}{}{}}\item}
    {\end{list}}
  \renewcommand{\solutionparams}{{\thechapter.\theexercice}}

  %%% Nouvelles commandes
  \newcommand{\cov}[1]{\mathrm{cov} ( #1 )}
  \renewcommand{\Cov}[1]{\mathrm{cov}\! \left( #1 \right)}
  \newcommand{\prob}[1]{\mathrm{Pr} [ #1 ]}
  \newcommand{\Prob}[1]{\mathrm{Pr}\! \left[ #1 \right]}
  \newcommand{\MSE}{\mathrm{MSE}}
  
  \DeclareMathOperator{\sign}{sign}
  \DeclareMathOperator{\vr}{var}
  \DeclareMathOperator{\sd}{sd}
  \DeclareMathOperator{\ex}{E}
  \renewcommand{\d}{\mbox{d}}


  %%% Un petit peu d'aide pour la césure
  \hyphenation{con-fiance}
  \hyphenation{con-train-te}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\frontmatter

\pagestyle{empty}
\include{pagetitre}


\pagestyle{companion}

\include{introduction}
%\include{notation}

\cleardoublepage
\tableofcontents*

\mainmatter


%\include{rappels}


\chapter{Modèles statistiques de base}
\label{chap:base}

\Opensolutionfile{reponses}[reponses-base]
\Opensolutionfile{solutions}[solutions-base]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:base}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:base}}

\end{Filesave}




%%%
%%% Début des exercices
%%%

% La notion d'échantillon aléatoire

\begin{exercice} 
Est-ce que les énoncés suivants constituent des exemples d'échantillon aléatoire? Justifiez votre réponse.

\begin{enumerate}
\item Le nombre annuel de cas de cancer du sein causant un décès au Québec entre 1970 et 2018.
\item Le résultat de 20 lancers de dés non truqués lors d'une partie d'un jeu de société.
\end{enumerate}
\begin{rep}
a) non \, \, b) oui 
\end{rep}
\begin{sol}
\begin{enumerate}
\item Les progrès en médecine ont affecté grandement la probabilité de détection précoce du cancer du sein, donc la probabilité de survie. Il y a donc une tendance à la baisse, et ces données ne sont pas identiquement distribuées dans le temps.
\item Puisque chaque lancer est indépendant et identiquement distribué, les 20 lancers sont considérés comme un échantillon aléatoire de taille 20 de lancers de dé.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Dans chacun des cas suivants, identifiez la loi de probabilité qui serait la plus appropriée selon le contexte de l'énoncé. Justifiez votre choix et spécifiez quels paramètres sont connus et lesquels sont inconnus.

\begin{enumerate}
\item La perte financière du propriétaire d'une maison rasée par les flammes.
\item Le nombre de sacs de grains de café devant être examinés avant de trouver 15 sacs contaminés.
\item Le nombre de sacs de grains de café contaminés parmi 15 sacs examinés.
\item La vitesse réelle d'un véhicule à un endroit spécifique sur l'autoroute.
\item La véritable résistance d'un câble utilisé dans un ordinateur.
\end{enumerate}
\begin{sol}
\begin{enumerate}
\item Il faudrait utiliser une distribution continue non négative sur l'intervalle $[0,\infty)$ étant donné que la perte financière est nécessairement positive et qu'il n'y a pas de borne supérieure naturelle. Des bons exemples de lois pourraient être : gamma, lognormale, Pareto.

\item La distribution adéquate est la loi binomiale négative. Considérant que chaque sac est indépendant des autres et a une probabilité $p$ d'être contaminé, la distribution est une loi binomiale négative avec paramètres $r=15$ et $p$ inconnu.

\item La loi binomiale est appropriée. Considérant que chaque sac est indépendant des autres et a une probabilité $p$ d'être contaminé, le nombre total de sacs contaminés suit une loi binomiale avec paramètres de taille $n=15$ et probabilité $p$ inconnu.

\item Le contexte est conforme à une distribution normale. La majorité des conducteurs vont tendre vers une vitesse moyenne malgré que certains vont conduire beaucoup plus vite ou lentement~; une distribution en forme de cloche semble donc appropriée. La moyenne $\mu$ et la variance $\sigma^2$ de la vitesse réelle des véhicules à cet endroit précis sur l'autoroute sont inconnues.

\item Le contexte suggère une loi Normale. Les câbles devraient avoir une résistance près de $\mu$, soit celle donnée par le producteur. L'écart-type $\sigma$ dépendra de la qualité de la précision de la fabrication, et est inconnu.
\end{enumerate}
\end{sol}
\end{exercice}

% Variables aléatoires discrètes et continues

\begin{exercice}
  La distribution de Weibull est fréquemment utilisée en assurance
  IARD pour la modélisation des montants de sinistres, entre autres.
  Sa fonction de répartition est
  \begin{displaymath}
    F(x) = 1 - e^{-\beta x^\alpha}, \quad x > 0, \alpha > 0, \beta > 0.
  \end{displaymath}
  \begin{enumerate}
  \item Déterminer la fonction de densité de probabilité de la
    Weibull.
  \item Calculer l'espérance et la variance de la Weibull.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $f(x) = \alpha \beta x^{\alpha - 1}e^{-\beta x^{\alpha}}$
    \item $\esp{X} = \beta^{-1/\alpha} \Gamma(1 + 1/\alpha)$, %
      $\var{X} = \beta^{-2/\alpha} (\Gamma(1 + 2/\alpha) -
      \Gamma(1 + 1/\alpha)^2)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a directement $f(x) = F^\prime(x) = \alpha \beta x^{\alpha
        - 1} e^{-\beta x^{\alpha}}$.
    \item On a
      \begin{align*}
        \esp{X}
        &= \int_0^\infty x f(x)\, dx \\
        &= \int_0^\infty \alpha \beta x^{\alpha} e^{-\beta x^{\alpha}}\,
        dx.
      \end{align*}
      On effectue le changement de variable $y = \beta x^{\alpha}$,
      d'où $dy = \alpha \beta x^{\alpha - 1}\, dx$ et donc
      \begin{align*}
        \esp{X}
        &= \frac{1}{\beta^{1/\alpha}}
        \int_0^\infty y^{1/\alpha} e^{-y} \,dy \\
        &= \frac{\Gamma(1 + \frac{1}{\alpha})}{\beta^{1/\alpha}}
        \int_0^\infty \frac{1}{\Gamma(1 + \frac{1}{\alpha})}\,
        y^{1/\alpha} e^{-y} \,dy \\
        &= \frac{\Gamma(1 + \frac{1}{\alpha})}{\beta^{1/\alpha}}
      \end{align*}
      puisque l'intégrande ci-dessus est la fonction de densité de
      probabilité d'une loi gamma de paramètre de forme $\alpha = 1 +
      \frac{1}{\alpha}$ et de paramètre d'échelle $\beta = 1$. En
      procédant exactement de la même façon, on trouve
      \begin{align*}
        \esp{X^2}
        &= \int_0^\infty x^2 f(x)\, dx \\
        &= \int_0^\infty \alpha \beta x^{\alpha + 1} e^{-\beta x^{\alpha}}\,
        dx \\
        &= \frac{1}{\beta^{2/\alpha}}
        \int_0^\infty y^{2/\alpha} e^{-y} \, dy \\
        &= \frac{\Gamma(1 + \frac{2}{\alpha})}{\beta^{2/\alpha}}.
      \end{align*}
      Par conséquent,
      \begin{equation*}
        \var{X} = \frac{\Gamma(1 + \frac{2}{\alpha}) -
          \Gamma(1 + \frac{1}{\alpha})^2}{\beta^{2/\alpha}}.
      \end{equation*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
Obtenir l'expression de $\Esp{X}$ et $\Var{X}$ quand la variable aléatoire $X$ suit une distribution Beta$(\alpha,\beta)$. [Indices:
\begin{itemize}
\item [i.] Pour tout $\alpha > 0$, $\Gamma(\alpha+1) = \alpha\Gamma(\alpha)$.

\item [ii.] Si $f$ est une densité, alors $\int_\mathbb{R}f(x)\d x=1$.]
\end{itemize}
\begin{rep}
$\ex(X)=\alpha/(\alpha+\beta)$ et $\Var{X}=(\alpha\beta)/\{(\alpha+\beta)^2(\alpha+\beta+1)\}$
\end{rep}
\begin{sol}
Soit $X$ une variable aléatoire avec distribution Beta$(\alpha,\beta)$. Alors,
\begin{align*}
\ex(X) & = \int_0^1 x \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\d x\\
&=\int_0^1  \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha}(1-x)^{\beta-1}\d x.
\end{align*}
Cette intégrale peut être résolue en reconnaissant la forme similaire à une distribution Beta$(\alpha+1,\beta)$ intégrée sur son domaine, mais avec les mauvaises constantes. On divise et multiplie par les constantes nécessaires pour trouver une intégrale avec valeur de 1.
\begin{align*}
\ex(X) & =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)}\frac{\Gamma(\alpha+1)}{\Gamma(\alpha+1+\beta)} \int_0^1  \frac{\Gamma(\alpha+1+\beta)}{\Gamma(\alpha+1)\Gamma(\beta)}x^{\alpha}(1-x)^{\beta-1}\d x\\
&=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha+1+\beta)}\frac{\Gamma(\alpha+1)}{\Gamma(\alpha)}\\
&= \frac{\Gamma(\alpha+\beta)}{(\alpha+\beta)\Gamma(\alpha+\beta)}\frac{ \alpha \Gamma(\alpha) }{\Gamma(\alpha)}\\
&=\frac{\alpha}{\alpha+\beta}.
\end{align*}

De plus,
\begin{align*}
\ex\{X^2\} & = \int_0^1 x^2 \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha-1}(1-x)^{\beta-1}\d x\\
&=\int_0^1  \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha+1}(1-x)^{\beta-1}\d x\\
& =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)}\frac{\Gamma(\alpha+2)}{\Gamma(\alpha+2+\beta)} \int_0^1  \frac{\Gamma(\alpha+2+\beta)}{\Gamma(\alpha+2)\Gamma(\beta)}x^{\alpha}(1-x)^{\beta-1}\d x\\
&=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha+2+\beta)}\frac{\Gamma(\alpha+2)}{\Gamma(\alpha)}\\
&= \frac{\Gamma(\alpha+\beta)}{(\alpha+\beta+1)\Gamma(\alpha+\beta+1)}\frac{ (\alpha+1) \Gamma(\alpha+1) }{\Gamma(\alpha)}\\
&= \frac{\Gamma(\alpha+\beta)}{(\alpha+\beta+1)(\alpha+\beta)\Gamma(\alpha+\beta)}\frac{ (\alpha+1)\alpha \Gamma(\alpha) }{\Gamma(\alpha)}\\
&= \frac{(\alpha+1)\alpha}{(\alpha+\beta+1)(\alpha+\beta)}.
\end{align*}
La variance peut alors être calculée ainsi~:
\begin{align*}
\vr(X) &= \ex(X^2) - \{ \ex(X)\}^2 \\
&= \frac{(\alpha+1)\alpha}{(\alpha+\beta+1)(\alpha+\beta)}-\frac{\alpha^2}{(\alpha+\beta)^2} \\
&= \frac{\alpha}{\alpha+\beta}\left[\frac{\alpha+1}{\alpha+\beta+1}-\frac{\alpha}{\alpha+\beta}\right]\\
&= \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
\end{align*}

\end{sol}
\end{exercice}

\begin{exercice}
Calculer la fonction génératrice des moments $M$ d'une distribution de Poisson avec paramètre $\lambda>0$. Utilisez $M$ pour montrer que l'espérance et la variance d'une variable aléatoire Poisson sont égales.
\begin{rep}
$M_X(t) = \exp\{\lambda(e^t-1)\}$ pour tout $t \in \mathbb{R}$.
\end{rep}
\begin{sol}
Soit $X$ une variable aléatoire Poisson avec paramètre $\lambda>0$. Sa fonction génératrice des moments, pour tout $t \in \mathbb{R}$, est donnée par
\begin{align*}
M_X(t) = \ex (e^{tX}) = \sum_{k=0}^\infty e^{tk}\frac{\lambda^k e^{-\lambda}}{k!} =  e^{-\lambda}\sum_{k=0}^\infty \frac{\{\lambda e^t\}^k}{k!}.
\end{align*}
L'équation de droite est la série de Taylor de $\exp(\lambda e^t)$. Ainsi,
$$
M_X(t) = e^{-\lambda}e^{\lambda e^t} = \exp\{\lambda(e^t-1)\}.
$$
La $k$e dérivée de $M$ évaluée à $t=0$ donne le $k$e moment de $X$. Ainsi, 
$$
\ex(X) = M^\prime_X(0) = \left.\exp\{\lambda(e^t-1)\}\times \lambda e^t\right|_{t=0} = \lambda
$$
de façon similaire,
\begin{align*}
\ex(X^2) & = M^{\prime \prime}_X(0) = \left.\exp\{\lambda(e^t-1)\}\times \lambda^2 e^{2t}+\exp\{\lambda(e^t-1)\}\times \lambda e^{t}\right|_{t=0} = \lambda^2+\lambda
\end{align*}
Alors,
$$
\vr(X) = \ex(X^2) - \{\ex(X)\}^2 = \lambda^2+\lambda-\lambda^2=\lambda.
$$
\end{sol}
\end{exercice}

\begin{exercice}
Une station service opère deux pompes à essence. Chacune d'entre elles peut produire jusqu'à 10~000 litres d'essence par mois. La quantité totale d'essence pompée à la station chaque mois est une variable aléatoire $Y$, mesurée en 10~000 litres, avec fonction de densité de probabilité donnée par
$$
f(y)=\begin{cases}
y, & 0\leq y<1,\\
2-y, & 1\leq y <2,\\
0, &\mbox{ailleurs.}
\end{cases}
$$
\begin{enumerate}
\item Trouver la fonction de répartition $F$ de $Y$.
\item Calculer la probabilité que la station service pompe entre 8~500 et 11~500 litres d'essence dans un mois.
\item Quel est le revenu mensuel espéré si la station service vend son essence au prix de 2,10~\$ le litre~?
\end{enumerate}
 \begin{rep}
     b) 0.2775 \, \, c) 21~000
   \end{rep}
\begin{sol}
\begin{enumerate}
\item Le support de $Y$ est $[0,2]$, donc $F(y)=0$ pour $y<0$ et $F(y)=1$ pour $y>2$. Pour $0\leq y <1$,
$$
F(y)=\Pr[Y\leq y]=\int_0^y x\d x = \frac{y^2}{2}.
$$
Pour $1\leq y <2$,
$$
F(y)=\Pr[Y\leq y]=\int_0^1 x\d x +\int_1^y (2-x)\d x = 1-\frac{(2-y)^2}{2}.
$$
Ainsi la fonction de répartition de $Y$ est
$$
F(y)= \begin{cases}
0, & y<0\\
y^2/2, & 0\leq y<1\\
1- (2-y)^2/2, & 1\leq y <2\\
1, & y>2.
\end{cases}
$$

\item On souhaite trouver la probabilité que $10000 Y$ se situe entre 8~500 et 11~500:
\begin{align*}
\Pr[8500<10000Y\leq 11500] & = \Pr[0.85<Y\leq 1.15]\\
&=F(1.15)-F(0.85)\\
&=1- \frac{(2-1.15)^2}{2}-\frac{0.85^2}{2}=0.2775.
\end{align*}

\item On désire trouver l'espérance du revenu, qui est de 2,10~\$ par litre, donc $\ex[2.10\times 10000Y]=21000\ex[Y]$. On a
\begin{align*}
\ex[Y]&=\int_0^1 y^2\d y +\int_1^2 y(2-y)\d y\\
&=\left.\frac{y^3}{3}\right|_0^1+\left.y^2-\frac{y^3}{3}\right|_1^2\\
&=\frac{1}{3}+4-\frac{2^3}{3}-1+\frac{1}{3} = 1.
\end{align*}
Ainsi, l'espérance de revenu mensuel est de 21~000~\$.
\end{enumerate}
\end{sol}
\end{exercice}


% Moments d'une variable aléatoire
\begin{exercice}
  \label{ex:rappels:moyenne}
  Soit $X$ une variable aléatoire de moyenne $\mu$ et de variance
  $\sigma^2$. Déterminer la valeur de $c$ qui minimise $\esp{(X -
    c)^2}$.
  \begin{rep}
    $c = \mu$
  \end{rep}
  \begin{sol}
    On doit trouver le point où la fonction $f(c) = \esp{(X - c)^2}$
    atteint son minimum. Or, $f^\prime(c) = -2 \esp{X - c} = 0$
    lorsque $\esp{X} - c = 0$, soit $c = \esp{X} = \mu$. De plus, il
    s'agit bien d'un minimum puisque $f^{\prime\prime}(c) = 2 > 0$
    pour tout $c$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $M_X(t)$ la fonction génératrice des moments de la variable
  aléatoire $X$.
  \begin{enumerate}
  \item Soit $Y = aX + b$, où $a$ et $b$ sont des constantes
    quelconques. Démontrer que
    \begin{displaymath}
      M_Y(t) = e^{bt} M_{X}(at).
    \end{displaymath}
  \item Soient $X_1, \ldots, X_n$ des variables aléatoires indépendantes
    et $Y = X_1 + \dots + X_n$. Démontrer que
    \begin{displaymath}
      M_Y(t) = \prod_{j=1}^n M_{X_j}(t).
    \end{displaymath}
  \end{enumerate}
  \begin{sol}
    \begin{enumerate}
    \item On a
      \begin{align*}
        M_Y(t)
        &= \esp{e^{Yt}} \\
        &= \esp{e^{(aX + b)t}} \\
        &= \esp{e^{aXt} e^{bt}} \\
        &= e^{bt} \esp{e^{aXt}} \\
        &= e^{bt} M_X(at).
      \end{align*}
    \item On a
      \begin{align*}
        M_Y(t)
        &= \esp{e^{Yt}}\\
        &= \esp{e^{(X_1 + \dots + X_n) t}}\\
        &= \esp{e^{X_1 t} \cdots e^{X_n t}} \\
        \intertext{et, par indépendance entre les variables
          aléatoires,}
        M_Y(t)
        &= \esp{e^{X_1t}} \cdots \esp{e^{X_nt}} \\
        &= \prod_{i = j}^n M_{X_j}(t).
      \end{align*}
      Si, en plus, les variables aléatoires $X_1, \dots, X_n$ sont
      identiquement distribuées comme $X$, alors $M_Y(t) =
      (M_X(t))^n$.
    \end{enumerate}
  \end{sol}
\end{exercice}

% Loi des grands nombres
\begin{exercice} 
Soit $X_1,\ldots,X_n$ un échantillon aléatoire tiré d'une distribution avec densité
$$
f(x) = \left\{ \begin{array}{ll}
\frac{200}{x^3},& \quad x\geq 10\\
0, & \quad \mbox{ailleurs}\\
\end{array}\right.
$$
Est-ce qu'il est possible d'utiliser la Loi faible des grands nombres pour cet exemple? Justifiez.
\begin{rep}
Non.
\end{rep}
\begin{sol}
La Loi faible des grands nombres indique que, lorsque $n\to\infty$, $\bar X_n\stackrel{P}{\to}\ex[X]$, où $X_1,\ldots,X_n$ est une suite de variables aléatoires indépendantes et identiquement ditribuées. Cependant, le résultat tient seulement si $\ex[X^2] < \infty$, ce qui n'est pas le cas ici puisque
\begin{align*}
\ex[X^2]=\int_{10}^\infty \frac{200}{x^3}x^2 \d x =\int_{10}^\infty \frac{200}{x} \d x = \left. 200\ln(x)\right|_{10}^\infty = \infty.
\end{align*} 
Ainsi, il n'est pas possible d'appliquer la WLLN dans ce cas.
\end{sol}
\end{exercice}

\begin{exercice}
Soit $X_1,\ldots,X_n$ un échantillon aléatoire tiré d'une distribution avec densité
$$
f(x)=4(1-x)^3, \quad \mbox{ pour } 0\leq x\leq 1.
$$
Montrer que $\bar X_n$ converge en probabilité vers une contante et trouver cette constante.
\begin{rep}
1/5
\end{rep}
\begin{sol}
Si $X$ a la même distribution que $X_1, \ldots , X_n$ et $\vr(X)<\infty$, la loi faible des grands nombres implique que, quand $n \to \infty$,
$
\frac{1}{n} \, \sum_{i=1}^n X_i
$
converge en probabilité vers $\esp{X}$. Puisque $X$ suit une distribution Beta avec paramètres $\alpha=1$ et $\beta=4$,  $\esp{X}=\alpha/(\alpha+\beta)=1/5$ et 
$$
\var{X}=\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}=\frac{4}{5^2\times 6}=\frac{2}{75}<\infty.
$$ 
Donc, la WLLN s'applique et $\bar X_n$ converge en probabilité vers 1/5.
\end{sol}
\end{exercice}

% Statistiques d'ordre

\begin{exercice}
  \label{ex:echantillon:min}%
  Soit $X_1, \dots, X_n$ un échantillon aléatoire tiré d'une loi avec
  fonction de répartition $F_X(\cdot)$ et $X_{(1)} \leq \dots \leq
  X_{(n)}$ les statistiques d'ordre correspondantes. Trouver la
  fonction de répartition de $X_{(1)} = \min (X_1, \dots X_n)$.
  \begin{rep}
    $F_{X_{(1)}}(x) = 1 - (1 -  F_X(x))^n$
  \end{rep}
  \begin{sol}
    Puisque $X_{(1)}$ est la plus petite valeur de l'échantillon, on a
    que
    \begin{align*}
      F_{X_{(1)}}(x)
      &= \prob{X_{(1)} \leq x} \\
      &= 1 - \prob{X_{(1)} > x} \\
      &= 1 - \prob{X_1 > x, X_2 > x, \dots, X_n > x}.
    \end{align*}
    Or, les variables aléatoires $X_1, \dots, X_n$ sont indépendantes
    et identiquement distribuées, d'où
    \begin{align*}
      F_{X_{(1)}}(x)
      &= 1 - \prob{X_1 > x} \prob{X_2 > x} \cdots \prob{X_n > x} \\
      &= 1 - (\prob{X > x})^n \\
      &= 1 - (1 -  F_X(x))^n.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soient $X_{(1)} \leq X_{(2)} \leq X_{(3)} \leq X_{(4)}$ les
  statistiques d'ordre d'un échantillon aléatoire de taille $4$ issu
  d'une distribution avec fonction de densité de probabilité $f(x) =
  e^{-x}$, $0 < x < \infty$. Calculer $\prob{X_{(4)} > 3}$.
  \begin{rep}
    $1 - (1 - e^{-3})^4$
  \end{rep}
  \begin{sol}
    On cherche la probabilité que la plus grande valeur de
    l'échantillon soit supérieure à 3, soit le complément de la
    probabilité que toutes les valeurs de l'échantillon soient
    inférieures à 3:
    \begin{align*}
      \prob{X_{(4)} > 3}
      &= 1 - \prob{X_{(4)} \leq 3} \\
      &= 1 - \prob{X_1 \leq 3} \prob{X_2 \leq 3}
      \prob{X_3 \leq 3} \prob{X_4 \leq 3} \\
      &= 1 - (F_X(3))^4.
    \end{align*}
    Or, on aura reconnu en $f(x)$ la densité d'une loi exponentielle
    de paramètre $\lambda = 1$. Par conséquent, $F_X(x) = 1 - e^{-x}$
    et $\prob{X_{(4)} > 3} = 1 - (1 - e^{-3})^4$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, X_2, X_3$ un échantillon aléatoire issu d'une loi bêta de
  paramètres $\alpha = 2$ et $\beta = 1$. Calculer la probabilité que
  la plus petite valeur de l'échantillon soit supérieure à la médiane
  (théorique) de la distribution.
  \begin{rep}
    $1/8$
  \end{rep}
  \begin{sol}
    Soit $m$ la médiane de la distribution. On cherche $\prob{X_{(1)}
      > m}$. Avec le résultat de
    l'exercice~\ref{chap:base}.\ref{ex:echantillon:min},
    \begin{align*}
      \prob{X_{(1)} > m}
      &= 1 - \prob{X_{(1)} \leq m} \\
      &= 1 - F_{X_{(1)}}(m) \\
      &= 1 - (1 - (1 - F_X(m))^3)\\
      &= (1 - F_X(m))^3 \\
      &= \frac{1}{8},
    \end{align*}
    car $F_X(m) = 1 - F_X(m) = 1/2$ par définition de la médiane. Le
    type de distribution ne joue donc aucun rôle dans cet exercice.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X$ une variable aléatoire discrète avec fonction de masse de
  probabilité $\prob{X = x} = 1/6$, $x = 1, 2, 3, 4, 5, 6$. Démontrer
  que la fonction de masse de probabilité du minimum d'un échantillon
  aléatoire de taille $5$ issu de cette distribution est
  \begin{displaymath}
    \Prob{X_{(1)} = x} =
    \left(
      \frac{7 - x}{6}
    \right)^5 -
    \left(
      \frac{6 - x}{6}
    \right)^5, \quad x = 1, 2, 3, 4, 5, 6.
  \end{displaymath}
  \begin{sol}
    On a que $X$ est distribuée uniformément sur $\{1, \dots, 6\}$,
    d'où $F_X(x) = x/6$, $x = 1, \dots, 6$. De
    l'exercice~\ref{chap:base}.\ref{ex:echantillon:min}, on a
    que
    \begin{align*}
      F_{X_{(1)}}(x)
      &= 1 - (1 - F_X(x))^5 \\
      &= 1 - \left( 1 - \frac{x}{6} \right)^5.
    \end{align*}
    Par conséquent, la fonction de masse de probabilité du minimum est
    \begin{align*}
      \prob{X_{(1)} = x}
      &= \lim_{y \rightarrow x^+} F_{X_{(1)}}(y) -
      \lim_{y \rightarrow x^-} F_{X_{(1)}}(y) \\
      &= F_{X_{(1)}}(x) - F_{X_{(1)}}(x - 1) \\
      &= \left(1 - \frac{x-1}{6} \right)^5 -
      \bigg( 1 - \frac{x}{6} \bigg)^5 \\
      &= \left(\frac{7 - x}{6}\right)^5 -
      \left(\frac{6 - x}{6}\right)^5.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soient $X_{(1)} \leq \dots \leq X_{(n)}$ les statistiques d'ordre
  d'un échantillon aléatoire tiré d'une loi de Weibull, dont la
  fonction de répartition est $F_X(x) = 1 - e^{-\beta x^\alpha}$.
  Calculer la fonction de répartition, la fonction de densité et
  l'espérance de $X_{(1)}$.
  \begin{rep}
    $X_{(1)} \sim \text{Weibull}(\alpha, n\beta)$.
  \end{rep}
  \begin{sol}
    De l'exercice~\ref{chap:base}.\ref{ex:echantillon:min}, on
    a
    \begin{align*}
      F_{X_{(1)}}(x)
      &= 1 - (1 - F_X(x))^n \\
      &= 1 - (e^{-\beta x^\alpha})^n \\
      &= 1 - e^{-n \beta x^\alpha},
    \end{align*}
    d'où $X_{(1)} \sim \text{Weibull}(\alpha, n \beta)$. Ainsi, la fonction de densité de probabilité du minimum de
    l'échantillon est
    \begin{equation*}
      f_{X_{(1)}}(x) = n \beta \alpha x^{\alpha - 1} e^{-n \beta x^\alpha}
    \end{equation*}
    et l'espérance est
    \begin{equation*}
      \esp{X_{(1)}} = \frac{\Gamma(1 + 1/\alpha)}{(n \beta)^{\frac{1}{\alpha}}}.
    \end{equation*}
  \end{sol}
\end{exercice}

\begin{exercice}
\label{ex:conjointe}
Soit un échantillon aléatoire $X_1,\ldots,X_n$ tiré d'une distribution $F$ avec densité $f(x)$, pour $x\in\mathbb{R}$. Trouver la fonction de densité conjointe de $X_{(1)}=\min (X_1, \dots X_n)$ et de $X_{(n)}= \max (X_1, \dots X_n)$, $f_{X_{(1)}, X_{(n)}}(x,y)$.
\begin{rep}
$f_{X_{(1)}, X_{(n)}}(x,y) = n (n - 1) (F(y) - F(x))^{n - 2}  f(x) f(y), \quad x<y$
\end{rep}
\begin{sol}
À partir de la fonction de répartition, on a
\begin{align*}
F_{X_{(1)}, X_{(n)}}(x,y)&= \prob{X_{(1)} \leq x, X_{(n)} \leq y} \\
&= \prob{X_{(n)} \leq y} - \prob{X_{(1)} > x, X_{(n)} \leq y} \\
&= \prob{X_1 \leq y, \dots, X_n \leq y} - \prob{x < X_1 \leq y, \dots, x < X_n \leq y} \\
&= (F(y))^n - (F(y)-F(x))^n, \quad \text{car iid.}
\end{align*}
Donc, pour $x < y$,
$$
f_{X_{(1)}, X_{(n)}}(x,y) = \frac{\d F_{X_{(1)}, X_{(n)}}(x,y)}{\d x \d y} = n (n - 1) (F(y) - F(x))^{n - 2}  f(x) f(y).
$$
\end{sol}
\end{exercice}

\begin{exercice}
\label{ex:etendue}
Soit un échantillon aléatoire $X_1,\ldots,X_n$. L'\emph{étendue} (ou \emph{dispersion}) empirique est
\begin{displaymath}
   R = X_{(n)} - X_{(1)},
\end{displaymath}
et la \emph{mi-étendue} (ou \emph{mi-dispersion}) empirique est
\begin{displaymath}
   T = \frac{X_{(1)} + X_{(n)}}{2}.
\end{displaymath}
En utilisant le fait que la densité conjointe de $X_{(1)}$ et de $X_{(n)}$ est donnée, pour $x<y$, par
\begin{displaymath}
  f_{X_{(1)}, X_{(n)}}(x,y) = n (n - 1) (F_X(y) - F_X(x))^{n - 2}  f_X(x) f_X(y),
\end{displaymath}
trouver la distribution conjointe de $(R,T)$.
\begin{rep}
$f_{R, T}(r, t) = n (n - 1)\{ F_X(t + r/2) - F_X(t - r/2)\}^{n - 2}
    f_X(t + r/2) f_X(t - r/2),$ 
    
pour $r > 0$ et $-\infty < t < \infty$.
\end{rep}
\begin{sol}
On pose 
$$
r = y - x \mbox{  et } t= \frac{x + y}{2}
$$
ce qui est équivalent à
$$
    x = -\frac{r}{2} + t \mbox{  et }
    y = \frac{r}{2} + t.
$$
Le jacobien de la transformation de $(x, y)$ vers $(r, t)$ est
\begin{equation*}
  J =  \begin{vmatrix}
    -1/2 & 1 \\
    1/2  & 1
  \end{vmatrix}
  = -1.
\end{equation*}
On a donc que la densité conjointe de l'étendue $R$ et de la mi-étendue $T$ est
 \begin{displaymath}
    f_{R, T}(r, t) = n (n - 1)
    \left(
      F_X\left(t + \frac{r}{2}\right) -
      F_X\left(t - \frac{r}{2}\right)
    \right)^{n - 2}
    f_X\left(t + \frac{r}{2}\right)
    f_X\left(t - \frac{r}{2}\right),
  \end{displaymath}
  pour $r > 0$ et $-\infty < t < \infty$.
%
%  La densité de l'étendue est
%  \begin{displaymath}
%    f_R(r) = \int_{-\infty}^\infty f_{R, T}(r, t)\, dt
%  \end{displaymath}
%  alors que celle de la mi-étendue est
%  \begin{displaymath}
%    f_T(t) = \int_{0}^\infty f_{R, T}(r, t)\, dr.
%  \end{displaymath}
\end{sol}
\end{exercice}

\begin{exercice}
\label{ex:etendueunif}
Si $X \sim \mathcal{U}(0, 1)$, montrer que la densité de l'étendue est
$ f_R(r) = n (n - 1) r^{n - 2} (1 - r),$ 
pour $0 < r < 1$.

\begin{sol}
Ici, $f_X(x)=1$ pour $x\in(0,1)$ et $F_X(x)=x$, pour $x\in(0,1)$. Selon l'exercice \ref{chap:base}.\ref{ex:etendue}, on trouve que
\begin{align*}
f_{R, T}(r, t) &= n (n - 1)\{ F_X(t + r/2) - F_X(t - r/2)\}^{n - 2}
    f_X(t + r/2) f_X(t - r/2)\\
&= n (n - 1)\{ (t + r/2) - (t - r/2)\}^{n - 2}\\
&= n (n - 1) r^{n-2},
\end{align*}
pour $0 < r < 1$ et $t + r/2 \in(0,1)$ et $t - r/2\in(0,1)$, donc $\frac{r}{2} < t < 1 - \frac{r}{2}$. 
Par conséquent,
\begin{align*}
  f_R(r) &= n (n - 1) r^{n - 2} \int_{r/2}^{1 - r/2} dt \\
   &= n (n - 1) r^{n - 2} (1 - r), \quad 0 < r < 1.
  \end{align*}
%
%  De manière similaire, on peut démontrer que
%  \begin{equation*}
%    f_T(t) =
%    \begin{cases}
%      n (2t)^{n - 1},        & 0 < t < 1/2 \\
%      n [2 (1 - t)]^{n - 1}, & 1/2 < t < 1.
%    \end{cases}
%  \end{equation*}
\end{sol}
\end{exercice}

\begin{exercice}
  Calculer la probabilité que l'étendue d'un échantillon aléatoire
  de taille $4$ issu d'une loi uniforme sur l'intervalle $(0, 1)$ soit
  inférieure à $1/2$.
  \begin{rep}
    $5/16$
  \end{rep}
  \begin{sol}
    Soit $R$ l'étendue de l'échantillon aléatoire. Avec l'exercice \ref{chap:base}.\ref{ex:etendueunif}, on sait que
    \begin{displaymath}
      f_R(x) = n (n - 1) x^{n - 2} (1 - x), \mbox{ pour } x\in(0,1).
    \end{displaymath}
    Par conséquent
    \begin{align*}
      \Prob{R \leq \frac{1}{2}}
      &= \int_{0}^{1/2} f_X(x)\, dx\\
      &= (4)(3)\int_{0}^{1/2} x^2 (1 - x)\, dx\\
      &= \frac{5}{16}.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Si un échantillon de taille $2$ est tiré d'une loi bêta avec
  paramètres $\alpha = 1$ et $\beta = 2$, quelle est la probabilité
  que l'une des deux valeurs de l'échantillon soit au moins deux fois
  plus grande que l'autre? (\emph{Astuce}: intégrer la densité
  conjointe des deux valeurs de l'échantillon au-dessus de la surface
  correspondant à la probabilité recherchée.)
  \begin{rep}
    $7/12$
  \end{rep}
  \begin{sol}
    On a que $X \sim \text{Bêta}(1, 2)$, c'est-à-dire que $f_X(x) =
    2(1 - x)$, $0 < x < 1$. Soit $X_1, X_2$ un échantillon aléatoire
    tiré de cette densité. Par indépendance, on a
    \begin{align*}
      f_{X_1 X_2}(x_1, x_2)
      &= f_{X_1}(x) f_{X_2}(x) \\
      &= 4 (1 - x_1)(1 - x_2).
    \end{align*}
    On cherche $\prob{X_2 \geq 2X_1 \cup X_1 \geq 2X_2}$. Par
    définition,
    \begin{equation*}
      \prob{X_1 \geq 2X_2 \cup X_2 \geq 2X_1}
      = \iint\limits_{\mathcal{R}} f_{X_1 X_2}(x_1, x_2) \, dx_2\, dx_1,
    \end{equation*}
    où $\mathcal{R}$ est la région du domaine de définition de
    $f_{X_1X_2}$ telle que $x_1 > 2x_2$ ou $x_2 > 2x_1$. Cette région
    est représentée à la figure~\ref{fig:echantillon:domaine}. On a
    donc
    \begin{align*}
      \prob{X_1 \geq 2X_2 \cup X_2 \geq 2X_1}
      &= 4 \int_0^{1/2} \int_{2x_1}^1 (1 - x_1)(1 - x_2)\, dx_2\,dx_1 \\
      &\phantom{=}
      + 4 \int_0^{1/2} \int_{2x_2}^1 (1 - x_1)(1 - x_2)\, dx_1\,dx_2 \\
      &= 4 \int_0^{1/2} (1 - x_1)
      \left(
        \frac{1}{2} - 2x_1 + 2x_1^2
      \right)\, dx_1 \\
      &\phantom{=}
      + 4 \int_0^{1/2} (1 - x_2)
      \left(
        \frac{1}{2} - 2x_2 + 2x_2^2
      \right)\, dx_2 \\
      &= \frac{7}{12}.
    \end{align*}
    \begin{figure}
      \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=0.6\textwidth]{figure/unnamed-chunk-8-1} 

\end{knitrout}
      \caption{Domaine de définition de $f_{X_1X_2}(x_1, x_2) =  4 (1
        - x_1)(1 - x_2)$, $x_1, x_2 \in (0, 1)$. Les zones hachurées
        représentent les aires où $x_2 > 2x_1$ ou $x_1 > 2x_2$.}
      \label{fig:echantillon:domaine}
      \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X \sim \mathcal{U}(0, 1)$. Calculer l'espérance de la mi-étendue $T = (X_{(1)} + X_{(n)})/2$ d'un
  échantillon de taille $n$ issu de cette distribution.
  \begin{rep}
    $1/2$.
  \end{rep}
  \begin{sol}
    Soit $T = (X_{(1)} + X_{(n)})/2$ la mi-étendue et $R$ l'étendue.
    On sait que, pour $r > 0$ et $-\infty < t < \infty$, 
    \begin{displaymath}
      f_{RT}(r, t) = n(n-1)r^{n - 2}.
    \end{displaymath}
    On doit calculer la densité marginale de $T$. Il faut voir que le
    domaine de $R$ (et donc le domaine d'intégration) dépend
    indirectement de $T$. En effet, si $0 \leq t \leq 1/2$, on doit
    avoir $0 < r < 2t$. Par contre, si $1/2 < t < 1$, il faut que $0 <
    r < 2(1-t)$. On obtient
    \begin{align*}
      f_T(t) &=
      \begin{cases}
        n(2t)^{n-1}, & 0 < t < 1/2 \\
        n(2(1 - t))^{n - 1}, & 1/2 < t < 1.
      \end{cases}
    \end{align*}
    Ainsi,
    \begin{align*}
      \esp{T} &= 2^{n - 1} n
      \left(
        \int_0^{1/2}t^n\,dt +
        \int_{1/2}^1 t (1 - t)^{n - 1}\, dt
      \right) \\
      &= 2^{n-1}n
      \left(
        \frac{0,5^{n+1}}{n+1} +
        \frac{n+2}{n(n + 1)}
        \left(
          \frac{1}{2^{n+1}}
        \right)
      \right) \\
      &= \frac{1}{2}.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire d'une loi uniforme
  sur l'intervalle $(0, 1)$.
   \begin{enumerate}
   \item Calculer la moyenne et la variance de $R = X_{(n)} - X_{(1)}$.
   \item Calculer la moyenne et la variance de $T = (X_{(1)} + X_{(n)})/2$.
   \end{enumerate}
   \begin{rep}
     \begin{enumerate}
     \item $\esp{R} = (n - 1)/(n + 1)$ et $\var{R} = (2n - 2)/[(n +
       1)^2 (n + 2)]$.
     \item $\esp{T} = 1/2$ et $\var{T} = 1/[2 (n + 1)(n + 2)]$.
     \end{enumerate}
   \end{rep}
   \begin{sol}
   On sait que $X_i \sim U(0, 1)$. On trouve la fonction de répartition du minimum $X_{(1)}$ avec
$$
   F_{X_{(1)}}(x)= 1 - (1 - F_X(x))^n = 1 - (1-x)^n ,
$$
   d'où sa fonction de densité est
$$
   f_{X_{(1)}}(x)=\frac{\d}{\d x} F_{X_{(1)}}(x) = n(1-x)^{n-1}
$$
 qui peut se réécrire sous la forme
 $$
 f_{X_{(1)}}(x)=\frac{\Gamma(1+n)}{\Gamma(1)\Gamma(n)}x^{1-1}(1-x)^{n-1}.
 $$
 On remarque que $X_{(1)} \sim \text{Bêta}(1, n)$. De la même façon pour $X_{(n)}$,
$$
   F_{X_{(n)}}(x)= (F_X(x))^n =x^n \text{ et }f_{X_{(n)}}(x)=\frac{\d}{\d x} F_{X_{(n)}}(x) = n x^{n-1}
$$
 qui peut se réécrire sous la forme
 $$
 f_{X_{(n)}}(x)=\frac{\Gamma(n+1)}{\Gamma(n)\Gamma(1)}x^{n-1}(1-x)^{1-1}.
 $$
 On remarque que $X_{(n)} \sim \text{Bêta}(n, 1)$. Ainsi, depuis l'exercice~\ref{chap:base}.\ref{ex:conjointe}, la densité conjointe de $(X_{(1)},X_{(n)})$ peut être exprimée comme suit, pour $0<x<y<1$,
$$
f_{X_{(1)},X_{(n)}}(x, y) = n(n - 1)(y - x)^{n-2}.
$$
Ainsi,
     \begin{equation*}
       \Esp{X_{(1)}X_{(n)}} = n (n - 1) \int_0^1 x \int_x^1
       y(y - x)^{n-2}\,dy\,dx.
     \end{equation*}
     L'intégrale intérieure ci-dessus se résoud par parties en posant
     $u = y$ et $dv = (y - x)^{n - 2}\,dy$. On obtient alors
     \begin{align*}
       \Esp{X_{(1)}X_{(n)}} &= n(n - 1) \int_0^1 x
       \left(
         \frac{(y-x)^{n-1}y}{n - 1} -
         \frac{1}{n - 1} \int_x^1 (y - x)^{n - 1}\, dy
       \right)\, dx\\
       &= n \int_0^1 x (1 - x)^{n-1}\, dx -
       \int_0^1 x (1 - x)^n \,dx \\
       &= \frac{1}{n+1} - \frac{1}{(n+1)(n+2)}\\
       &= \frac{1}{n+2},
     \end{align*}
     en intégrant une seconde fois par parties. Par conséquent,
     \begin{align*}
       \Cov{X_{(1)}, X_{(n)}} &= \Esp{X_{(1)} X_{(n)}} -
       \Esp{X_{(1)}}\Esp{X_{(n)}} \\
       &= \frac{1}{n + 2} -
       \left(
         \frac{1}{n + 1}
       \right)
       \left(
         \frac{n}{n + 1}
       \right) \\
       &= \frac{1}{(n + 1)^2 (n + 2)}.
     \end{align*}
     \begin{enumerate}
       \item On a
         \begin{align*}
           \Esp{R} &= \Esp{X_{(n)}} - \Esp{X_{(1)}}\\
           &= \frac{n}{n+1} - \frac{1}{n+1}\\
           &= \frac{n-1}{n+1}
         \end{align*}
         et
         \begin{align*}
           \Var{R} &= \Var{X_{(1)}} + \Var{X_{(n)}} -
           2\, \Cov{X_{(1)}, X_{(n)}}\\
           &= \frac{n}{(n+1)^2(n+2)} + \frac{n}{(n+1)^2(n+2)} -
           \frac{2}{(n+1)^2(n+2)} \\
           &=\frac{2n-2}{(n+1)^2(n+2)}.
         \end{align*}
       \item On a
         \begin{align*}
           \Esp{T} &= \frac{\Esp{X_{(1)}} + \Esp{X_{(n)}}}{2}\\
           &= \frac{1}{2}\left(\frac{n}{n+1} + \frac{1}{n+1}\right)\\
           &= \frac{1}{2}
         \end{align*}
         et
         \begin{align*}
           \Var{T} &= \frac{\Var{X_{(1)}}+\Var{X_{(n)}} +
             2\,\Cov{X_{(1)}, X_{(n)}}}{4}\\
           &= \frac{1}{4}
           \left[
             \frac{n}{(n+1)^2(n+2)} +
             \frac{n}{(n+1)^2(n+2)} +
             \frac{2}{(n+1)^2(n+2)}
           \right] \\
           &= \frac{1}{2(n+1)(n+2)}.
         \end{align*}
       \end{enumerate}
   \end{sol}
 \end{exercice}



\Closesolutionfile{solutions}
\Closesolutionfile{reponses}


%\section*{Exercices proposés dans \cite{Wackerly:mathstat:7e:2008}}
%
%\begin{trivlist}
%\item 3.145--3.151, 3.153, 3.155, 3.158, 3.159, 3.161, 3.162, 3.163
%\end{trivlist}


%%%
%%% Insérer les réponses
%%%
\input{reponses-base}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_analyse_statistique"
%%% coding: utf-8-unix
%%% End:


\chapter{Distributions d'échantillonnage}
\label{chap:C}

\Opensolutionfile{reponses}[reponses-echantillon]
\Opensolutionfile{solutions}[solutions-echantillon]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:C}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:C}}

\end{Filesave}




%%%
%%% Début des exercices
%%%

% concept de statistique et distribution d'échantillonnage

% Distributions liées à la loi normale

% moyenne avec échantillon normal
\begin{exercice}
  Soit $\bar{X}_{5}$ la moyenne d'un échantillon de taille $5$ tiré d'une
  distribution normale avec moyenne $10$ et variance $125$. Trouver la constante $c$
  telle que $\prob{\bar{X}_{5} < c} = 0,90$.
  \begin{rep}
    $16,41$
  \end{rep}
  \begin{sol}
    On sait que $\bar{X}_{5} \sim \mathcal{N}(\esp{\bar{X}_{5}}, \var{\bar{X}_{5}})$ avec
      $\esp{\bar{X}_{5}} = \esp{X} = 10$ et $\var{\bar{X}_{5}} = \var{X}/n =
      125/5 = 5$. Par conséquent,
    \begin{align*}
      \prob{\bar{X}_{5} < c}
      &= \Prob{\frac{\bar{X}_{5} - 10}{\sqrt{25}} < %
        \frac{c - 10}{\sqrt{25}}} \\
      &= \prob{Z < z_\alpha} \\
      &= 1 - \alpha
    \end{align*}
    avec $Z \sim \mathcal{N}(0, 1)$ et $z_\alpha = (c - 10)/5$. Ici, on a $1 -
    \alpha = 0,90$. On trouve dans une table de quantiles de la loi
    normale que $z_{0,10} = 1,282$, d'où $c = 16,41$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Si $\bar{X}_n$ est la moyenne d'un échantillon de taille $n$ tiré
  d'une distribution normale de moyenne $\mu$ et de variance $100$,
  trouver la valeur de $n$ telle que $\Prob{\mu - 5 < \bar{X}_n < \mu +
    5} = 0,954$.
  \begin{rep}
    $16$
  \end{rep}
  \begin{sol}
    On a $\esp{\bar{X}_n} = \esp{X} = \mu$, $\var{\bar{X}_n} = \var{X}/n =
    100/n$ et $\bar{X}_n\sim \mathcal{N}(\mu, 100/n)$. Ainsi, on cherche $n$ tel
    que
    \begin{align*}
      \prob{\mu - 5 < \bar{X}_n< \mu + 5}
      &= \Prob{-\frac{5}{10/\sqrt{n}} < %
        \frac{\bar{X}_n- \mu}{10/\sqrt{n}} < %
        \frac{5}{10/\sqrt{n}}} \\
      &= \Phi\left( \frac{5 \sqrt{n}}{10} \right)
      - \Phi\left( -\frac{5 \sqrt{n}}{10} \right) \\
      &= 2 \Phi\left( \frac{5\sqrt{n}}{10} \right) - 1 \\
      &= 0,954,
    \end{align*}
    soit
    \begin{equation*}
     \Phi \left( \frac{5\sqrt{n}}{10} \right) = 0,977.
    \end{equation*}
    On trouve dans une table de loi normale que $5 \sqrt{n}/10 = 2$,
    d'où $n = 16$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_{25}$ un échantillon aléatoire issu d'une
  distribution $\mathcal{N}(0, 16)$ et $Y_1, \dots, Y_{25}$ un échantillon
  aléatoire issu d'une distribution $\mathcal{N}(1, 9)$. Les deux échantillons
  sont indépendants. Soient $\bar{X}_{25}$ et $\bar{Y}_{25}$ les moyennes des
  deux échantillons. Calculer la probabilité $\prob{\bar{X}_{25} > \bar{Y}_{25}}$.
  \begin{rep}
    $0,159$
  \end{rep}
  \begin{sol}
    On a $\bar{X}_{25} \sim \mathcal{N}(0, 16/25)$, $\bar{Y}_{25} \sim \mathcal{N}(1, 9/25)$ et, par
    conséquent, $\bar{X}_{25} - \bar{Y}_{25} \sim \mathcal{N}(-1, 1)$. On a donc
    \begin{align*}
      \prob{\bar{X}_{25} > \bar{Y}_{25}}
      &= \prob{\bar{X}_{25} - \bar{Y}_{25} > 0} \\
      &= \Prob{\frac{\bar{X}_{25}-\bar{Y}_{25} - (-1)}{\sqrt{1}} > %
        \frac{0 - (-1)}{\sqrt{1}}} \\
      &= 1 - \Phi(1)\\
      &= 0,159.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice} 
Soit $Y_1,\ldots,Y_8$ un échantillon aléatoire issu d'une distribution $\mathcal{N}(0,1)$ et 
$$
\bar Y= (Y_1+\cdots+Y_7)/7.
$$ 
Quelle est la distribution des statistiques suivantes? Justifiez vos réponses.
%Let $Y_1,\ldots,Y_8$ be a random sample from a $\mathcal{N}(0,1)$ and $\bar Y= (Y_1+\cdots+Y_7)/7$. What is the distribution of the following statistics? (Justify your answers.)

\begin{enumerate}
\item $W = \sum_{i=1}^7 Y_i^2$ 
\item $U = \sum_{i=1}^7 (Y_i-\bar Y)^2$ 
\item $Y_8^2+U$
\item $\sqrt{7} Y_8/\sqrt{W}$
\item $\sqrt{6}Y_8/\sqrt{U}$
\item $3(7\bar Y^2+Y_8^2)/U$
\end{enumerate}
\begin{sol}
\begin{enumerate}
\item La statistique $W = Y_1^2 + \cdots + Y^2_7$ suit une loi khi carré avec $7$ degrés de liberté, $\chi^2_{(7)}$, puisque $W$ est une somme de sept variables \textbf{indépendantes}, chacune d'entre elle étant le carré d'une variable aléatoire normale centrée réduite.

\item Si $Y_1, \ldots , Y_n$ forme un échantillon aléatoire tiré d'une $\mathcal{N}(\mu, \sigma^2)$, alors
$$
\frac{(n-1)S^2_n}{\sigma^2} = \frac{1}{\sigma^2} \sum_{i=1}^n (Y_i - {\bar Y}_n)^2
$$
a une distribution khi carré avec $n-1$ degrés de liberté, $\chi^2_{(n-1)}$. Si on pose $n=7$ et $\sigma^2 =1$, on trouve que
$$
U = \sum_{i=1}^7 (Y_i - {\bar Y})^2 \sim \chi^2_{(6)}.
$$

\item Selon (b), $U$ a une distribution $\chi^2_{(6)}$. Cela signifie que la somme a la même distribution que $Z_1^2 + \cdots + Z_6^2$, où $Z_1, \ldots , Z_6$ sont iid $\mathcal{N}(0,1)$ et indépendantes de $Y_8$, qui suit aussi une $\mathcal{N}(0,1)$. Ainsi, 
$$
Y_8^2 +U \sim \chi^2_{(7)}.
$$

\item 
D'abord, on observe que $\sqrt{7} \, Y_8 /\sqrt{W} = Y_8/\sqrt{W/7}$, où $Y_8 \sim \mathcal{N}(0,1)$ et $W \sim \chi^2_{(7)}$ selon a). De plus, $W$ et $Y_8$ sont des variables aléatoires indépendantes. Ainsi, $Y_8/\sqrt{W/7}$ suit une loi Student $t_{(7)}$.

\item On note que $U \sim \chi^2_{(6)}$ selon b). Ensuite, $U$ est indépendant de $Y_8 \sim \mathcal{N}(0,1)$. Ainsi,
$$
\frac{\sqrt{6}Y_8}{\sqrt{U}} = \frac{Y_8}{\sqrt{U/6}} \sim t_{(6)}.
$$

\item On sait que la moyenne échantillonnale ${\bar Y}$ est indépendante de la variance échantillonnale $U/6$. Donc, ${\bar Y}$ et $Y_8$ sont toutes deux indépendantes de $U$. Puisque ${\bar Y} \sim \mathcal{N}(0,1/7)$, on a $\sqrt{7} \, {\bar Y} \sim \mathcal{N}(0,1)$ et ainsi
$$
7{\bar Y}^2 + Y^2_8 = (\sqrt{7} \, {\bar Y})^2 + Y^2_8 \sim \chi^2_{(2)}.
$$
De plus, $U \sim \chi^2_{(6)}$ avec (b). Il en découle que
$$
\frac{3(7{\bar Y}^2 + Y^2_8)}{U} = \frac{\displaystyle \frac{(\sqrt{7} \, {\bar Y})^2+Y^2_8}{2}}{U/6} \sim \mathcal{F}_{(2,6)}.
$$
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire d'une loi normale de moyenne $\mu$ et variance~$\sigma^2$. Trouver la moyenne et la variance de la statistique
  \begin{displaymath}
    S^2_n = \frac{1}{(n-1)}\sum_{i=1}^n (X_i - \bar{X})^2.
  \end{displaymath}

  \begin{rep}
    $\esp{S^2_n} = \sigma^2$, $\var{S^2_n} = 2 \sigma^4/(n-1)$
  \end{rep}
  \begin{sol}
    On sait que $(n-1)S^2_n/\sigma^2 \sim \chi^2(n - 1)$, que l'espérance
    d'une loi $\chi^2$ est égale à son nombre de degrés de liberté et
    que sa variance est égale à deux fois son nombre de degrés de
    liberté. On a donc
    \begin{align*}
      \esp{S^2_n}
      &= \frac{\sigma^2}{(n-1)}\, \Esp{\frac{(n-1)S^2_n}{\sigma^2}}\\\
      &= \frac{(n - 1) \sigma^2}{(n-1)}=\sigma^2 \\
      \intertext{et}
      \var{S^2_n}
      &= \frac{\sigma^4}{(n-1)^2}\, \Var{\frac{(n-1)S^2_n}{\sigma^2}}\\
      &= \frac{2 (n - 1) \sigma^4}{(n-1)^2}= \frac{2 \sigma^4}{(n-1)}
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $S^2_6$ la variance d'un échantillon de taille $6$ d'une
  distribution normale de moyenne $\mu$ et de variance $10$. Calculer
  $\prob{2,30 < S^2_6 < 22,2}$.
  \begin{rep}
    $0,90$
  \end{rep}
  \begin{sol}
    On sait que $5S^2_6/\sigma^2 \sim \chi^2(5)$. Soit $Y \sim
    \chi^2(5)$. On a donc,
    \begin{align*}
      \prob{2,30 < S^2_6 < 22,2}
      &= \Prob{\frac{5 (2,30)}{10} <
        \frac{5 S^2_6}{10} <
        \frac{5 (22,2)}{10}} \\
      &= \prob{1,15 < Y < 11,1}\\
      &= \prob{Y < 11,1} - \prob{Y <  1,15}.
    \end{align*}
    On trouve dans une table de quantiles de la loi khi~carré (ou avec
    la fonction \texttt{qchisq} dans \textsf{R}, par exemple) que
    $\prob{Y < 11,1} = 0,95$ et $\prob{Y < 1,15} = 0,05$. Par
    conséquent, $\prob{2,30 < S^2_6 < 22,2} = 0,90$.
  \end{sol}
\end{exercice}

\begin{exercice} 
Trouvez la fonction de densité de probabilité de $S_n^2$, la variance échantillonale d'un échantillon de taille $n$ d'une distribution $\mathcal{N}(0,\sigma^2)$. Est-ce que la distribution échantillonale de $S_n^2$ fait partie d'une famille de distributions connues~? Utilisez ce résultat pour trouver la moyenne et la variance de $S_n^2$. [\emph{Truc: Considérez $(n-1)S_n^2/\sigma^2\sim \chi^2_{(n-1)}$ et utilisez la méthode de la fonction de répartition pour obtenir la densité de la variable aléatoire transformée.}]
\begin{sol}
On sait que $W=(n-1)S_n^2/\sigma^2\sim \chi^2_{(n-1)}$, et on cherche à trouver la fonction de densité de $S_n^2=\sigma^2 W /(n-1)$. Puisque c'est une transformation d'échelle, on peut utiliser la méthode de la fonction de répartition. D'abord, on exprime la fonction de répartition de $S_n^2$ en termes de la fonction de répartition de $W$:
\begin{align*}
F_{S_n^2}(s)&=\Pr[S_n^2\leq s]=\Pr\left[\frac{\sigma^2 W}{n-1}\leq s\right]=\Pr[W\leq s(n-1)/\sigma^2]=F_W\left(\frac{s(n-1)}{\sigma^2}\right).
\end{align*}
Ensuite, on dérive $F_{S_n^2}(s)$ en fonction de $s$ pour trouver la densité:
\begin{align*}
f_{S_n^2}(s) &=\frac{\d}{\d s}F_W\left(\frac{s(n-1)}{\sigma^2}\right) = f_W\left(\frac{s(n-1)}{\sigma^2}\right)\frac{n-1}{\sigma^2}.
\end{align*}
Finalement, la densité de $W$ est la distribution d'une $\chi^2_{n-1}$. Donc, pour $s>0$,
\begin{align*}
f_{S_n^2}(s) &= \frac{n-1}{\sigma^2}\frac{1}{2^{(n-1)/2}\Gamma(\frac{n-1}{2})}\left(\frac{(n-1)s}{\sigma^2}\right)^{(n-1)/2-1}\exp\left(-\frac{(n-1)s}{2\sigma^2}\right)\\
&=\left(\frac{n-1}{2\sigma^2}\right)^{(n-1)/2}\frac{1}{\Gamma(\frac{n-1}{2})}s^{(n-1)/2-1}\exp\left(-\frac{s}{2\sigma^2/(n-1)}\right).
\end{align*}
Par conséquent, $S_n^2$ suit une distribution gamma avec paramètres $\alpha=(n-1)/2$ et $\beta=2\sigma^2/(n-1)$. La moyenne et la variance sont:
\begin{align*}
\ex[S_n^2]&= \alpha\beta= \frac{n-1}{2}\frac{2\sigma^2}{n-1}=\sigma^2\\
\vr(S_n^2)&=\alpha\beta^2=\frac{n-1}{2}\frac{4\sigma^4}{(n-1)^2}=\frac{2\sigma^4}{n-1}.
\end{align*}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $Z \sim \mathcal{N}(0, 1)$ avec fonction de densité de
  probabilité
  \begin{align*}
    \phi(z) &= \frac{1}{\sqrt{2 \pi}}\, e^{-\frac{1}{2}\, z^2}, \quad
    - \infty < z < \infty, \\
    \intertext{et fonction de répartition}
    \Phi(z) &= \int_{-\infty}^z \phi(x)\, dx.
  \end{align*}
  Exprimer les fonctions de densité et de répartition de $X = \mu +
  \sigma Z$ en fonction de $\phi(\cdot)$ et $\Phi(\cdot)$.
  \begin{rep}
    $f_X(x) = \sigma^{-1} \phi((x - \mu)/\sigma)$, %
    $F_X(x) = \Phi((x - \mu)/\sigma)$
  \end{rep}
  \begin{sol}
    On a $X = \mu + \sigma Z$. Par la technique de la fonction de
    répartition, on obtient:
    \begin{align*}
      F_X(x) &= \prob{X \leq x}\\
      &= \prob{\mu + \sigma Z \leq x}\\
      &= \Prob{Z \leq \frac{x - \mu}{\sigma}}\\
      &= \Phi\left(\frac{x-\mu}{\sigma}\right).
    \end{align*}
    Ainsi, la fonction de densité de probabilité de $X$ est
    \begin{displaymath}
      f_X(x) = \frac{d}{dx}\, \Phi\left(\frac{x - \mu}{\sigma}\right) =
      \frac{1}{\sigma}\, \phi\left(\frac{x-\mu}{\sigma}\right).
    \end{displaymath}
  \end{sol}
\end{exercice}

% La loi khi carré
\begin{exercice}
  \label{ex:transformations:khi2}
  Soit la variable aléatoire $Z \sim \mathcal{N}(0, 1)$. Démontrer que $Z^2 \sim
  \chi^2(1)$ avec la technique de la fonction génératrice des moments.
  (\emph{Note}: il faut intégrer pour trouver la fonction génératrice
  des moments de $Z^2$.)
  \begin{sol}
    Soit $Z \sim \mathcal{N}(0, 1)$. La fonction génératrice des moments de la
    variable aléatoire $Z^2$ est
    \begin{align*}
      M_{Z^2}(t) &= \Esp{e^{Z^2 t}} \\
      &= \int_{-\infty}^\infty e^{z^2 t} \phi(z)\, dz \\
      &= \int_{-\infty}^\infty
      \frac{1}{\sqrt{2\pi}}\, e^{z^2 t} e^{-z^2/2}\, dz\\
      &= \int_{-\infty}^\infty
      \frac{1}{\sqrt{2\pi}}\, e^{-z^2 (1 - 2t)/2}\, dz.
    \end{align*}
    En posant $\sigma^2 = (1 - 2t)^{-1}$, on voit que l'on peut écrire
    l'expression ci-dessus sous la forme
    \begin{equation*}
      M_{Z^2}(t) = \sigma \int_{-\infty}^\infty
      \frac{1}{\sigma \sqrt{2\pi}}\, e^{-z^2/(2 \sigma^2)}\,dz.
    \end{equation*}
    On reconnaît alors sous l'intégrale la densité d'une loi normale
    de moyenne $0$ et de variance $\sigma^2$. Par conséquent,
    \begin{equation*}
      M_{Z^2}(t) = \sigma = \left( \frac{1}{1 - 2t}\right)^{1/2},
    \end{equation*}
    soit la fonction génératrice des moments d'une loi gamma de
    paramètres $\alpha = 1/2 \text{ et } \beta = 2$ ou, de manière
    équivalente, d'une distribution $\chi^2(1)$.
  \end{sol}
\end{exercice}

\begin{exercice}
  \begin{enumerate}
  \item Soit $X \sim \mathcal{N}(0, \sigma^2)$. Trouver la distribution de $Y =
    X^2$.
  \item Soient $X_1$ et $X_2$ deux variables aléatoires indépendantes
    chacune distribuée selon une loi normale centrée réduite. Trouver
    la distribution de
    \begin{displaymath}
      Y = \frac{(X_1 - X_2)^2}{2}.
    \end{displaymath}
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item Gamma$(1/2, 2\sigma^2)$
    \item $\chi^2(1)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a $X \sim \mathcal{N}(0, \sigma^2)$ et $Y = X^2$. Il faut voir que
      $Y = X^2$ n'est pas une transformation bijective. On pose donc
      d'abord $Z = \abs{X}$ et on trouve la densité de $Z$ à l'aide de
      la technique de la fonction de répartition:
      \begin{align*}
        F_Z(z) &= \prob{\abs{X}\leq z}\\
        &= \prob{-z \leq X \leq z}\\
        &= F_X(z) - F_X(-z) \\
        \intertext{d'où}
        f_Z(z) &= f_X(z) + f_X(-z)\\
        &= \frac{2}{\sigma \sqrt{2\pi}}\, e^{-x^2/(2 \sigma^2)},
        \quad z > 0.
      \end{align*}
      Ensuite, on pose la transformation bijective $Y = Z^2 =
      \abs{X}^2 = X^2$. Par la technique du changement de variable, on
      a
      \begin{align*}
        f_Y(y) &= f_Z(y^{1/2}) \left| \frac{1}{2\sqrt{y}} \right| \\
        &= \frac{2}{\sigma \sqrt{2\pi}}\,
        e^{-y/(2 \sigma^2)} \left( \frac{1}{2\sqrt{y}} \right) \\
        &= \frac{(2 \sigma^2)^{-1/2}}{\Gamma(\frac{1}{2})}\,
        y^{1/2 - 1} e^{-y/(2 \sigma^2)}, \quad y > 0
      \end{align*}
      puisque $\Gamma(\frac{1}{2}) = \sqrt{\pi}$. On voit donc que
      \begin{equation*}
        Y \sim \text{Gamma}\left(\frac{1}{2}, 2\sigma^2\right).
      \end{equation*}
    \item On sait que $Z = X_1 - X_2 \sim \mathcal{N}(0, 2)$ et que $Z/\sqrt{2}
      \sim \mathcal{N}(0, 1)$. En utilisant le résultat de la partie a), on a
      immédiatement que $Y = Z^2/2 \sim \chi^2(1)$.
    \end{enumerate}
  \end{sol}
\end{exercice}

% La loi T

\begin{exercice}
  Soit $T$ une variable aléatoire distribuée selon une loi \emph{t}
  avec $10$ degrés de liberté.
  \begin{enumerate}
  \item Trouver $\prob{\abs{T} > 2,228}$ à l'aide d'une table de la loi
    \emph{t}.
  \item Répéter la partie a) à l'aide de \textsf{R}. La fonction
    \texttt{pt(x, n)} donne la valeur de la fonction de répartition en
    \texttt{x} d'une loi \emph{t} avec \texttt{n} degrés de liberté.
  \end{enumerate}
  \begin{rep}
    $0,05$
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Puisque la loi $t$ est symétrique autour de zéro, on a
      \begin{align*}
        \prob{\abs{T} > 2,228}
        &= \prob{T > 2,228} + \prob{T < -2,228} \\
        &= 2 \prob{T > 2,228}.
      \end{align*}
      Or, on trouve dans la table de la loi $t$ de
      l'annexe~\ref{chap:t} que $\prob{T \leq 2,228} = 0,975$ si $T
      \sim t(10)$. Par conséquent, $\prob{\abs{T} > 2,228} = 2 (1 -
      0,975) = 0,05$.
    \item Toutes les fonction \textsf{R} servant à évaluer des
      fonctions de répartition ont un argument \texttt{lower.tail}. Ce
      argument est \texttt{TRUE} par défaut, mais lorsque qu'il est
      \texttt{FALSE}, la fonction retourne la probabilité
      \emph{au-dessus} du point \texttt{x}. Ainsi, la probabilité
      cherchée ici est
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlnum{2} \hlopt{*} \hlkwd{pt}\hlstd{(}\hlnum{2.228}\hlstd{,} \hlnum{10}\hlstd{,} \hlkwc{lower.tail} \hlstd{=} \hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.05001177
\end{verbatim}
\end{kframe}
\end{knitrout}
      Il est recommandé d'utiliser cette approche parce qu'elle est,
      de manière générale, plus précise que le calcul du type
      \verb|1 - pt(x, n)|, surtout loin dans les queues des
      distributions.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $T$ une variable aléatoire distribuée selon une loi \emph{t}
  avec $14$ degrés de liberté.
  \begin{enumerate}
  \item Trouver la valeur de $b$ tel que $\prob{-b < T < b} = 0,90$ à
    l'aide d'une table de la loi \emph{t}.
  \item Répéter la partie a) à l'aide \textsf{R}. La fonction
    \texttt{qt(p, n)} retourne le \texttt{p}{\ieme} quantile d'une loi
    \emph{t} avec \texttt{n} degrés de liberté, c'est-à-dire la valeur
    de $x$ où la fonction de répartition vaut \texttt{p}.
  \end{enumerate}
  \begin{rep}
    $1,761$
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Par symétrie de la loi $t$,
      \begin{align*}
        \prob{-b < T < b}
        &= \prob{T < b} - \prob{T < -b} \\
        &= \prob{T < b} - (1 - \prob{T < b}) \\
        &= 2 \prob{T < b} - 1 \\
        &= 0,90.
      \end{align*}
      On cherche donc la valeur de $b$ tel que $\prob{T < b} = (1 +
      0,90)/2 = 0,95$, où $T \sim t(14)$. Dans la table de la loi $t$
      de l'annexe~\ref{chap:t} on trouve que $b = 1,761$.
    \item En définitive, on cherche le 95{\ieme} centile d'une loi
      $t(14)$. Avec \textsf{R}, on obtient
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qt}\hlstd{(}\hlnum{0.95}\hlstd{,} \hlnum{14}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 1.76131
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{enumerate}
  \end{sol}
\end{exercice}

% La loi F

\begin{exercice}
  Soit $U \sim \chi^2(r_1)$ et $V \sim \chi^2(r_2)$, deux variables
  aléatoires indépendantes.
  \begin{enumerate}
  \item Démontrer que la densité de
    \begin{displaymath}
      F = \frac{U/r_1}{V/r_2}
    \end{displaymath}
    est
    \begin{displaymath}
      f(x) =
      \frac{\Gamma((r_1 + r_2)/2) (r_1/r_2)^{r_1/2} x^{r_1/2 - 1}}{%
        \Gamma(r_1/2) \Gamma(r_2/2) (1 + r_1 x/r_2)^{(r_1 + r_2)/2}}.
    \end{displaymath}
  \item Calculer $\esp{F}$.
  \item Calculer $\var{F}$.
  \end{enumerate}
  \begin{rep}
   \begin{enumerate}
     \stepcounter{enumi}
    \item $r_2/(r_2 - 2)$
    \item $2 [r_2^2 (r_2 + r_1 - 2)]/[r_1 (r_2 - 2)^2 (r_2 - 4)]$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a $U \sim \chi^2(r_1)$ et $V \sim \chi^2(r_2)$. On pose
      $F = (U/r_1)/(V/r_2)$ et, disons, $G = V$. Pour trouver la
      densité (marginale) de $F$, il faudra passer par la densité
      conjointe de $F$ et $G$.

      Les équations régissant la transformation de variables
      aléatoires sont
      \begin{align*}
        x &= \frac{r_2}{r_1}\left(\frac{u}{v}\right) &
        u &= \frac{r_1}{r_2}\, x y \\
        y &= v &
        v &= y.
      \end{align*}
      Ainsi, le jacobien de la transformation est
      \begin{displaymath}
        J =
        \begin{vmatrix}
          r_1 y/r_2 & r_1 x/r_2 \\
          0         & 1
        \end{vmatrix}
        = \frac{r_1}{r_2}\, y
      \end{displaymath}
      et la densité conjointe de $F$ et $G$ est
      \begin{align*}
        f_{FG}(x, y)
        &= f_{UV}\left(\frac{r_1}{r_2}\, xy, y\right)
        \left|\frac{r_1}{r_2}\, y\right| \\
        &= \left(\frac{r_1}{r_2}\right) y
        f_U\left(\frac{r_1}{r_2}\, xy\right) f_V(y) \\
        &= \frac{%
          (r_1/r_2)
          (1/2)^{(r_1 + r_2)/2}
          (r_1 xy/r_2)^{r_1/2 - 1}
          y^{r_2/2}
          e^{-(r_1 x/r_2 + 1) y/2}}{%
          \Gamma(r_1/2) \Gamma(r_2/2)}
      \end{align*}
      pour $x > 0$ et $y > 0$. En intégrant, on trouve la densité
      marginale de $F$:
      \begin{align*}
        f_F(x) &= \int_0^{\infty} f_{FG}(x, y)\, dy \\
        &= \frac{(r_1/r_2)^{(r_1 + r_2)/2} x^{r_1/2 - 1}}{%
          \Gamma(r_1/2) \Gamma(r_2/2)} \\
        &\phantom{=} \times
        \int_0^\infty
        \left(\frac{1}{2}\right)^{(r_1 + r_2)/2}
        y^{(r_1 + r_2)/2 - 1}
        e^{-(r_1 x/r_2 + 1) y/2}\, dy \\
        &= \frac{\Gamma((r_1 + r_2)/2) (r_1/r_2)^{r_1/2} x^{r_1/2 - 1}}{%
          \Gamma(r_1/2) \Gamma(r_2/2) (r_1 x/r_2 + 1)^{(r_1 + r_2)/2}} \\
        &\phantom{=} \times
        \int_0^\infty
        \frac{(1/2)^{(r_1 + r_2)/2} (r_1 x/r_2 + 1)^{(r_1 + r_2)/2}}{%
          \Gamma((r_1 + r_2)/2)}\,
        y^{(r_1 + r_2)/2 - 1}
        e^{-(r_1 x/r_2 + 1) y/2}\, dy \\
        &= \frac{\Gamma((r_1 + r_2)/2) (r_1/r_2)^{r_1/2} x^{r_1/2 - 1}}{%
        \Gamma(r_1/2) \Gamma(r_2/2) (1 + r_1 x/r_2)^{(r_1 + r_2)/2}},
      \end{align*}
      puisque l'intégrande ci-dessus est la densité d'une loi gamma.
      La loi de la variable aléatoire $F$ est appelée loi $F$ avec
      $r_1$ et $r_2$ degrés de liberté.
    \item Par indépendance entre les variables aléatoires $U$ et $V$,
      on a
      \begin{align*}
        \esp{F} &= \Esp{\frac{U/r_1}{V/r_2}} \\
        &= \frac{r_2}{r_1}\, \Esp{\frac{U}{V}} \\
        &= \frac{r_2}{r_1}\, \esp{U} \Esp{\frac{1}{V}}.
      \end{align*}
      Or, $\esp{U} = r_1$ et
      \begin{align*}
        \Esp{\frac{1}{V}}
        &= \int_0^\infty \frac{1}{v}\, f_V(v)\, dv \\
        &= \int_0^\infty
        \frac{1}{2^{r_2/2} \Gamma(r_2/2)}\,
        v^{r_2/2 - 1 - 1} e^{-v/2}\, dv \\
        &= \frac{2^{r_2/2 - 1} \Gamma(r_2/2 - 1)}{%
          2^{r_2/2} \Gamma(r_2/2)}, \quad
        \frac{r_2}{2} - 1 > 0.
      \end{align*}
      Avec la propriété de la fonction gamma $\Gamma(x) = (x - 1)
      \Gamma(x - 1)$, cette expression se simplifie en
      \begin{equation*}
        \Esp{\frac{1}{V}} = \frac{1}{r_2 - 2},
      \end{equation*}
      d'où, enfin,
      \begin{displaymath}
        \esp{F} = \frac{r_2}{r_2 - 2}
      \end{displaymath}
      pour $r_2 > 2$.
    \item En procédant comme en b), on trouve que $\esp{U^2} = \var{U}
      + \esp{U}^2 = 2 r_1 + r_1^2$, que
      \begin{displaymath}
        \Esp{\frac{1}{V^2}} = \frac{1}{(r_2 - 2)(r_2 - 4)}
      \end{displaymath}
      et donc que
      \begin{align*}
        \esp{F^2}
        &= \frac{r_2^2}{r_1^2}\, \esp{U^2} \Esp{\frac{1}{V^2}} \\
        &= \frac{r_2^2 (r_1 + 2)}{r_1 (r_2 - 2)(r_2 - 4)}.
      \end{align*}
      Par conséquent,
      \begin{align*}
        \var{F}
        &= \frac{r_2^2 (r_1 + 2)}{r_1 (r_2 - 2)(r_2 - 4)} -
        \left(\frac{r_2}{r_2 - 2}\right)^2 \\
        &= 2 \left(\frac{r_2}{r_2 - 2}\right)^2
        \left(\frac{r_2 + r_1 - 2}{r_1 (r_2 - 4)}\right),
      \end{align*}
      pour $r_2 > 4$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:transformations:Finverse}
  Soit $F$ une variable aléatoire distribuée selon une loi \emph{F}
  avec $\nu_1$ et $\nu_2$ degrés de liberté (dans l'ordre). Démontrer que
  $1/F$ est aussi distribuée selon une loi \emph{F}, mais avec $\nu_2$
  et $\nu_1$ degrés de liberté.
  \begin{sol}
    On a
    \begin{align*}
      \frac{1}{F} &=
      \left(\frac{U/\nu_1}{V/\nu_2}\right)^{-1} \\
      &=\frac{V/\nu_2}{U/\nu_1}
    \end{align*}
    où $U \sim \chi^2(\nu_1)$ et $V \sim \chi^2(\nu_2)$. Puisqu'il s'agit
    d'un ratio de deux variables aléatoires $\chi^2$ divisées chacune
    par son nombre de degrés de liberté, on a donc que
    \begin{displaymath}
      \frac{1}{F} \sim F(\nu_2, \nu_1).
    \end{displaymath}
  \end{sol}
\end{exercice}

\begin{exercice}
  Si $F$ a une distribution \emph{F} avec paramètres $\nu_1 = 5$ et $\nu_2
  = 10$, trouver $a$ et $b$ de sorte que $\prob{F \leq a} = 0,05$ et
  $\prob{F \leq b} = 0,95$. Les quantiles de la loi \emph{F} peuvent
  être trouvés soit dans une table, soit à l'aide de la fonction
  \texttt{qf(x, v1, v2)} de \textsf{R}. (\emph{Astuce}: en travaillant
  avec une table, utiliser le fait que $\prob{F \leq a} = \prob{F^{-1}
    \geq a^{-1}} = 1 - \prob{F^{-1} \leq a^{-1}}$.)
  \begin{rep}
    $a = 0,211$ et $b = 3,33$
  \end{rep}
  \begin{sol}
    On a $F \sim F(5, 10)$ et l'on cherche $a$ et $b$ tel que $\prob{F
      \leq a} = 0,05$ et $\prob{F \leq b} = 0,95$. Dans une table de
    loi $F$, on trouve que $\prob{F \leq 3,326} = 0,95$ et donc que $b
    = 3,33$. Puisque les quantiles inférieurs ne sont pas inclus dans
    la table de l'annexe~\ref{chap:F}, on doit utiliser pour trouver
    $a$ la relation $\prob{F \leq a} = 1 - \prob{F^{-1} \leq a^{-1}}$
    où, tel que démontré à
    l'exercice~\ref{chap:echantillon}.\ref{ex:transformations:Finverse},
    $F^{-1} \sim F(10, 5)$. Dans une table, on trouve que $a^{-1} =
    4,74$, d'où $a = 0,211$.

    Avec \textsf{R}, on obtient les mêmes résultats encore plus
    simplement:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qf}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{0.05}\hlstd{,} \hlnum{0.95}\hlstd{),} \hlnum{5}\hlstd{,} \hlnum{10}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.2111904 3.3258345
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $T = W/\sqrt{V/r}$, où $W$ et $V$ sont des variables aléatoires
  indépendantes avec une distribution, respectivement, normale centrée
  réduite et khi carré avec $r$ degrés de liberté. Démontrer que la
  distribution de $T^2$ est $F$ avec $1$ et $r$ degrés de liberté.
  \begin{sol}
    On sait que $W^2 \sim \chi^2(1)$. Ainsi,
    \begin{displaymath}
      T^2 = \frac{W^2/1}{V/r},
    \end{displaymath}
    qui est un ratio de deux variables aléatoires $\chi^2$ divisées
    par leur nombre de degrés de liberté. Par définition de la loi
    $F$, on a donc que $T^2 \sim F(1, r)$.
  \end{sol}
\end{exercice}

% TCL 

\begin{exercice}
  Démontrer à l'aide du Théorème central limite que la distribution
  gamma avec paramètre de forme $\alpha$ entier et paramètre d'échelle
  $\beta$ tend vers la distribution normale avec moyenne
  $\alpha\beta$ et variance $\alpha\beta^2$ lorsque $\alpha$
  tend vers l'infini. (\emph{Astuce}: définir $Y = X_1 + \dots +
  X_\alpha$ où $X_i \sim \text{Exponentielle}(\beta)$ et trouver la
  distribution asymptotique de $Y$.)
  \begin{sol}
    Soit
    \begin{equation*}
      Y = \sum_{i = 1}^\alpha X_i
    \end{equation*}
    avec $X_i \sim \text{Exponentielle}(\beta)$ et $X_1, \dots,
    X_\alpha$ indépendantes. Par le Théorème central limite,
    \begin{align*}
      \lim_{\alpha \rightarrow \infty} Y =
      \lim_{\alpha  \rightarrow \infty} \sum_{i = 1}^\alpha X_i \sim
      \mathcal{N}(\alpha \esp{X_i}, \alpha \var{X_i}).
    \end{align*}
    Par conséquent,
    \begin{equation*}
      \lim_{\alpha \rightarrow \infty} Y \sim
      N\left(\alpha\beta, \alpha\beta^2\right).
    \end{equation*}
    On trouve à la figure~\ref{fig:echantillon:gammas} les graphiques
    de densités gamma pour quelques valeurs du paramètre $\alpha$. On
    observe, en effet, que la distribution tend vers une normale
    lorsque $\alpha$ augmente.
    \begin{figure}
      \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=0.6\textwidth]{figure/unnamed-chunk-13-1} 

\end{knitrout}
      \caption{Densités de lois gamma pour quelques valeurs du
        paramètre de forme $\alpha$.}
      \label{fig:echantillon:gammas}
    \end{figure}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\bar{X}_{100}$ la moyenne d'un échantillon aléatoire de taille $100$
  tiré d'une loi $\chi^2(50)$.
  \begin{enumerate}
  \item Trouver la distribution exacte de $\bar{X}_{100}$.
  \item Calculer à l'aide d'un logiciel statistique la valeur exacte
    de $\prob{49 < \bar{X}_{100} < 51}$.
  \item Calculer une valeur approximative de la probabilité en b).
  \end{enumerate}
  \begin{rep}

    \begin{inparaenum}
    \item Gamma$(\nombre{2500}, 1/50)$
    \item $0,6827218$
    \item $0,682$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a l'échantillon aléatoire $X_1, \dots, X_{100}$, où
      \begin{equation*}
        X_i \sim \chi^2(50) \equiv
        \text{Gamma}\left( \frac{50}{2},  2 \right), \quad
        i = 1, \dots, 100.
      \end{equation*}
      Or, on sait que $Y = X_1 + \dots + X_{100} \sim \text{Gamma}(100
      (25), 2)$ et que $\bar{X}_{100} = Y/100 \sim
      \text{Gamma}(\nombre{2500}, 2/100)$.
    \item On peut, par exemple, obtenir la probabilité demandée avec
      \textsf{R} ainsi:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pgamma}\hlstd{(}\hlnum{51}\hlstd{,} \hlnum{2500}\hlstd{,} \hlnum{50}\hlstd{)} \hlopt{-} \hlkwd{pgamma}\hlstd{(}\hlnum{49}\hlstd{,} \hlnum{2500}\hlstd{,} \hlnum{50}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.6827218
\end{verbatim}
\end{kframe}
\end{knitrout}
    \item On a $\esp{\bar{X}_{100}} = \nombre{2500}/50 = 50$ et
      $\var{\bar{X}_{100}} = \nombre{2500}/50^2 = 1$. En utilisant
      l'approximation normale, on trouve
      \begin{align*}
        \prob{49 < \bar{X}_{100} < 51}
        &= \Prob{\frac{49 - 50}{1} <
          \frac{\bar{X}_{100} - 50}{1} <
          \frac{51 - 50}{1}} \\
        &\approx \Phi(1) - \Phi(-1) \\
        &= 2\Phi(1) - 1 \\
        &= 0,682.
      \end{align*}
      On trouve la valeur de $\Phi(1)$ dans une table de quantiles de
      la loi normale ou à l'aide d'un logiciel statistique.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\bar{X}$ la moyenne d'un échantillon de taille $128$ d'une loi
  Gamma$(2, 4)$. Trouver une approximation pour $\prob{7 < \bar{X} <
    9}$.
  \begin{rep}
    $0,954$
  \end{rep}
  \begin{sol}
    Puique l'on ne demande qu'une valeur approximative pour $\prob{7 <
      \bar{X} < 9}$, on va utiliser l'approximation normale. La taille
    de l'échantillon étant relativement grande, l'approximation sera
    très bonne. Soit $\bar{X} = (X_1 + \dots + X_{128})/128$, où $X_i
    \sim \text{Gamma}(2, 4)$, $i = 1, \dots, 128$. On a
    $\esp{\bar{X}} = \esp{X_i} = 8$ et $\var{\bar{X}} = \var{X_i}/128
    = 1/4$. Par conséquent,
    \begin{align*}
      \prob{7 < \bar{X} < 9}
        &= \Prob{\frac{7 - 8}{\sqrt{1/4}} <
          \frac{\bar{X} - 8}{\sqrt{1/4}} <
          \frac{9 - 8}{\sqrt{1/4}}} \\
      &\approx \Phi(2) - \Phi(-2) \\
      &= 2 \Phi(2) - 1 \\
      &= 0,954.
    \end{align*}
    On trouve la valeur de $\Phi(2)$ dans une table de quantiles de
    la loi normale ou à l'aide d'un logiciel statistique.
  \end{sol}
\end{exercice}

\begin{exercice}
  Trouver une valeur approximative de la probabilité que la moyenne
  d'un échantillon de taille $15$ d'une loi avec densité $f(x) = 3
  x^2$, $0 < x < 1$, soit entre $3/5$ et $4/5$.
  \begin{rep}
    $0,840$
  \end{rep}
  \begin{sol}
    On souhaitera utiliser l'approximation normale. Cela requiert de
    connaître les valeurs de l'espérance et de la variance de la
    moyenne de l'échantillon, $\bar{X}$, et, par ricochet, celles de
    la variable aléatoire avec densité $f(x)$. Or,
    \begin{align*}
      \esp{X} &= \int_0^1 3x^3\,dx = \frac{3}{4} \\
      \esp{X^2} &= \int_0^1 3x^4\,dx = \frac{3}{5}
    \end{align*}
    et donc $\var{X} = 3/80$. Ainsi, $\esp{\bar{X}} = \esp{X} = 3/4$
    et $\var{\bar{X}} = \var{X}/15 = 1/400$ et
    \begin{align*}
      \Prob{\frac{3}{5} < \bar{X} < \frac{4}{5}}
      &= \Prob{\frac{3/5 - 3/4}{\sqrt{1/400}} <
        \frac{\bar{X} - 3/4}{\sqrt{1/400}} <
        \frac{4/5 - 3/4}{\sqrt{1/400}}} \\
      &\approx \Phi(1) - \Phi(-3) \\
      &= 0,840.
    \end{align*}
  \end{sol}
\end{exercice}


\begin{exercice} 
On suppose que $X_1,\ldots,X_n$ et $Y_1,\ldots,Y_n$ sont des échantillons aléatoires indépendants de populations avec moyennes $\mu_1$ et $\mu_2$ et variances $\sigma_1^2>0$ et $\sigma_2^2>0$, respectivement. Démontrer que, quand $n\to\infty$,
$$
U_n = \frac{(\bar X_n-\bar Y_n)-(\mu_1-\mu_2)}{\sqrt{(\sigma_1^2+\sigma_2^2)/n}}
$$
est asymptotiquement $\mathcal{N}(0,1)$.
\begin{sol}
Pour chaque $i \in \{1,\dots, n \}$, on pose $D_i = X_i - Y_i$. $D_1, \ldots, D_n$ sont mutuellement indépendants et identiquement distribués avec moyenne
$$
\ex(D_1) = \ex(X_1) - \ex(Y_1) = \mu_1 - \mu_2
$$
et variance
$$
\vr(D_1) = \vr(X_1) + \vr(Y_1) = \sigma_1^2 + \sigma_2^2.
$$
Par le Théorème central limite, on a donc que, quand $n \to \infty$,
$$
\sqrt{n} \, \frac{\bar D_n - \ex[D_1]}{\sqrt{\vr(D_1)}} = \frac{(\bar X_n - \bar Y_n) - (\mu_1 -\mu_2)}{\sqrt{(\sigma_1^2 + \sigma_2^2)/n}} = Z_n
$$
converge en distribution vers $\mathcal{N}(0,1)$.
\end{sol}
\end{exercice}

\begin{exercice} 
Une machine d'embouteillage peut être réglée de sorte qu'elle remplisse en moyenne les bouteilles de $\mu$ onces de liquide par bouteille. Il a été observé que la quantité de liquide distribuée par la machine suit une loi normale avec $\sigma= 2,5$ onces.
\begin{enumerate}
\item Si $n=9$ bouteilles sont sélectionnées aléatoirement à la sortie de la machine, quelle est la probabilité que la moyenne échantillonnale diffère de $\mu$ d'au plus 0,2 once?
\item Trouver la probabilité que la moyenne échantillonale diffère de $\mu$ d'au plus 0,2 once lorsque la taille de l'échantillon est $n = 25, 36,$ et $64$. Que remarquez-vous lorsque $n$ augmente? Pouvez-vous expliquer cette remarque?
\item Quelle est la taille d'échantillon requise pour s'assurer que la probabilité que la moyenne échantillonnale diffère de $\mu$ d'au plus 0,2 once est d'au moins 95\%?
\item Comment la probabilité obtenue en a) change-t-elle lorsque $\sigma$ est inconnu et que la variance échantillonnale est égale à $s_n^2= 5,5$?
\end{enumerate}
\begin{rep}
a) 0,1896 \,\, b) 0,3108~; 0,3688~; 0,4778 \,\, c) 601 \,\, d) 0,1955
\end{rep}
\begin{sol}
\begin{enumerate}
\item La probabilité peut être calculée comme suit:
\begin{align*}
\Pr(|\bar X_n - \mu| \le 0,2) & = \Pr(-0,2 \le \bar X_n - \mu \le 0,2) \\
& = \Pr\left(-\frac{0,2}{\sqrt{2,5^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \le \frac{0,2}{\sqrt{2,5^2/9}} \right).
\end{align*}
On sait que 
$$
\frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \sim \mathcal{N}(0,1),
$$
la probabilité est alors
\begin{align*}
\Pr\left(-\frac{0,2}{\sqrt{2,5^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \le \frac{0,2}{\sqrt{2,5^2/9}} \right) &= \Phi\left(\frac{0,2}{\sqrt{2,5^2/9}}\right)-\Phi\left(-\frac{0,2}{\sqrt{2,5^2/9}}\right)\\
& = 2\Phi\left(\frac{0,2}{\sqrt{2,5^2/9}}\right)-1,
\end{align*}
où $\Phi$ représente la fonction de répartition de la loi normale centrée réduite. Cette probabilité peut être évaluée avec une table de la loi normale ou avec \textsf{R}; ce dernier donne 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlnum{2}\hlopt{*}\hlkwd{pnorm}\hlstd{(}\hlnum{0.2}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{2.5}\hlopt{^}\hlnum{2}\hlopt{/}\hlnum{9}\hlstd{))}\hlopt{-}\hlnum{1}
\end{alltt}
\begin{verbatim}
## [1] 0.1896697
\end{verbatim}
\end{kframe}
\end{knitrout}

\item De la même façon qu'en a), on trouve
\begin{align*}
\Pr(|\bar X_n - \mu| \le 0,2)& =\Pr\left(-\frac{0,2}{\sqrt{2,5^2/n}} \le \frac{\bar X_n - \mu}{\sqrt{2,5^2/9}} \le \frac{0,2}{\sqrt{2,5^2/n}} \right)\\
& = 2\Phi\left(\frac{0,2}{\sqrt{2,5^2/n}}\right)-1.
\end{align*}
On considère $n=25$, $n=36$ et $n=64$ dans la formule, ce qui donne 
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{n} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlnum{25}\hlstd{,}\hlnum{36}\hlstd{,}\hlnum{64}\hlstd{)}
\hlnum{2}\hlopt{*}\hlkwd{pnorm}\hlstd{(}\hlnum{0.2}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{2.5}\hlopt{^}\hlnum{2}\hlopt{/}\hlstd{n))}\hlopt{-}\hlnum{1}
\end{alltt}
\begin{verbatim}
## [1] 0.3108435 0.3687726 0.4778274
\end{verbatim}
\end{kframe}
\end{knitrout}
On remarque que la probabilité que la moyenne échantillonnale diffère de la vraie moyenne d'au plus 0,2 once augmente avec $n$. C'est le résultat qu'on attend selon la Loi faible des grands nombres, qui indique que cette probabilité tend vers $1$ quand $n\to \infty$.

\item Si
$$
\Pr ( | {\bar X}_n - \mu | \le 0,2) = 2\Phi\left(\frac{0,2}{\sqrt{2,5^2/n}}\right)-1 \geq 0,95,
$$
alors
$\Phi \left(0,2\sqrt{n}/2,5\right) \geq 0,975$ et ainsi $0,2\sqrt{n}/2,5 \geq 1,96$, ce qui implique que $\sqrt{n} \geq 24,5$ ou $n\geq 600,25$. La taille d'échantillon doit donc être de $n=601$ pour que la probabilité soit \emph{le plus près possible, mais pas plus petite que} 0,95.

\item Dans ce cas, la probabilité à calculer est 
$$
 \Pr\left(-\frac{0,2}{\sqrt{s_n^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{S_n^2/9}} \le \frac{0,2}{\sqrt{s_n^2/9}} \right).
$$
On sait que 
$$
\frac{\bar X_n - \mu}{\sqrt{S_n^2/9}} \sim t_{(8)},
$$
on trouve que
\begin{align*}
 \Pr\left(-\frac{0,2}{\sqrt{s_n^2/9}} \le \frac{\bar X_n - \mu}{\sqrt{S_n^2/9}} \le \frac{0,2}{\sqrt{s_n^2/9}} \right) & =  \Pr\left(-\frac{0,2}{\sqrt{5,5/9}} \le \frac{\bar X_n - \mu}{\sqrt{5,5/9}} \le \frac{0,2}{\sqrt{5,5/9}} \right) \\
 &= T_{(8)}\left(\frac{0,2}{\sqrt{5,5/9}} \right) - T_{(8)} \left(-\frac{0,2}{\sqrt{5,5/9}} \right),
\end{align*}
où $T_{(8)}$ est la fonction de répartition d'une loi Student $t$ avec $8$ degrés de liberté. Encore une fois, cette expression peut être évaluée avec une table de la loi normale ou avec \textsf{R}; ce dernier donne
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pt}\hlstd{(}\hlnum{0.2}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{5.5}\hlopt{/}\hlnum{9}\hlstd{),}\hlkwc{df}\hlstd{=}\hlnum{8}\hlstd{)} \hlopt{-}\hlkwd{pt}\hlstd{(}\hlopt{-}\hlnum{0.2}\hlopt{/}\hlkwd{sqrt}\hlstd{(}\hlnum{5.5}\hlopt{/}\hlnum{9}\hlstd{),}\hlkwc{df}\hlstd{=}\hlnum{8}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.1954708
\end{verbatim}
\end{kframe}
\end{knitrout}
La probabilité obtenue est plus grande qu'en a) étant donné la variabilité ajoutée avec l'estimation de la variance.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
L'\emph{Agence de Protection de l'Environnement} est responsable d'établir les critères pour la quantité autorisée de certains produits chimiques en eau douce. Une mesure commune de la toxicité pour les polluants est la concentration d'un produit chimique causant la mort de la moitié d'une population animale donnée dans un laps de temps connu. Cette mesure est appelée CL50 (concentration létale médiane). Dans plusieurs études, les valeurs observées du logarithme naturel de l'indicateur CL50 sont normalement distribuées. L'analyse est donc basée sur les données de ln(CL50).

Soit $S^2_n$ la variance échantillonnale d'un échantillon de $n=10$ valeurs de ln(CL50) pour le cuivre, et $S^2_m$ la variance échantillonnale d'un échantillon de $m=6$ valeurs de ln(CL50) pour le plomb, tous les deux utilisant la même espèce de poisson. La variance de la population des mesures sur le cuivre est supposée être le double de la variance de la population des mesures sur le plomb. Supposer que $S^2_n$ est indépendante de $S^2_m$.
\begin{enumerate}
\item Expliquer comment il est possible, en référant à une table statistique appropriée, de trouver les nombres $a$ et $b$ tels que
$$
\Pr\left(\frac{S_n^2}{S_m^2}\leq b\right)=0,95, \quad \Pr\left(\frac{S_n^2}{S_m^2}\geq a\right)=0,95.
$$
\emph{Astuce: Observer que $\Pr[U_1/U_2\leq k]=\Pr[U_2/U_1\geq 1/k]$.}

\item Si $a$ et $b$ sont les mêmes qu'en a), calculer
$$
\Pr\left(a\leq\frac{S_n^2}{S_m^2}\leq b\right).
$$
\end{enumerate}
\begin{rep}
a) 0,5744 \,\, b) 0,9
\end{rep}
\begin{sol}
\begin{enumerate}
\item On peut utiliser le fait que
$$
\frac{S_n^2/\sigma_1^2}{S_m^2/\sigma_2^2} \sim F_{(n-1,m-1)},
$$
où $\sigma^2_1$ et $\sigma_2^2$ représentent la variance du premier et du second échantillon, respectivement. Dans ce contexte, $n=10$, $m=6$ et $\sigma_1^2 = 2\sigma_2^2$. Donc,
$$
\frac{S_n^2}{2 S_m^2} \sim F(9,5).
$$
Si $W$ représente une variable aléatoire $F_{(9,5)}$, on a
$$
\Pr\left(  \frac{S_n^2}{ S_m^2} \le b\right) = \Pr(W \le b/2)
$$
et donc $b/2$ est le $95$e quantile de la distribution $F_{(9,5)}$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qf}\hlstd{(}\hlnum{0.95}\hlstd{,}\hlkwc{df1}\hlstd{=}\hlnum{9}\hlstd{,}\hlkwc{df2}\hlstd{=}\hlnum{5}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 4.772466
\end{verbatim}
\end{kframe}
\end{knitrout}
Par conséquent,
$$
b = 2 \times 4.7724656 =  9.5449312. 
$$
Pour trouver $a$, on note que 
$$
\frac{S_m^2/\sigma_2^2}{S_n^2/\sigma_1^2} = 2 \frac{S_m^2}{ S_n^2} \sim F_{(5,9)}.
$$
Alors,
$$
\Pr\left(  \frac{S_n^2}{ S_m^2} \ge a\right) = \Pr\left(  \frac{S_m^2}{ S_n^2} \le \frac{1}{a}\right) = \Pr\left(  2\frac{S_m^2}{ S_n^2} \le \frac{2}{a}\right). 
$$
Donc, $2/a$ est le $95$e quantile de la distribution $F_{(5,9)}$.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qf}\hlstd{(}\hlnum{0.95}\hlstd{,}\hlkwc{df1}\hlstd{=}\hlnum{5}\hlstd{,}\hlkwc{df2}\hlstd{=}\hlnum{9}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 3.481659
\end{verbatim}
\end{kframe}
\end{knitrout}
et ainsi
$$
a = \frac{2}{3.4816587} = 0.5744389.
$$
\item Cette probabilité égale
\begin{align*}
\Pr\left(a \le \frac{S_n^2}{S_m^2}\le b\right) & = \Pr\left(\frac{S_n^2}{S_m^2}\le b\right) - \Pr\left(\frac{S_n^2}{S_m^2}\le a\right)\\
& = \Pr\left(\frac{S_n^2}{S_m^2}\le b\right) + \Pr\left(\frac{S_n^2}{S_m^2}\ge a\right) -1 = 2\times 0,95 -1 = 0,9.
\end{align*}
\end{enumerate}
\end{sol}
\end{exercice}



%\begin{exercice} 
%The following \textsf{R} function returns the sample variances of $N$ random samples of size $n$ drawn from the Normal distribution with mean $\mu$ and variance $\sigma^2$:
%<< >>=
%sample.var <- function(n=10, N=100, mu=0, sd=1){
%    data <- rnorm(n*N, mean=mu, sd=sd)
%    data.mat <- matrix(data, ncol=N)
%    apply(data.mat, 2, var)
%}
%@
%For example, to compute sample variances of $N = 1000$ samples of size $n = 28$ from the $\mathcal{N}(0,1)$, call
%<<eval=FALSE>>=
%sample.var(n=28, N=1000, mu=0, sd=1)
%@
%\begin{enumerate}
%\item Generate $N = 1000$ samples of size $n = 10$ from the $\mathcal{N}(0, 1)$ distribution. Compute $S_n^2$ for each of these samples and summarize the results in a histogram. What is the mean of these sample variances? Is it close to $\sigma^2$? What is the sample variance of your sample of sample variances? Is this what you expect? Overlay the histogram with a plot of the theoretical density of the sampling distribution of the sample variance from Q8. Does this density provide a good approximation to the histogram?
%
%\item Repeat part a) for samples of size $n = 200$. How do the results compare? What does the theoretical density of the sample variance remind you of?
%
%\item Adapt the function \texttt{sample.var} so that it draws samples from the Exponential distribution with mean 1. Redo parts (a) and (b) when the random samples are drawn from this distribution. Although not justified theoretically, you can still overlay the resulting histograms with the theoretical sampling density of the sample variance of a Normal sample with variance 1. What do you see now?
%\end{enumerate}
%\begin{sol}
%\begin{enumerate}
%\item The sample of sample variances can be drawn as follows; the result is stored in the vector \texttt{out}:
%<<>>=
%sample.var <- function(n=10,N=100){
%	data <- rnorm(n*N,mean=0,sd=1)
%	data.mat <- matrix(data,ncol=N)
%	apply(data.mat,2,var)
%	}
%set.seed(28)	
%out <- sample.var(n=10,N=1000)	
%@
%
%The mean and variance of the sample of sample variances are then
%<<>>=
%mean(out)
%var(out)
%@
%The sample mean is indeed close to the theoretical value, i.e., $\sigma^2=1$. The sample variance is also closed to the theoretical value found in Q8, i.e., $2\sigma^4/(n-1)=0.22$. The histogram of the sample variance values along with the theoretical density of the Gamma with shape parameter $\alpha=(n-1)/2=9/2$ and scale parameter $\beta=2\sigma^2/(n-1)=2/9$ can then be plotted as follows:
%<<fig.height=4>>=
%hist(out, freq=FALSE, ylim=c(0,1),
%     main="Histogramme des variances \u{E9}chantillonnales")
%curve(dgamma(x, shape=9/2, scale=2/9), col="red", lwd=2, add=TRUE)
%@
%
%The fit seems to be very good.
%
%\item First, we draw 1000 samples from the Normal distribution of size $n=200$ and then we compute the sample variances, as well as the mean and variance of the resulting sample of sample variances:
%<<>>=
%set.seed(17)
%out <- sample.var(n=200, N=1000)
%mean(out)
%var(out)
%@
%Again, the mean is close to the theoretical variance, i.e., $\sigma^2=1$. Furthermore, compared to part (a), the sample variance is much smaller, indicating that the observed values of $S_n^2$ are now much closer to one. The theoretical density of $S_n^2$ is given by $\mathcal{G}(199/2,2/199)$. The histogram of the sample variances can be produced as follows:
%<<fig.height=4>>=
%hist(out, freq=FALSE, ylim=c(0,5),
%     main="Histogramme des variances \u{E9}chantillonnales")
%curve(dgamma(x, shape=199/2, scale=2/199), col="red", lwd=2, add=TRUE)
%@
%The fit is again very good. In addition, the true density of $S_n^2$ resembles the Normal density. This is not a coincidence, because the $\chi^2_\nu$ distribution can be well approximated by the Normal distribution as $\nu \to \infty$.
% 
%\item The function \texttt{sample.var} can be adapted as follows: 
% <<>>=
% sample.var.exp <- function(n=10,N=100){
%	data <- rexp(n*N,rate=1)
%	data.mat <- matrix(data,ncol=N)
%	apply(data.mat,2,var)
%	}
% @
%First, we compute sample variances  corresponding to $1000$ samples of size $n=5$ and $n=500$ from the Exponential distribution with mean $1$. We also compute the means and variances of the samples of sample variances:
%<<>>=
%set.seed(123)
%out1 <- sample.var.exp(n=10, N=1000)
%mean(out1)
%var(out1)
%out2 <- sample.var.exp(n=200, N=1000)
%mean(out2)
%var(out2)
%@
%We can see that the mean is quite close to the theoretical variance of the Exponential distribution with mean $1$. Also, as $n$ increases from $10$ to $200$, the sample variance decreases. However, the sample variance is considerably higher when compared to the case of Normal samples investigated in parts (a) and (b).
% 
%The histograms and densities of $S_n^2$ computed pretending the samples are drawn from the Normal distribution can be plotted as follows:
%<<fig.height=4>>=
%par(mfrow=c(1,2)) # this draws two pictures next to one another
%hist(out1, freq=FALSE,main="Histogram of sample variances",ylim=c(0,1))
%curve(dgamma(x,shape=9/2,scale=2/9),col="red",lwd=2,add=TRUE)
%hist(out2, freq=FALSE,main="Histogram of sample variances",ylim=c(0,6.5))
%curve(dgamma(x,shape=199/2,scale=2/199),col="red",lwd=2,add=TRUE)
%@
%We can clearly see, especially when $n=200$, that the sampling distribution of the sample variance for exponential samples is not the same as the sampling distribution of the sample variance for Normal samples. It can be shown that, as $n\to \infty$, the sampling distribution of the sample variance converges to the Normal distribution. However, the variance of the limiting Normal distribution depends on the distribution from which the original samples were drawn.
%\end{enumerate}
%\end{sol}
%\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}


%\section*{Exercices proposés dans \cite{Wackerly:mathstat:7e:2008}}
%
%\begin{trivlist}
%\item 3.145--3.151, 3.153, 3.155, 3.158, 3.159, 3.161, 3.162, 3.163
%\end{trivlist}
%

%%%
%%% Insérer les réponses
%%%
\input{reponses-echantillon}



\chapter{Estimation}
\label{chap:estimation}

\Opensolutionfile{reponses}[reponses-estimation]
\Opensolutionfile{solutions}[solutions-estimation]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:estimation}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:estimation}}

\end{Filesave}

%%%
%%% Début des exercices
%%%

% Biais et EQM

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire d'une
  distribution avec moyenne $\mu$ et variance $\sigma^2$. Démontrer que
  $n^{-1} \sum_{i=1}^n (X_i - \mu)^2$ est un estimateur sans biais de
  $\sigma^2$.
  \begin{sol}
    On a
    \begin{align*}
      \Esp{\frac{1}{n}\sum_{i=1}^n(X_i - \mu)^2}
      &= \frac{1}{n} \sum_{i=1}^n \esp{(X_i-\mu)^2} \\
      &= \frac{1}{n} \sum_{i=1}^n \esp{X_i^2 -2\mu X_i + \mu^2} \\
      &= \frac{1}{n} \sum_{i=1}^n (\esp{X_i^2} - \mu^2) \\
      &= \frac{1}{n} \sum_{i=1}^n [(\sigma^2 + \mu^2) - \mu^2] \\
      &= \frac{1}{n} \sum_{i=1}^n \sigma^2 \\
      &= \sigma^2,
    \end{align*}
    d'où l'expression du côté gauche de l'égalité est un estimateur
    sans biais du paramètre $\sigma^2$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Si $X_1, \dots, X_n$ est un échantillon aléatoire d'une
  distribution avec moyenne $\mu$, quelle condition doit-on imposer sur
  les constantes $a_1, \dots, a_n$ pour que
  \begin{displaymath}
    a_1 X_1 + \cdots + a_n X_n
  \end{displaymath}
  soit un estimateur sans biais de $\mu$?
  \begin{rep}
    $\sum_{i=1}^n a_i = 1$
  \end{rep}
  \begin{sol}
    On a,
    \begin{align*}
      \esp{a_1 X_1 + \dots + a_n X_n}
      &= \esp{a_1 X_1} + \dots + \esp{a_n X_n}\\
      &= (a_1 + \dots + a_n) \esp{X_1} \\
      &= (a_1 + \dots + a_n) \mu.
    \end{align*}
    Pour que $a_1 X_1 + \dots + a_n X_n$ soit un estimateur sans biais
    de $\mu$, il faut que $\sum_{i=1}^n a_i = 1$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire d'une
  distribution avec moyenne $\mu$ et variance~$\sigma^2$.
  \begin{enumerate}
  \item Démontrer que $\bar{X}_n^2$ est un estimateur biaisé de $\mu^2$ et
    calculer son biais.
  \item Démontrer que $\bar{X}_n^2$ est un estimateur asymptotiquement
    sans biais de $\mu^2$.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $\sigma^2/n$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Il faut d'abord calculer l'espérance de l'estimateur:
      \begin{align*}
        \esp{\bar{X}_n^2} &= \var{\bar{X}_n} + \esp{\bar{X}_n}^2 \\
        &= \frac{\sigma^2}{n} + \mu^2.
      \end{align*}
      On voit que $\bar{X}_n^2$ est un estimateur biaisé de $\mu^2$ et
      que le biais est $\sigma^2/n$.
    \item Puisque
      \begin{align*}
        \lim_{n \rightarrow \infty} \esp{\bar{X}_n^2} =
        \lim_{n \rightarrow \infty} \frac{\sigma^2}{n} + \mu^2 = \mu^2,
      \end{align*}
      $\bar{X}_n^2$ est un estimateur asymptotiquement sans biais de
      $\mu^2$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soient $X_{(1)} < X_{(2)} < X_{(3)}$ les statistiques d'ordre d'un
  échantillon aléatoire de taille $3$ tiré d'une distribution uniforme
  avec fonction de densité
  \begin{displaymath}
    f(x) = \theta^{-1}, \quad 0 < x < \theta, \quad \theta > 0.
  \end{displaymath}
  Démontrer que $4 X_{(1)}$ et $2 X_{(2)}$ sont tous deux des
  estimateurs sans biais de $\theta$. Trouver la variance de chacun de
  ces estimateurs.
  \begin{rep}
    $\var{4X_{(1)}} = 3 \theta^2/5$, $\var{2X_{(2)}} = \theta^2/5$.
  \end{rep}
  \begin{sol}
    La fonction de densité de probabilité de la $k${\ieme} statistique
    d'ordre est donnée par le Théorème 6.5 des notes de cours.
    $$
    f_{X_{(k)}}(x)=\frac{n!}{(k-1)!(n-k)!}F(x)^{k-1}\{1-F(x)\}^{n-k}f(x)
    $$
    Pour $n = 3$ et $X_i \sim U(0, \theta)$, $i = 1, 2, 3$, on a
    \begin{align*}
      f_{X_{(1)}}(x)
      &= 3 \left(1 - \frac{x}{\theta} \right)^2
      \left( \frac{1}{\theta} \right) \\
      &= 3 \left(
        \frac{1}{\theta} - \frac{2x}{\theta^2} + \frac{x^2}{\theta^3}
      \right), \quad 0 < x < \theta \\
      \intertext{et}
      f_{X_{(2)}}(x)
      &= 6 \left( \frac{x}{\theta} \right)
      \left( 1 - \frac{x}{\theta} \right)
      \left( \frac{1}{\theta} \right) \\
      &= 6 \left(
        \frac{x}{\theta^2} - \frac{x^2}{\theta^3}
      \right), \quad 0 < x < \theta.
    \end{align*}
    Ainsi, d'une part,
    \begin{align*}
      \esp{4X_{(1)}} &= 4 \int_0^{\theta} x f_{X_{(1)}}(x)\, dx \\
      &= 12 \int_0^{\theta}
      \left(
        \frac{x}{\theta} - \frac{2x^2}{\theta^2} + \frac{x^3}{\theta^3}
      \right)\, dx \\
      &= 12 \left(
        \frac{\theta}{2} - \frac{2 \theta}{3} + \frac{\theta}{4}
      \right) \\
      &= \theta \\
      \intertext{et} \displaybreak[0]
      \esp{2X_{(2)}} &= 2 \int_0^{\theta} x f_{X_{(2)}}(x)\, dx \\
      &= 12 \int_0^\theta
      \left(
        \frac{x^2}{\theta^2} - \frac{x^3}{\theta^3}
      \right)\, dx \\
      &= 12 \left( \frac{\theta}{3} - \frac{\theta}{4} \right) \\
      &= \theta,
    \end{align*}
    d'où $4 X_{(1)}$ et $2 X_{(2)}$ sont des estimateurs sans biais de
    $\theta$. D'autre part,
    \begin{align*}
      \esp{(4X_{(1)})^2} &= 16 \int_0^{\theta} x^2 f_{X_{(1)}}(x)\, dx \\
      &= 48 \int_0^{\theta}
      \left(
        \frac{x^2}{\theta} - \frac{2x^3}{\theta^2} + \frac{x^4}{\theta^3}
      \right)\, dx \\
      &= 48 \left(
        \frac{\theta^2}{3} - \frac{2 \theta^2}{4} + \frac{\theta^2}{5}
      \right) \\
      &= \frac{8 \theta^2}{5} \\
      \intertext{et} \displaybreak[0]
      \esp{(2X_{(2)})^2} &= 4 \int_0^{\theta} x^2 f_{X_{(2)}}(x)\, dx \\
      &= 24 \int_0^\theta
      \left(
        \frac{x^3}{\theta^3} - \frac{x^4}{\theta^3}
      \right)\, dx \\
      &= 24 \left( \frac{\theta^2}{4} - \frac{\theta^2}{5} \right) \\
      &= \frac{6 \theta^2}{5}.
    \end{align*}
    Par conséquent,
    \begin{align*}
      \var{4 X_{(1)}}
      &= \frac{8 \theta^2}{5} - \theta^2 = \frac{3 \theta^2}{5} \\
      \intertext{et}
      \var{2 X_{(2)}}
      &= \frac{6 \theta^2}{5} - \theta^2 = \frac{\theta^2}{5}.
    \end{align*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire d'une distribution
  uniforme sur l'intervalle $(0, \theta)$.
  \begin{enumerate}
  \item Développer un estimateur sans biais de $\theta$ basé sur
    $\max(X_1, \dots, X_n)$.
  \item Répéter la partie a), mais cette fois à partir de $\min(X_1,
    \dots, X_n)$.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $(n + 1) X_{(n)}/n$
    \item $(n + 1) X_{(1)}$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Soit $X_{(n)} = \max(X_1, \dots, X_n)$ et $X_{(1)} = \min(X_1,
    \dots, X_n)$, où $X_i \sim U(0, \theta)$, $i = 1, \dots, n$. On
    sait alors que
    \begin{align*}
      f_{X_{(n)}}(x)
      &= n (F(x))^{n - 1} f(x) \\
      &= \frac{n x^{n - 1}}{\theta^n}, \quad 0 < x < \theta \\
      \intertext{et que}
      f_{X_{(1)}}(x)
      &= n (1 - F(x))^{n - 1} f(x) \\
      &= \frac{n (\theta - x)^{n - 1}}{\theta^n}, \quad 0 < x < \theta.
    \end{align*}
    \begin{enumerate}
    \item On souhaite développer un estimateur sans biais de $\theta$
      basé sur $X_{(n)}$. En premier lieu,
      \begin{align*}
        \esp{X_{(n)}} &= \int_0^\theta x f_{X_{(n)}}(x)\, dx \\
        &= \frac{n}{\theta^n}\int_0^\theta x^n \,dx\\
        &= \frac{n \theta}{n + 1}.
      \end{align*}
      Un estimateur sans biais de $\theta$ est donc $(n + 1)
      X_{(n)}/n$.
    \item Comme en a), on calcule d'abord l'espérance de la statistique:
      \begin{align*}
        \esp{X_{(1)}} &= \int_0^\theta x f_{X_{(1)}}(x)\, dx \\
        &= \frac{n}{\theta^n}
        \int_{0}^\theta
        x (\theta - x)^{n - 1}x\,dx \\
        &=\frac{\theta}{n + 1}
      \end{align*}
      en intégrant par parties. Un estimateur sans biais de $\theta$
      basé sur le minimum de l'échantillon est donc $(n + 1) X_{(1)}$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X \sim \text{Binomiale}(n, p)$. Démontrer que, malgré que
  $X/n$ soit un estimateur sans biais de $p$,
  \begin{displaymath}
    n \left(\frac{X}{n}\right) \left(1 - \frac{X}{n}\right)
  \end{displaymath}
  est un estimateur biaisé de la variance de $X$. Calculer le biais de
  l'estimateur.
  \begin{rep}
    $-p (1 - p)$
  \end{rep}
  \begin{sol}
    On sait que $\esp{X} = np$ et que $\var{X} = np (1 - p)$. Or,
    \begin{align*}
      \Esp{n\left(\frac{X}{n}\right)\left(1-\frac{X}{n}\right)} &=
      \esp{X} - \frac{\esp{X^2}}{n}\\
      &= np - \frac{\var{X} + \esp{X}^2}{n}\\
      &= np - \frac{np(1 - p) + n^2 p^2}{n}\\
      &= (n - 1) p (1 - p) \\
      &= n p (1 - p) - p(1 - p).
    \end{align*}
    La statistique est donc un estimateur biaisé de la variance et le
    biais est $-p (1 - p)$. La statistique sur-estime la variance.
  \end{sol}
\end{exercice}

% Convergence

\begin{exercice}
  Démontrer, à partir de la définition, que $X_{(1)} = \min(X_1,
  \dots, X_n)$ est un estimateur convergent du paramètre $\theta$
  d'une loi uniforme sur l'intervalle $(\theta, \theta + 1)$.
  \begin{sol}
    Il faut démontrer que $\lim_{n \rightarrow \infty}
    \prob{\abs{X_{(1)} - \theta} < \epsilon} = 1$. On sait que si $X
    \sim U(\theta, \theta + 1)$, alors
    \begin{align*}
      f_X(x) &= 1, \quad \theta < x < \theta + 1 \\
      F_X(x) &= x - \theta, \quad \theta < x < \theta + 1
    \end{align*}
    et
    \begin{align*}
      f_{X_{(1)}}(x) &= n f_X(x) (1 - F_X(x))^{n - 1} \\
      &= n (1 - x + \theta)^{n - 1}, \quad \theta < x < \theta + 1.
    \end{align*}
    Ainsi,
    \begin{align*}
      \prob{\abs{X_{(1)} - \theta} < \epsilon}
      &= \prob{\theta - \epsilon < X_{(1)} < \theta + \epsilon} \\
      &= \int_\theta^{\theta + \epsilon} n (1 - x + \theta)^{n-1}\, dx \\
      &= 1 - (1 - \epsilon)^n.
    \end{align*}
    Or, cette dernière expression tend vers $1$ lorsque $n$ tend vers
    l'infini, ce qui complète la démonstration.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire d'une loi
  exponentielle de moyenne $\theta$. Démontrer que $\bar{X}$ est un
  estimateur convergent de $\theta$.
  \begin{sol}
    Puisque $\bar{X}$ est un estimateur sans biais de la moyenne d'une
    distribution, quelqu'elle fut, et que $\lim_{n \rightarrow \infty}
    \var{\bar{X}} = \lim_{n \rightarrow \infty} \var{X}/n = 0$, alors
    $\bar{X}$ est toujours un estimateur convergent de la
    moyenne.
  \end{sol}
\end{exercice}

% Efficacité relative

\begin{exercice}
Soit $X_1,\ldots,X_n$ un échantillon aléatoire de taille $n\geq 3$ d'une population avec moyenne $\mu$ et variance $\sigma^2 > 0$. On considère les trois estimateurs suivants pour $\mu$:
$$
\hat\mu_1=\frac{X_1+X_2}{2},\quad\quad \hat\mu_2=\frac{X_1}{4}+\frac{X_2+\cdots+X_{n-1}}{2(n-2)}+\frac{X_n}{4}, \quad\mbox{ et }\quad \hat\mu_3=\sum_{j=1}^{n}\frac{X_j}{n}.
$$
\begin{enumerate}
\item Montrer que ces estimateurs sont sans biais.
\item Trouver l'efficacité de $\hat\mu_3$ par rapport à $\hat\mu_2$ et $\hat\mu_1$, respectivement.
\item Quel estimateur est préférable et pourquoi?
\end{enumerate}
\begin{rep}
b)  $\frac{n^2}{8(n-2)}$ et $\frac{n}{2}$ \quad c) ${\hat \mu}_3$
\end{rep}
\begin{sol}
\begin{enumerate}
\item On peut conclure que ${\hat \mu}_1$ est un estimateur sans biais de $\mu$ par le fait que
$$
\ex({\hat \mu}_1) = \frac{1}{2} \, \{ \ex (X_1) + \ex(X_2)\} = \frac{1}{2} \, (\mu + \mu) = \mu.
$$ 
De même, ${\hat \mu}_2$ est un estimateur sans biais de $\mu$, car
\begin{eqnarray*}
\ex ({\hat \mu}_2) &=& \frac{1}{4} \, \ex (X_1) + \frac{1}{2(n-2)} \, \sum_{i=2}^{n-1} \ex (X_i) + \frac{1}{4} \, \ex (X_n) \\
&=& \frac{1}{4} \, \mu + \frac{1}{2(n-2)} \, (n-2)\mu + \frac{1}{4} \, \mu \\
&=& \frac{1}{4} \, \mu + \frac{1}{2} \, \mu + \frac{1}{4} \, \mu = \mu.
\end{eqnarray*}
Finalement, ${\hat \mu}_3$ est un estimateur sans biais de $\mu$ puisque
$$
\ex ( {\hat \mu}_3) = \frac{1}{n} \, \sum_{i=1}^n \ex (X_i) = \frac{n\mu}{n} = \mu.
$$

\item Par définition,
$$
\mbox{eff} ({\hat \mu}_3, {\hat \mu}_2) = \frac{\vr({\hat \mu}_2)} {\vr ({\hat \mu}_3)} \quad \mbox{et} \quad \mbox{eff} ({\hat \mu}_3, {\hat \mu}_1) = \frac{\vr({\hat \mu}_1)} {\vr ({\hat \mu}_3)} \,.  
$$
Pour calculer ces ratios, on doit commencer par déterminer la variance de chacun des trois estimateurs. D'abord, on trouve
$$
\vr({\hat \mu}_1) = \frac{1}{4} \, \{ \vr (X_1) + \vr (X_2) \} = \frac{\sigma^2}{2} \, .
$$
Ensuite,
\begin{eqnarray*}
\vr({\hat \mu}_2) &=& \frac{1}{16} \, \vr( X_1) + \frac{1}{4(n-2)^2} \, \sum_{i=2}^{n-1} \vr (X_i) + \frac{1}{16} \, \vr( X_n) \\
&=& \frac{1}{16} \, \sigma^2 + \frac{1}{4(n-2)^2} \, (n-2) \sigma^2 + \frac{1}{16} \, \sigma^2 \\
&=& \frac{2(n-2) + 4}{16(n-2)} \, \sigma^2 \\
&=& \frac{n}{8(n-2)} \, \sigma^2.
\end{eqnarray*}
Finalement,
$$
\vr({\hat \mu}_3) = \frac{1}{n^2} \, \sum_{i=1}^n \vr (X_i) = \frac{n}{n^2} \, \sigma^2 = \frac{\sigma^2}{n} \, .
$$
Il en découle que 
$$
\mbox{eff} ({\hat \mu}_3, {\hat \mu}_2) =  \frac{\displaystyle\frac{n\sigma^2}{8(n-2)}}{\displaystyle\frac{\sigma^2}{n}} =  \frac{n^2}{8(n-2)}
$$
et
$$
\mbox{eff} ({\hat \mu}_3, {\hat \mu}_1) =  \frac{\displaystyle\frac{\sigma^2}{2}}{\displaystyle\frac{\sigma^2}{n}} =  \frac{n}{2} \, .
$$

\item Pour toute valeur de $n \ge 3$, on a
$$
\mbox{eff} ({\hat \mu}_3, {\hat \mu}_1) > 1 \quad \mbox{et} \quad \mbox{eff} ({\hat \mu}_3, {\hat \mu}_2) > 1.
$$
Donc, ${\hat \mu}_3$ est toujours préférable à ${\hat \mu}_1$ ou ${\hat \mu}_2$.
\end{enumerate}
\end{sol}
\end{exercice}

% Exhaustivité

\begin{exercice}
Soit $X_1,\ldots,X_n$ un échantillon aléatoire avec fonction de répartition
$$
F(x)=\begin{cases}
0,& x<\beta\\
1-(\beta/x)^\alpha, & x\geq \beta,
\end{cases}
$$
où $\alpha,\beta>0$. La fonction de densité de probabilité correspondante est
$$
f(x)=\begin{cases}
\alpha\beta^\alpha x^{-(\alpha+1)}, & x\geq \beta,\\
0,& \mbox{ailleurs}.
\end{cases}
$$
\begin{enumerate}
\item Trouver la fonction de répartition de $\hat\beta=\min(X_1,\ldots,X_n)$.

\item Montrer que $\hat\beta$ est un estimateur convergent de $\beta$.

\item Calculer le biais et l'erreur quadratique moyenne de l'estimateur $\hat\beta$. Est-ce que cet estimateur est sans biais ou asymptotiquement sans biais?

\item Si $\alpha$ est connu, montrer que $\min(X_1,\ldots,X_n)$ est une statistique exhaustive pour $\beta$.

\item Si $\beta$ est connu, montrer que $X_1\times\cdots\times X_n$ est une statistique exhaustive pour $\alpha$.

\item Trouver une paire de statistiques conjointement exhaustives dans le cas où $\alpha$ et $\beta$ sont inconnus.
\end{enumerate}
\begin{sol}
\begin{enumerate}
\item La fonction de répartition de $\hat \beta=\min(X_1,\ldots,X_n)$, pour $x\geq \beta$, est
\begin{align*}
\Pr[\min(X_1,\ldots,X_n)\leq x] & = 1- \Pr[\min(X_1,\ldots,X_n)> x]\\
&= 1-\Pr[X_1>x,\ldots,X_n>x]\\
&=1-\Pr[X_1>x]\times\cdots\times\Pr[X_n>x],\mbox{ par indépendance}\\
&=1-\{\Pr[X_1>x]\}^n,\mbox{ par i.d.}\\
&=1-\left\{\frac{\beta}{x}\right\}^{\alpha n}\\
\end{align*}
Donc, $\min(X_1,\ldots,X_n)$ a la même fonction de répartition que $X$ avec un nouveau paramètre $\alpha^\star=\alpha n$. Ce qui signifie que la densité est
$$
f_{\min}(x)=\alpha n \beta^{\alpha n}x^{-(\alpha n+1)},\quad x\geq \beta.
$$

\item On utilise la définition d'un estimateur convergent. Si $\varepsilon>0$,
\begin{align*}
\Pr[|\hat\beta-\beta|\leq \varepsilon]&=\Pr[\beta-\varepsilon<\hat\beta\leq \beta+\varepsilon] \\
&=\Pr[\hat\beta\leq \beta+\varepsilon],
\end{align*}
puisque $\Pr[\hat\beta<\beta]=0$ par le fait que le domaine du minimum est le même que celui des observations, $[\beta,\infty)$. Ainsi,
\begin{align*}
\Pr[|\hat\beta-\beta|\leq \varepsilon]&=\Pr[\hat\beta\leq \beta+\varepsilon]=1-\left(\frac{\beta}{\beta+\varepsilon}\right)^{\alpha n}.
\end{align*}
Puisque $\varepsilon$ est positif, le ratio $\frac{\beta}{\beta+\varepsilon}<1$ et donc $\left(\frac{\beta}{\beta+\varepsilon}\right)^{\alpha n}\to 0$ quand $n\to\infty$. Ainsi,
$$
\lim_{n\to\infty} \Pr[|\hat\beta-\beta|\leq \varepsilon]=1,
$$
et $\hat\beta$ est convergent pour $\beta$.

\item On trouve d'abord $\ex[\hat\beta]$ et $\ex[\hat\beta^2]$:
\begin{align*}
\ex[\hat\beta]&=\int_\beta^\infty x \alpha n \beta^{\alpha n}x^{-(\alpha n+1)}\d x = \int_\beta^\infty  \alpha n \beta^{\alpha n}x^{-\alpha n}\d x = \frac{\alpha n\beta}{\alpha n-1}\\
\ex[\hat\beta^2]&=\int_\beta^\infty x^2 \alpha n \beta^{\alpha n}x^{-(\alpha n+1)}\d x = \int_\beta^\infty  \alpha n \beta^{\alpha n}x^{-(\alpha n-1)}\d x = \frac{\alpha n\beta^2}{\alpha n-2}.
\end{align*}
Le biais est donc
$$
B(\hat\beta)=\ex[\hat\beta]-\beta = \frac{\alpha n\beta}{\alpha n-1}-\beta = \frac{\beta}{\alpha n -1}\to 0
$$
quand $n\to \infty$. L'estimateur est donc asymptotiquement sans biais.

La variance est
$$
\vr(\hat\beta)= \frac{\alpha n\beta^2}{\alpha n-2}-\left(\frac{\alpha n\beta}{\alpha n-1}\right)^2 = \frac{\alpha n\beta^2}{(\alpha n-2)(\alpha n-1)^2}
$$
et l'erreur quadratique moyenne est
$$
\mbox{EQM}(\hat\beta)=\vr(\hat\beta)+B^2(\hat\beta)= \frac{\alpha n\beta^2}{(\alpha n-2)(\alpha n-1)^2}+\frac{\beta^2}{(\alpha n -1)^2}=\frac{2\beta^2}{(\alpha n-1)(\alpha n-2)}.
$$

\item Si $\alpha$ est connu, le paramètre inconnu est $\beta$ et 
$$
f(x_1|\alpha,\beta) \times \dots \times f(x_n|\alpha,\beta)  =\underbrace{ \left(\prod_{i=1}^n x_i\right)^{-(\alpha+1)}}_{h(x_1, \dots, x_n)} \underbrace{(\alpha \beta^\alpha)^n\mathbf{1} \{ \min(x_1,\dots, x_n) \ge \beta\} }_{g\{ \min(x_1,\dots, x_n), \beta \}}.
$$
Par le théorème de factorisation de Fisher--Neyman, $\min(X_1,\dots, X_n)$ est une statistique exhaustive pour $\beta$.

\item On observe que
\begin{align*}
f(y_1|\alpha,\beta) \times \dots \times f(x_n|\alpha,\beta)  & = \alpha\beta^{\alpha}x_1^{-(\alpha+1)} \mathbf{1}(x_1 \ge \beta)\times \dots \times \alpha\beta^{\alpha} x_n^{-(\alpha+1)} \mathbf{1}(x_n \ge \beta)\\
& = (\alpha \beta^\alpha)^n \left(\prod_{i=1}^n x_i\right)^{-(\alpha+1)} \times \mathbf{1} \{ \min(x_1,\dots, x_n) \ge \beta\}. 
\end{align*}
Si $\beta$ est connu, le paramètre incconu est $\alpha$. Puisque
$$
f(x_1|\alpha,\beta) \times \dots \times f(x_n|\alpha,\beta)  =\underbrace{(\alpha \beta^\alpha)^n \left(\prod_{i=1}^n x_i\right)^{-(\alpha+1)}}_{g(\prod x_i , \alpha)} \underbrace{\mathbf{1} \{ \min(x_1,\dots, x_n) \ge \beta\}}_{h(x_1,\dots, x_n)},
$$
on peut conclure par le théorème de factorisation de Fisher--Neyman que $X_1 \times \cdots \times X_n$ est une statistique exhaustive pour $\alpha$. 

\item Lorsque $\alpha$ et $\beta$ sont inconnus,
$$
f(x_1|\alpha,\beta) \times \dots \times f(x_n|\alpha,\beta)  =\underbrace{(\alpha \beta^\alpha)^n \left(\prod_{i=1}^n x_i\right)^{-(\alpha+1)}\mathbf{1} \{ \min(x_1,\dots, x_n) \ge \beta\}}_{g\{\prod x_i, \min(x_1,\dots, x_n), \alpha,\beta\}}
$$
donc les statistiques $X_1 \times \cdots \times X_n$ et $\min(X_1,\dots, X_n)$ sont conjointement exhaustives pour $\alpha$ et $\beta$.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1$ une observation d'une loi normale avec moyenne $0$ et
  variance $\sigma^2$, $\sigma > 0$. Démontrer que $\abs{X_1}$ est une
  statistique exhaustive pour $\sigma^2$.
  \begin{sol}
    On a un échantillon aléatoire de taille 1. Or,
    \begin{align*}
      f(x_1; \sigma^2)
      &= \frac{1}{\sqrt{2\pi \sigma^2}}\, e^{-x_1^2/(2 \sigma^2)} \\
      &= \frac{1}{\sqrt{2\pi\sigma^2}}\, e^{-\abs{x_1}^2/(2 \sigma^2)} \\
      &= g(\abs{x_1}; \sigma^2) h(x_1), \\
      \intertext{avec}
      g(x; \sigma^2)
      &= \frac{1}{\sqrt{2\pi\sigma^2}}\, e^{-x^2/(2 \sigma^2)}
    \end{align*}
    et $h(x) = 1$. Ainsi, par le théorème de factorisation de Fisher--Neyman,
    $\abs{X_1}$ est une statistique exhaustive pour $\sigma^2$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Trouver une statistique exhaustive pour le paramètre $\theta$ de la
  loi uniforme sur l'intervalle $(-\theta, \theta)$.
  \begin{rep}
    $\max_{i = 1, \dots, n}(\abs{X_i})$, ou $(X_{(1)}, X_{(n)})$
  \end{rep}
  \begin{sol}
    Soit $X_1, \dots, X_n$ un échantillon aléatoire d'une distribution
    uniforme sur l'intervalle $(-\theta, \theta)$. La fonction de
    vraisemblance de cet échantillon est
    \begin{equation*}
      f(x_1, \dots, x_n; \theta) =
      \begin{cases}
        (2 \theta)^{-n}, & -\theta < x_i < \theta, i = 1, \dots, n \\
        0, & \text{ailleurs}.
      \end{cases}
    \end{equation*}
    La fonction de vraisemblance est donc non nulle seulement si
    toutes les valeurs de l'échantillon se trouvent dans l'intervalle
    $(-\theta, \theta)$ ou, de manière équivalente, si $\max_{i = 1,
      \dots, n} (\abs{x_i}) < \theta$. On peut donc, par exemple,
    réécrire la fonction de vraisemblance sous la forme
    \begin{align*}
      f(x_1, \dots, x_n; \theta)
      &= \left( \frac{1}{2\theta} \right)^n
      I_{\{0 \leq \max_{i = 1, \dots, n}(\abs{x_i}) \leq \theta)\}} \\
      &= g \left( \max_{i = 1, \dots, n}(\abs{x_i}); \theta \right)
      h(x_1, \dots, x_n), \\
      \intertext{avec}
      g(x; \theta)
      &= \left( \frac{1}{2\theta} \right)^n
      I_{\{0 \leq x \leq \theta)\}}
    \end{align*}
    et $h(x_1, \dots, x_n) = 1$. Ainsi, par le théorème de
    factorisation de Fisher--Neyman, on établit que $T = \max_{i = 1, \dots,
      n}(\abs{X_i})$ est une statistique exhaustive pour le paramètre
    $\theta$. Une autre factorisation possible serait
    \begin{equation*}
      f(x_1, \dots, x_n; \theta) =
      \left( \frac{1}{2\theta} \right)^n
      I_{\{-\infty < x_{(n)} < \theta)\}}
      I_{\{-\theta < x_{(1)} < \infty)\}},
    \end{equation*}
    ce qui donne comme statistique exhaustive $T = (X_{(1)}, X_{(n)})$.
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:ponctuelle:fam_exponentielle}
  Démontrer que la somme des éléments d'un échantillon aléatoire issu
  d'une loi de Poisson est une statistique exhaustive pour le
  paramètre de cette loi.
  \begin{sol}
    Nous allons démontrer un résultat général applicable à plusieurs
    distributions, dont la Poisson. Il existe une famille de
    distributions que l'on nomme la \emph{famille exponentielle} (il
    ne s'agit pas d'une référence à la densité exponentielle, bien que
    cette dernière soit un cas particulier de la famille
    exponentielle). Cette famille comprend toutes les distributions
    dont la densité peut s'écrire sous la forme
    \begin{displaymath}
      f(x; \theta) = h(x) c(\theta) e^{\eta(\theta) t(x)},
    \end{displaymath}
    où $h$, $c$, $\eta$ et $t$ sont des fonctions quelconques. Par
    exemple, la fonction de masse de probabilité de la loi de Poisson
    peut s'écrire comme suit:
    \begin{align*}
      f(x; \theta) &= \frac{\theta^x e^{-\theta}}{x!} \\
      &= \left( \frac{1}{x!} \right) e^{-\theta} e^{\ln(\theta) x} \\
      &= h(x) c(\theta) e^{\eta(\theta) t(x)},
    \end{align*}
    avec $h(x) = (x!)^{-1}$, $c(\theta) = e^{-\theta}$, $\eta(\theta)
    = \ln \theta$ et $t(x) = x$. La loi est donc membre de la famille
    exponentielle. Les lois binomiale, gamma, normale et bêta font
    aussi partie de la famille exponentielle. En revanche, des lois
    comme l'uniforme sur $(0,\theta)$ et l'exponentielle translatée
    n'en font pas partie.

    Pour tous les membres de la famille exponentielle, on a
    \begin{equation*}
      f(x_1, \dots, x_n; \theta) =
      \left( \prod_{i = 1}^n h(x_i) \right)
      (c(\theta))^n
      e^{ \eta(\theta) \sum_{i=1}^n x_i}.
    \end{equation*}
    Ainsi, on voit que le théorème de factorisation permet de conclure
    que la statistique
    \begin{displaymath}
      T(X_1, \dots, X_n)  = \sum_{i = 1}^n X_i
    \end{displaymath}
    est une statistique exhaustive pour le paramètre $\theta$ pour
    tous les membres de la famille exponentielle, dont la loi de
    Poisson.
  \end{sol}
\end{exercice}

\begin{exercice}
Soit $X_1, \dots, X_n$ un échantillon aléatoire tiré d'une loi de Poisson avec paramètre $\lambda$ inconnu. On sait que $T(X_1, \dots, X_n) = \sum_{i=1}^n X_i$ est une statistique exhaustive pour $\lambda$. On estime $\lambda$ par $\tilde{\lambda} = X_1$.
\begin{enumerate}
\item Montrer que $\tilde{\lambda}$ est un estimateur sans biais de $\lambda$.
\item Utiliser le théorème de Rao--Blackwell pour trouver un estimateur $\lambda^\ast$ à partir de~$\tilde{\lambda}$.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item On trouve que $\tilde{\lambda}$ est sans biais pour $\lambda$ par le fait que
$$
\ex[\tilde{\lambda}] = \ex[X_1] = \lambda.
$$

\item Par le théorème de Rao--Blackwell, on trouve
$$
\lambda^\ast = \ex \left[ \tilde{\lambda} | T \right] = \ex \left[ X_1 | \sum_{i=1}^n X_i = t \right].
$$
Par contre, on remarque que
$$
\sum_{i=1}^n \ex \left[ X_i | \sum_{i=1}^n X_i = t \right] = \ex \left[\sum_{i=1}^n X_i | \sum_{i=1}^n X_i = t \right] = t.
$$
Puisque $X_1, \dots, X_n$ sont i.i.d., chaque élément de la somme doit être équivalent et donc égal à $t/n$.
On trouve que
$$
\lambda^\ast = \frac{T}{n} = \frac{\sum_{i=1}^n X_i}{n} = \bar{X}.
$$
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire d'une loi
  géométrique avec fonction de masse de probabilité
  \begin{displaymath}
    \prob{X = x} = \theta (1 - \theta)^x, \quad x = 0, 1, \dots
  \end{displaymath}
  Démontrer que $T(X_1, \dots, X_n) = \sum_{i=1}^n X_i$ est une
  statistique exhaustive pour $\theta$.
  \begin{sol}
    On peut réécrire la fonction de masse de probabilité de la loi
    géométrique comme suit:
    \begin{align*}
      \prob{X = x} &= \theta e^{\ln(1 - \theta) x} \\
      &= h(x) c(\theta) e^{\eta(\theta) t(x)},
    \end{align*}
    avec $h(x) = 1$, $c(\theta) = \theta$, $\eta(\theta) = \ln (1 -
    \theta)$ et $t(x) = x$. La loi géométrique est donc membre de la
    famille exponentielle (voir la solution de
    l'exercice~\ref{chap:estimation}.\ref{ex:ponctuelle:fam_exponentielle}).
    Par conséquent, $T(X_1, \dots, X_n) = \sum_{i=1}^n X_i$ est une
    statistique exhaustive pour le paramètre $\theta$.
  \end{sol}
\end{exercice}


% MVUE et Borne Cramér-Rao

\begin{exercice}
Soit $X_1,\ldots,X_n$ un échantillon aléatoire d'une loi de Poisson avec moyenne $\lambda$.
\begin{enumerate}
\item Démontrer que $T=\sum_{i=1}^n X_i$ est exhaustive minimale pour $\lambda$.
\item Est-ce que $\bar X_n=T/n$ est un estimateur sans biais de variance minimale (MVUE) pour $\lambda$? Expliquer.
\item L'\emph{inégalité de Cram\'er--Rao--Fr\'echet} indique que si $\hat\lambda_n$ est un estimateur sans biais de $\lambda$, alors
$$
\vr(\hat\lambda_n)\geq \left\{n \ex\left[-\frac{\partial^2}{\partial\lambda^2}\ln f(X;\lambda)\right]\right\}^{-1}.
$$
Un estimateur avec variance égale à la borne inférieure est dit être \emph{efficace}. Montrer que $\bar X_n$ est en effet un estimateur efficace pour $\lambda$.
\end{enumerate}
\begin{sol}
\begin{enumerate}
\item Puisque
$$
\Pr[X_1=x_1,\ldots,X_n=x_n]=\prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!}=\lambda^{\sum_{i=1}^n x_i}e^{-\lambda n} \frac{1}{\prod_{i=1}^n x_i!}
$$
se factorise en $g(\sum_{i=1}^n x_i,\lambda)=\lambda^{\sum_{i=1}^n x_i}e^{-\lambda n}$ et $h(x_1,\ldots,x_n)=\frac{1}{\prod_{i=1}^n x_i!}$, le théorème de factorisation de Fisher--Neyman implique que $\sum_{i=1}^n X_i$ est une statistique exhaustive pour $\lambda$. On voit facilement avec le critère de Lehman-Scheffé que cette statistique est exhaustive minimale.

\item La règle du pouce pour trouver un MVUE est de trouver un estimateur qui est sans biais et qui est basé sur une statistique exhaustive obtenue par le théorème de factorisation de Fisher--Neyman. On a 
$$
\ex[\bar X_n]=\lambda,
$$
est donc sans biais et $\bar X_n=T/n$, où $T$ est une statistique exhaustive pour $\lambda$, comme montré en a).
Ainsi, $\bar X_n$ est un MVUE pour $\lambda$.

\item Dans ce cas,
$$
\ln \{ f(x;\lambda)\} = -\lambda   +x \ln (\lambda) - \ln(x!)
$$
et 
$$
\frac{\partial^2}{\partial \lambda^2} \, f(x;\lambda) = - \frac{x}{\lambda^2}.
$$
Ainsi,
$$
n \ex\left[ -\frac{\partial^2}{\partial \lambda^2} \ln \{f(X;\lambda)\}\right] = \frac{n}{\lambda^2} \ex(X) = \frac{n}{\lambda}.
$$
La borne inférieure de Cram\'er--Rao--Fr\'echet est donc
$$
\frac{\lambda}{n} = \vr(\bar X_n),
$$
ce qui montre que $\bar X_n$ est un estimateur efficace pour $\lambda$.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Démontrer que, sous les hypothèses appropriées,
  \begin{displaymath}
    \Esp{\left( \frac{\partial}{\partial \theta}
        \ln f(X; \theta) \right)^2} =
    - \Esp{\frac{\partial^2}{\partial \theta^2}
      \ln f(X; \theta)}.
  \end{displaymath}
  Pour ce faire, dériver par rapport à $\theta$ l'identité
  \begin{displaymath}
    \int_{-\infty}^\infty f(x; \theta)\, dx = 1
  \end{displaymath}
  afin d'obtenir
  \begin{displaymath}
    \int_{-\infty}^\infty
    \left(
      \frac{\partial}{\partial \theta} \ln f(x; \theta)
    \right) f(x; \theta)\, dx = 0,
  \end{displaymath}
  puis dériver de nouveau par rapport à $\theta$.
  \begin{sol}
    Pour commencer, on a l'identité suivante:
    \begin{align*}
      \frac{\partial}{\partial \theta} \ln f(x;\theta) &=
      \frac{1}{f(x;\theta)}\frac{\partial}{\partial \theta}
      f(x;\theta) \\
      \intertext{qui peut être réécrite sous la forme}
      \left(
        \frac{\partial}{\partial \theta} \ln f(x;\theta)
      \right) f(x;\theta)
      &= \frac{\partial}{\partial \theta} f(x;\theta).
    \end{align*}
    Ainsi, en dérivant de part et d'autre
    \begin{displaymath}
      \int_{-\infty}^\infty f(x; \theta)\, dx = 1,
    \end{displaymath}
    par rapport à $\theta$, on obtient
    \begin{equation*}
      \int_{-\infty}^\infty
      \left(
        \frac{\partial}{\partial \theta} \ln f(x;\theta)
        \right) f(x;\theta)\, dx = 0.
    \end{equation*}
    En dérivant une seconde fois cette identité, on a alors
    \begin{gather*}
      \int_{-\infty}^\infty
      \left(
        \frac{\partial^2}{\partial \theta^2} \ln f(x;\theta)
      \right) f(x;\theta)\, dx +
      \int_{-\infty}^\infty
      \left(
        \frac{\partial}{\partial \theta} \ln f(x;\theta)
      \right)
      \left(
        \frac{\partial}{\partial \theta} f(x;\theta)
      \right) dx = 0 \\
      \intertext{ou, de manière équivalente,}
      \int_{-\infty}^\infty
      \left(
        \frac{\partial}{\partial \theta} \ln f(x;\theta)
      \right)^2
      f(x;\theta)\, dx =
      - \int_{-\infty}^\infty
      \left(
        \frac{\partial^2}{\partial \theta^2} \ln f(x;\theta)
      \right) f(x;\theta)\, dx, \\
      \intertext{soit}
      \Esp{\left( \frac{\partial}{\partial \theta}
          \ln f(X; \theta) \right)^2} =
      - \Esp{\frac{\partial^2}{\partial \theta^2}
        \ln f(X; \theta)}.
    \end{gather*}
  \end{sol}
\end{exercice}

\begin{exercice}
  Démontrer que la moyenne arithmétique est un estimateur sans biais
  à variance minimale du paramètre $\lambda$ d'une loi de Poisson.
  \begin{sol}
    On sait que $\esp{\bar{X}_n} = \esp{X}$ pour toute distribution et
    donc que $\bar{X}$ est toujours un estimateur sans biais de la
    moyenne. Pour une loi de Poisson, la moyenne est égale à $\lambda$
    et donc $\bar{X}$ est un estimateur sans biais de $\lambda$. Pour
    démontrer que la statistique est un estimateur à variance
    minimale, il faut montrer que $\bar{X}_n$ est une statistique exhaustive minimale pour $\lambda$. On a que, pour tout $x_1,\ldots,x_n, \in\mathbb{R}$,
    \begin{align*}
    f(x_1;\lambda)\times\cdots\times f(x_n;\lambda) & =  \frac{\exp(-n\lambda)\lambda^{x_1+\cdots+x_n}}{\prod_{i=1}^n x_i !} \\
    &= \frac{\exp(-n\lambda)\lambda^{n \bar{x}_n}}{\prod_{i=1}^n x_i !} = g(\bar{x}_n; \lambda)h(x_1,\ldots,x_n).
    \end{align*}
    Selon le critère de Fisher--Neyman, $\bar{X}_n$ est une statistique exhaustive. De plus, cette statistique est minimale, puisque pour tout $x_1,\ldots,x_n,y_1,\ldots,y_n \in\mathbb{R}$, on a que le ratio
    \begin{align*}
    \frac{f(x_1;\lambda)\times\cdots\times f(x_n;\lambda)}{f(y_1;\lambda)\times\cdots\times f(y_n;\lambda)} 
    &= \frac{\exp(-n\lambda)\lambda^{n \bar{x}_n}}{\prod_{i=1}^n x_i !} \frac{\prod_{i=1}^n y_i !}{\exp(-n\lambda)\lambda^{n \bar{y}_n}} \\
&= \lambda^{n (\bar{x}_n- \bar{y}_n)} \frac{\prod_{i=1}^n y_i !}{\prod_{i=1}^n x_i !} 
    \end{align*}
    ne dépend pas de $\lambda$ si et seulement si $\bar{x}_n=\bar{y}_n$.
    
    De façon alternative, on pourrait aussi démontrer que l'estimateur est sans biais à variance minimale en montrant qu'il est sans biais, puis en montrant que sa variance est égale à la borne inférieure de
    de Rao--Cramér. Or, d'une part, on a
    \begin{equation*}
      \var{\bar{X}} = \frac{\var{X}}{n} = \frac{\lambda}{n}.
    \end{equation*}
    D'autre part,
    \begin{align*}
      \frac{\partial}{\partial \lambda} \ln f(x; \lambda)
      &= \frac{\partial}{\partial \lambda}
      (x \ln(\lambda) - \lambda - \ln(x!)) \\
      &= \frac{x}{\lambda} - 1 \\
      &= \frac{x - \lambda}{\lambda}
    \end{align*}
    et donc
    \begin{align*}
      \Esp{\left( \frac{\partial}{\partial \lambda}
          \ln f(X; \lambda) \right)^2}
      &= \frac{1}{\lambda^2}\Esp{(X - \lambda)^2} \\
      &= \frac{\Var{X}}{\lambda^2} \\
      &= \frac{1}{\lambda}.
    \end{align*}
    Ainsi, la borne de Rao--Cramér est $\lambda/n$. On a donc démontré
    que $\bar{X}$ est un estimateur sans biais à variance minimale du
    paramètre $\lambda$ de la loi de Poisson.
  \end{sol}
\end{exercice}

\begin{exercice}
  Démontrer que la proportion de succès $X/n$ est un estimateur sans
  biais à variance minimale de la probabilité de succès $\theta$ d'une
  distribution Binomiale. (\emph{Astuce}: considérer $X/n$ comme la
  moyenne d'un échantillon aléatoire d'une distribution de Bernoulli.)
  \begin{sol}
    Si $X$ a une distribution binomiale de paramètres $n \in \N$ et $0
    \leq \theta \leq 1$, alors on peut représenter la variable
    aléatoire sous la forme $X = Y_1 + \dots + Y_n$, où $Y_i \sim
    \text{Bernoulli}(\theta)$, $i = 1, \dots, n$. Ainsi, $X/n =
    \bar{Y}$. Dès lors, on sait $\bar{Y}$ est un estimateur sans biais
    de $\esp{Y} = \theta$. Pour montrer qu'il est MVUE, soit on montre que $\bar{Y}$
    est une statistique exhaustive minimale, ce qui est le cas selon le critère de Lehmann--Scheffé, soit on montre que la variance atteint la borne minimale de Cramer--Rao.
    
    En effet, on a que $\var{\bar{Y}} = \var{Y}/n = \theta(1
    - \theta)/n$. De plus, si $f(y; \theta) = \theta^y (1 - \theta)^{1
      - y}$, $y = 0, 1$, est la densité d'une Bernoulli, alors
    \begin{align*}
      \Esp{\left( \frac{\partial}{\partial \theta}
          \ln f(Y; \theta) \right)^2}
      &= \Esp{\left( \frac{Y - \theta}{\theta (1 - \theta)}
        \right)^2} \\
      &= \frac{\var{Y}}{[\theta (1 - \theta)]^2} \\
      &= \frac{1}{\theta (1 - \theta)}
    \end{align*}
    et donc la borne de Rao--Cramér est $\theta (1 - \theta)/n =
    \var{\bar{Y}}$. Par conséquent, $\bar{Y}$ est un estimateur sans
    biais à variance minimale du paramètre $\theta$ de la Bernoulli
    ou, de manière équivalente, $X/n$ est un estimateur sans biais à
    variance minimale du paramètre $\theta$ de la binomiale.
  \end{sol}
\end{exercice}

\begin{exercice}
  Supposons que $\bar{X}_1$ est la moyenne d'un échantillon aléatoire
  de taille $n$ d'une population normale avec moyenne $\mu$ et
  variance $\sigma_1^2$, que $\bar{X}_2$ est la moyenne d'un
  échantillon aléatoire de taille $n$ d'une population normale avec
  moyenne $\mu$ et variance $\sigma_2^2$ et que les deux échantillons
  aléatoires sont indépendants.
  \begin{enumerate}
  \item Démontrer que $\omega \bar{X}_1 + (1 - \omega) \bar{X}_2$, $0
    \leq \omega \leq 1$, est un estimateur sans biais de $\mu$.
  \item Démontrer que la variance de $\omega \bar{X}_1 + (1 - \omega)
    \bar{X}_2$ est minimale lorsque
    \begin{displaymath}
      \omega = \frac{\sigma_2^2}{\sigma_1^2 + \sigma_2^2}.
    \end{displaymath}
  \item Calculer l'efficacité relative de l'estimateur en a) avec
    $\omega = \frac{1}{2}$ à celle de l'estimateur à variance
    minimale trouvé en b).
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
      \addtocounter{enumi}{2}
    \item $(\sigma_1^2 + \sigma_2^2)^2/(4 \sigma_1^2 \sigma_2^2)$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a
      \begin{align*}
        \esp{\omega \bar{X}_1 + (1 - \omega)\bar{X}_2} &=
        \omega\Esp{\bar{X}_1} + (1 - \omega)\Esp{\bar{X}_2}\\
        &= \omega\mu +(1 - \omega)\mu\\
        &= \mu.
      \end{align*}
    \item En premier lieu,
      \begin{align*}
        \var{\omega\bar{X}_1 - (1 - \omega)\bar{X}_2} &=
        \omega^2 \var{\bar{X}_1} + (1 - \omega)^2 \var{\bar{X}_2}\\
        &= \frac{\omega^2 \sigma_1^2}{n} +
        \frac{(1 - \omega)^2 \sigma_2^2}{n}
      \end{align*}
      Or, en résolvant l'équation
      \begin{equation*}
        \frac{d}{d\omega}\, \var{\omega \bar{X}_1 - (1 - \omega)\bar{X}_2} =
        \frac{2 \omega \sigma_1^2}{n} -
        \frac{2 (1 - \omega) \sigma_2^2}{n} = 0,
      \end{equation*}
      on trouve que $\omega = \sigma_2^2/(\sigma_1^2 + \sigma_2^2)$.
      La vérification des conditions de deuxième ordre (laissée en
      exercice) démontre qu'il s'agit bien d'un minimum.
    \item Après quelques transformations algébriques, la variance de
      l'estimateur à son point minimum est
      \begin{displaymath}
        \frac{\sigma_1^2 \sigma_2^2}{n(\sigma_1^2 + \sigma_2^2)}.
      \end{displaymath}
      Lorsque $\omega = 1/2$, la variance de l'estimateur est
      \begin{equation*}
        \Var{\frac{\bar{X}_1 - \bar{X}_2}{2}} =
        \frac{\sigma_1^2 + \sigma_2^2}{4n}.
      \end{equation*}
      L'efficacité relative est donc
      \begin{align*}
        \frac{(\sigma_1^2 + \sigma_2^2)^2}{4 \sigma_1^2 \sigma_2^2}
      \end{align*}
      (ou l'inverse).
    \end{enumerate}
  \end{sol}
\end{exercice}


%\begin{exercice}
%On vous donne le code \textsf{R} suivant:
%<<fig.show="hide">>=
%data <- rbinom(1000, size=1, prob=0.5)
%plot(cumsum(data)/(1:1000), ylim=c(0,1), xlab="n",
%     ylab="sample mean", type="l")
%abline(h=0.5, lty=2)
%data <- rbinom(1000, size=1, prob=0.5)
%points(1:1000, cumsum(data)/(1:1000), col=2, type="l")
%@
%\begin{enumerate}
%
%\item Exécuter le code \textsf{R} ci-dessus. Exécuter les deux dernières lignes plusieurs fois (au moins 8), avec un argument \texttt{col} différent de la fonction \texttt{points} pour faire un graphique plus propre. Qu'est-ce que le graphique représente? Expliquer en termes mathématiques ce que ce code effectue. Quelle propriété des estimateurs est illustrée ici?
%
%\item Refaire a) en changeant la probabilité de succès de la loi Bernoulli pour 0,01 (modifier l'axe des $y$ en conséquence). Qu'est-ce qu'on peut observer?
%
%\item Soit $Y_1,\ldots,Y_n$ un échantillon aléatoire d'une loi exponentielle avec paramètre $\beta>0$. En classe, on a démontré que $\tilde\beta_n=n\min(X_1,\ldots,X_n)$ est un estimateur sans biais pour $\beta$. Adapter le code ci-dessus pour comparer les performances de la moyenne échantionnale $\bar X_n$ et de $\tilde\beta_n$ comme estimateurs pour $\beta$. Est-ce que ces résultats sont surprenants? Fournir un ou plusieurs graphiques qui supportent les résultats trouvés. [\emph{Astuce: essayer la fonction } \texttt{cummin}.]
%
%\end{enumerate}
%\begin{sol}
%\begin{enumerate}
%\item Le graphique obtenu est présenté ci-dessous
%
%<<fig.width=6,fig.height=5>>=
%data <- rbinom(1000, size=1, prob=0.5)
%plot(cumsum(data)/(1:1000), ylim=c(0,1), xlab="n", 
%     ylab="moyenne \u{E9}chantillonnale", type="l")
%abline(h=0.5, lty=2)
%for (j in 2:9)
%{
%data <- rbinom(1000, size=1, prob=0.5)
%points(1:1000, cumsum(data)/(1:1000), col=j, type="l")
%}
%@
%
%Soit $X_1,\ldots,X_n$ un échantillon aléatoire tiré d'une loi Bernoulli avec probabilité de succès $p=0,5$. Chaque ligne du graphique correspond à un échantillon de Bernoulli, et montre la moyenne échantillonnale comme une fonction de la taille de l'échantillon $n$. À mesure que la taille de l'échantillon augmente, la moyenne échantillonnale se rapproche de la valeur réelle de la moyenne de la population, 0,5. Le graphique illustre la convergence de la moyenne échantillonnale $\bar X_n$ comme estimateur du paramètre $p$ dans une distribution Bernoulli.
%
%\item Si on change pour $p=0,01$, on obtient le graphique suivant
%
%<<fig.width=6,fig.height=5>>=
%data <- rbinom(1000, size=1, prob=0.01)
%plot(cumsum(data)/(1:1000), ylim=c(0,0.1), xlab="n", 
%     ylab="moyenne \u{E9}chantillonnale", type="l")
%abline(h=0.01, lty=2)
%for (j in 2:9)
%{
%data <- rbinom(1000, size=1, prob=0.01)
%points(1:1000, cumsum(data)/(1:1000), col=j, type="l")
%}
%@
%La courbe a des sauts lorsque des succès sont observés. L'estimateur est toujours convergent.
%
%\item Soit $Y_1,\ldots,Y_n$ un échantillon aléatoire d'une loi exponentielle avec paramètre $\beta>0$. En classe, on a démontré que $\tilde\beta_n=n\min(X_1,\ldots,X_n)$ est un estimateur sans biais pour $\beta$. 
%<<fig.width=6,fig.height=5>>=
%data <- rexp(1000,rate=1)
%plot(cumsum(data)/(1:1000), ylim=c(0,3), xlab="n",
%     ylab="estimation", type="l")
%points(1:1000,cummin(data)*(1:1000), type="l", lty=2)
%abline(h=1,lty=2)
%for (j in 2:3)
%{
%data <- rexp(1000, rate=1)
%points(1:1000, cumsum(data)/(1:1000), col=j, type="l")
%points(1:1000, cummin(data)*(1:1000), col=j, type="l", lty=2)
%}
%legend("topright", c("beta chapeau","beta tilde"),
%       lwd=rep(1,2), lty=1:2)
%@
%Le graphique ci-dessus compare la performance de la moyenne échantionnale avec $\tilde\beta_n$ comme estimateurs pour $\beta$. Il est évident que la moyenne échantillonnale est meilleure. Les courbes pour $\tilde\beta$, affichées en lignes pointillées, ne convergent pas vers la valeur réelle de $\beta$ lorsque $n$ augmente. Ce résultat n'est pas surprenant, puisqu'on a vu en classe que $\tilde\beta$ n'est pas un estimateur convergent pour $\beta$. Les courbes de moyenne échantillonnale se comportent comme prévu, devenant rapidement près de la valeur théorique de 1. Voici un autre graphique qui montre plusieurs exemples de courbes pour $\tilde\beta$.
%<<fig.width=6,fig.height=5>>=
%data <- rexp(1000, rate=1)
%plot(1:1000, cummin(data)*(1:1000), ylim=c(0,5), xlab="n",
%     ylab="estimation", type="l")
%abline(h=1, lty=2)
%for (j in 2:8)
%{
%data <- rexp(1000, rate=1)
%points(1:1000, cummin(data)*(1:1000), col=j, type="l")
%}
%@
%\end{enumerate}
%\end{sol}
%\end{exercice}


\Closesolutionfile{solutions}
\Closesolutionfile{reponses}

%\section*{Exercices proposés dans \cite{Wackerly:mathstat:7e:2008}}
%
%\begin{trivlist}
%\item 6.72, 6.73, 6.74, 6.81, 6.86, 6.76, 6.89, 7.9, 7.10, 7.15, 7.18,
%  7.20, 7.27, 7.36, 7.37, 7.38, 7.73, 7.74, 7.75, 7.76, 7.77, 7.78,
%  7.81, 7.88, 7.89, 7.90, 7.92, 7.96, 7.97
%\end{trivlist}


%%%
%%% Insérer les réponses
%%%
\input{reponses-estimation}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_analyse_statistique"
%%% coding: utf-8-unix
%%% End:

%%%% BORNE DE CRAMER-RAO

%\begin{exercice}
%  Soit $X_1, \dots, X_n$ un échantillon aléatoire de taille $n >
%  2$ de la densité
%  \begin{displaymath}
%    f(x; \theta) = \theta x^{\theta - 1}, \quad 0 < x < 1.
%  \end{displaymath}
%  \begin{enumerate}
%  \item Vérifier que la borne de Rao--Cramér pour un estimateur de
%    $\theta$ est $\theta^2/n$.
%  \item Trouver la distribution de la variable aléatoire $Y_i = -\ln
%    X_i$, puis celle de $Z = - \sum_{i=1}^n \ln X_i$. Vérifier alors
%    que $\esp{Z} = n/\theta$.
%  \item Le résultat précédent suggère d'utiliser $1/Z$ comme
%    estimateur de $\theta$. Développer un estimateur sans biais de
%    $\theta$ basé sur $1/Z$.
%  \end{enumerate}
%  \begin{rep}
%    \begin{inparaenum}
%      \stepcounter{enumi}
%    \item $Y_i \sim \text{Exponentielle}(\theta)$
%    \item $(n - 1)/Z$
%    \end{inparaenum}
%  \end{rep}
%  \begin{sol}
%    \begin{enumerate}
%    \item On a
%      \begin{align*}
%        \ln(f(x;\theta)) &= \ln(\theta) +(\theta-1)\ln(x) \\
%        \frac{\partial}{\partial \theta} \ln(f(x;\theta)) &=
%        \frac{1}{\theta} + \ln(x) \\
%        \frac{\partial^2}{\partial \theta^2} f(x;\theta) &=
%        -\frac{1}{\theta^2} \\
%        \intertext{et donc}
%        -n \Esp{\frac{\partial^2}{\partial \theta^2} \ln f(X; \theta)}
%        &= \frac{n}{\theta^2}.
%      \end{align*}
%      Par conséquent, la borne de Rao--Cramér est $\theta^2/n$.
%    \item Soit $Y = - \ln X$. On a
%      \begin{align*}
%        F_Y(y) &= \prob{-\ln(X) \leq y}\\
%        &= \prob{X > e^{-y}} \\
%        &= \int_{e^{-y}}^1 \theta x^{\theta - 1}\, dx \\
%        &= 1 - e^{- \theta y},
%      \end{align*}
%      d'où $Y \sim \text{Exponentielle}(\theta)$. Par conséquent, $Z =
%      -\sum_{i=1}^n \ln(X_i) = \sum_{i=1}^n Y_i$ obéit à une loi
%      Gamma$(n, \theta)$ et donc, directement, $\esp{Z} =
%      n/\theta$.
%    \item Étant donné le résultat en b), on a
%      \begin{align*}
%        \Esp{\frac{1}{Z}}
%        &= \frac{\theta^n}{\Gamma(n)} \int_0^\infty
%        \left( \frac{1}{z} \right) z^{n - 1} e^{-\theta z}\, dz \\
%        &= \frac{\theta^n}{\Gamma(n)}
%        \frac{\Gamma(n - 1)}{\theta^{n-1}} \\
%        &= \frac{\theta}{n - 1}.
%      \end{align*}
%      Par conséquent, $(n - 1)/Z$ constitue un estimateur sans biais
%      du paramètre $\theta$ de la densité en a).
%    \end{enumerate}
%  \end{sol}
%\end{exercice}
%
%\begin{exercice}
%  L'inégalité de Rao--Cramér fournit un seuil minimal pour la
%  variance d'un estimateur du paramètre $\theta$ d'une distribution
%  $f(x; \theta)$. Qu'en est-il si l'on souhaite estimer non pas le
%  paramètre $\theta$, mais plutôt une fonction $g$ de celui-ci? (On
%  peut penser, ici, à la moyenne d'une loi Exponentielle.) L'inégalité
%  de Rao--Cramér se généralise ainsi: soit $\hat{\theta} = T(X_1,
%  \dots, X_n)$ un estimateur de $g(\theta)$; alors
%  \begin{displaymath}
%    \var{\hat{\theta}} \geq \frac{(g^\prime(\theta))^2}
%    {n\, \Esp{\D \left( \frac{\partial}{\partial \theta} \ln f(X;
%          \theta) \right)^2 } }.
%  \end{displaymath}
%  Soit $X_1, \dots, X_n$ un échantillon aléatoire issu d'une loi de
%  Poisson de paramètre $\lambda$.
%  \begin{enumerate}
%  \item Calculer la borne de Rao--Cramér pour un estimateur de
%    $\lambda$.
%  \item Considérer $g(\lambda) = e^{-\lambda} = \Prob{X = 0}$. Calculer
%    la borne de Rao--Cramér pour un estimateur de $e^{-\lambda}$.
%  \item Soit la statistique
%    \begin{displaymath}
%      T = \frac{1}{n} \sum_{i=1}^n I_{\{X_i = 0\}},
%    \end{displaymath}
%    où $I_{\mathcal{A}}$ est une fonction indicatrice valant $1$ si
%    $\mathcal{A}$ est vraie et $0$ sinon. La statistique $T$
%    représente donc la proportion d'observations nulles dans
%    l'échantillon. Démontrer que $T$ est un estimateur sans biais de
%    $e^{-\lambda}$ et que
%    \begin{displaymath}
%      \var{T} = \frac{e^{-\lambda} (1 - e^{-\lambda})}{n}.
%    \end{displaymath}
%  \item Calculer l'efficacité de la statistique $T$ définie en c).
%  \end{enumerate}
%  \begin{rep}
%    \begin{inparaenum}
%    \item $\lambda/n$
%    \item $\lambda e^{-2\lambda}/n$
%      \stepcounter{enumi}
%    \item $\lambda e^{-\lambda}/(1 - e^{-\lambda})$
%    \end{inparaenum}
%  \end{rep}
%  \begin{sol}
%    \begin{enumerate}
%    \item On a
%      \begin{align*}
%        \ln(f(x; \lambda)) &= k\ln(\lambda) - \lambda - \ln(k!)\\
%        \frac{\partial}{\partial \lambda} \ln(f(x;\lambda)) &=
%        \frac{k}{\lambda} - 1\\
%        \frac{\partial^2}{\partial \lambda^2} \ln(f(x; \lambda)) &=
%        - \frac{k}{\lambda^2}\\
%        \intertext{et donc}
%        -n \Esp{\frac{\partial^2}{\partial \lambda^2} \ln f(X; \lambda)}
%        &= \frac{n}{\lambda}.
%      \end{align*}
%      La borne de Rao--Cramér est donc $\lambda/n$.
%    \item Le dénominateur de la borne de Rao--Cramér reste le même
%      qu'en a), il suffit de calculer le numérateur. On a $g(\lambda)
%      = e^{-\lambda}$ et donc $g^\prime(\lambda) = - e^{-\lambda}$,
%      d'où la borne de Rao-Cramér pour un estimateur de $g(\lambda)$
%      est $\lambda e^{-2\lambda}/n$.
%    \item La variable aléatoire $I_{\{X = 0\}}$, où $X \sim
%      \text{Poisson}(\lambda)$, obéit à une loi de Bernoulli avec
%      probabilité de succès $\theta = \prob{X = 0} = e^{-\lambda}$.
%      Ainsi, $n T = \sum_{i = 1}^n I_{\{X_i = 0\}} \sim
%      \text{Binomiale}(n, e^{-\lambda})$. De là, il est immédiat que
%      \begin{align*}
%        \esp{T} &= \frac{\esp{nT}}{n} \\
%        &= \frac{n e^{-\lambda}}{n} \\
%        &= e^{-\lambda} \\
%        \intertext{et que}
%        \var{T} &= \frac{\var{nT}}{n^2} \\
%        &= \frac{n e^{-\lambda} (1 - e^{-\lambda})}{n^2} \\
%        &= \frac{e^{-\lambda} (1 - e^{-\lambda})}{n}.
%      \end{align*}
%    \item L'efficacité d'un estimateur est le rapport entre la
%      variance de l'estimateur et la borne de Rao--Cramér. On obtient
%      alors
%      \begin{equation*}
%        \frac{\lambda e^{-2\lambda}/n}{%
%          e^{-\lambda} (1 - e^{-\lambda})/n} =
%        \frac{\lambda e^{-\lambda}}{1 - e^{-\lambda}}.
%      \end{equation*}
%
%    \end{enumerate}
%  \end{sol}
%\end{exercice}


\chapter{Ajustement de modèles}
\label{chap:ajustement}

\Opensolutionfile{reponses}[reponses-ajustement]
\Opensolutionfile{solutions}[solutions-ajustement]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:ajustement}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:ajustement}}

\end{Filesave}




%%%
%%% Début des exercices
%%%

% Méthode des moments

\begin{exercice}
  \label{ex:ponctuelle:emm}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire issu des
  distributions ci-dessous. Dans chaque cas, trouver l'estimateur du
  paramètre $\theta$ à l'aide de la méthode des moments.
  \begin{enumerate}
  \item $f(x; \theta) = \theta^x e^{-\theta}/x!$, $x = 0, 1, \dots$,
    $\theta > 0$.
  \item $f(x; \theta) = \theta x^{\theta - 1}$, $0 < x < 1$, $\theta >
    0$.
  \item $f(x; \theta) = \theta^{-1} e^{-x/\theta}$, $\theta > 0$.
  \item $f(x; \theta) = e^{-|x - \theta|}/2$, $-\infty < x <
    \infty$, $-\infty < \theta < \infty$.
  \item $f(x; \theta) = e^{-(x - \theta)}$, $x \geq \theta$, $\theta >
    0$.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\bar{X}$
    \item $\bar{X}/(1 - \bar{X})$
    \item $\bar{X}$
    \item $\bar{X}$
    \item $\bar{X} - 1$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a une une distribution de Poisson de paramètre $\theta$,
      d'où $\esp{X} = \theta$. L'estimateur des moments est donc
      $\hat{\theta} = \bar{X}$.
    \item La densité est celle d'une distribution bêta de paramètres
      $\theta$ et $1$. Ainsi, $\esp{X} = \theta/(\theta + 1)$ et en posant
      \begin{equation*}
        \frac{\theta}{\theta + 1} = \bar{X}
      \end{equation*}
      on trouve que l'estimateur des moments de $\theta$ est
      \begin{equation*}
        \hat{\theta} = \frac{\bar{X}}{1 - \bar{X}}.
      \end{equation*}
    \item On reconnaît la densité d'une distribution Gamma de
      paramètres $1$ et $\theta$. Ainsi, on sait que $\esp{X} =
      \theta$, d'où l'estimateur des moments est $\hat{\theta} =
      \bar{X}$.
    \item Cette densité est celle de la loi de Laplace. On a
      \begin{align*}
        \esp{X} &= \frac{1}{2}
        \left(
          \int_{-\infty}^\theta x e^{x-\theta}\, dx +
          \int_{\theta}^\infty xe^{-x + \theta}\, dx
        \right) \\
        &= \frac{1}{2}(2\theta)\\
        &= \theta.
      \end{align*}
      L'estimateur des moments de $\theta$ est donc $\hat{\theta} =
      \bar{X}$.
    \item On a la densité d'une exponentielle de paramètre 1
      translatée de $\theta$ vers la droite. Par conséquent,
      $\esp{X} = \theta + 1$, un résultat facile à vérifier en
      intégrant. En posant $\theta + 1 = \bar{X}$, on trouve
      facilement que $\hat{\theta} = \bar{X} - 1$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
\label{ex:moment:distln}
Si $Z\sim \mathcal{N}(\mu,\sigma^2)$ et $Y=e^{Z}$, alors $Y$ a une distribution log-normale avec paramètres $\mu$ et $\sigma^2$. La fonction de répartition est
$$
F(y)=\Phi\left(\frac{\ln(y)-\mu}{\sigma}\right), \quad y>0,
$$
où $\Phi$ est la fonction de répartition de la loi normale standard. 
\begin{enumerate}
\item Utiliser la fonction génératrice des moments de la loi normale pour montrer que les deux premiers moments de la loi log-normale sont
$$
\ex[Y]=e^{\mu+\sigma^2/2}\quad \mbox{ et } \ex[Y^2]=e^{2\mu+2\sigma^2}.
$$
\item Utiliser la méthode des moments pour construire un estimateur de $\mu$ et $\sigma^2$ si $Y_1, \ldots, Y_n$ forment un échantillon aléatoire d'une loi log-normale.

\item Montrer que les estimateurs trouvés en (b) sont convergents.
\end{enumerate}
\begin{rep}
b) $\hat \mu = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\}$ et $\hat\sigma^2 = \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\}$
\end{rep}
\begin{sol}
\begin{enumerate}
\item La fonction génératrice des moments de $Z\sim\mathcal{N}(\mu,\sigma^2)$ est
$$
M_{Z}(t)=\ex[e^{tZ}]=\exp(\mu t +\sigma^2 t^2/2).
$$
Ainsi,
\begin{align*}
\ex[Y]&= \ex[e^{Z}]=M_Z(1)=\exp(\mu +\sigma^2/2),\\
\ex[Y^2]&= \ex[e^{2Z}]=M_Z(2)=\exp(2\mu +\sigma^2 2^2/2).
\end{align*}

\item On doit résoudre les deux équations où $m_1=\bar Y_n$ est égal à $\ex[Y]$ et $m_2=\frac{1}{n}\sum_{i=1}^n Y_i^2$ est égal à $\ex[Y^2]$:
\begin{align}
\exp(\hat\mu +\hat\sigma^2/2)&=\bar Y_n \label{eq:ajust1}\\
\exp(2\hat\mu +2\hat\sigma^2 )&=\frac{1}{n}\sum_{i=1}^n Y_i^2.\label{eq:ajust2}
\end{align}
On note que
 \begin{align*}
 \exp(2\hat\mu +2\hat\sigma^2 )&=\left\{\exp(\hat\mu +\hat\sigma^2/2)\right\}^2\exp(\hat\sigma^2)\\
 &=\bar Y_n^2 \exp(\hat\sigma^2),\quad \mbox{ avec \eqref{eq:1}}.
 \end{align*}
 En remplaçant dans \eqref{eq:ajust2}, on obtient que
 $$
 \bar{Y}_n^2 \exp(\hat\sigma^2) = \frac{1}{n}\sum_{i=1}^n Y_i^2 
 $$
 est équivalent à 
 $$
 \hat\sigma^2 = \ln\left\{\frac{\sum_{i=1}^n Y_i^2/n}{\bar{Y}_n^2} \right\}.
 $$
 En utilisant \eqref{eq:ajust1}, $\hat\mu +\hat\sigma^2/2=\ln(\bar Y_n )$, donc
\begin{align*}
\hat\mu & = \ln(\bar Y_n )-\frac{1}{2}\ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\}\\
& = \ln(\bar Y_n )+\ln\left\{\frac{\bar Y_n}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\}\\
& = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\}.
\end{align*}
Ainsi, les estimateurs des moments sont
$$
\hat \mu = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\} \quad \mbox{ et }\quad \hat\sigma^2 = \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\}.
$$

\item Pour montrer la convergence, on note premièrement que, selon la Loi faible des grands nombres
\begin{align*}
\bar Y &\stackrel{P}{\rightarrow} \ex[Y]=e^{\mu+\sigma^2/2}\\
\frac{1}{n}\sum_{i=1}^n Y_i^2 &\stackrel{P}{\rightarrow} \ex[Y^2]=e^{2\mu+2\sigma^2}\\
\end{align*}
Puisque $\ln$, le carré et la racine carrée sont toutes des fonctions continues, on trouve que, quand $n\to\infty$,
\begin{align*}
\hat\mu = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\} &\stackrel{P}{\rightarrow} \ln\left\{\frac{ e^{2\mu+\sigma^2}}{\sqrt{e^{2\mu+2\sigma^2}}} \right\}=\ln\left\{ e^{2\mu+\sigma^2-\mu-\sigma^2} \right\}=\mu\\
\hat\sigma^2 = \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\} &\stackrel{P}{\rightarrow} \ln\left\{\frac{e^{2\mu+2\sigma^2}}{e^{2\mu+\sigma^2}} \right\}=\ln\left\{e^{2\mu+2\sigma^2-2\mu-\sigma^2} \right\}=\sigma^2.
\end{align*}
Ainsi, les estimateurs sont convergents.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Considérer la distribution géométrique avec fonction de masse de
  probabilité
  \begin{displaymath}
    \prob{X = x} = \theta (1 - \theta)^x, \quad x = 0, 1, \dots.
  \end{displaymath}
  On a obtenu l'échantillon aléatoire suivant de cette distribution:
  \begin{center}
    5\; 7\; 4\; 11\; 0\; 9\; 1\; 1\; 3\; 2\; 1\; 0\; 6\; 0\; 1\; 1\;
    1\; 9\; 2\; 0.
  \end{center}
  Utiliser la méthode des moments pour obtenir une estimation
  ponctuelle du paramètre $\theta$.
  \begin{rep}
    $0,2381$.
  \end{rep}
  \begin{sol}
    Pour obtenir l'estimateur des moments de $\theta$, on pose
    \begin{equation*}
      \esp{X} = \frac{1 - \theta}{\theta} = \bar{X},
    \end{equation*}
    d'où
    \begin{equation*}
      \hat{\theta} = \frac{1}{\bar{X} + 1}.
    \end{equation*}
    La moyenne de l'échantillon est $\bar{x} = 64/20 = 3,2$. On a donc
    \begin{equation*}
      \hat{\theta} = \frac{1}{4,2} = 0,2381.
    \end{equation*}
  \end{sol}
\end{exercice}

\begin{exercice}
  On a un dé régulier avec $\theta$ faces numérotées de $1$ à
  $\theta$, où $\theta$ est un entier inconnu. Soit $X_1, \dots,
  X_n$ un échantillon aléatoire composé de $n$ lancers de dés indépendants.
  \begin{enumerate}
  \item Déterminer l'estimateur des moments de $\theta$.
  \item Calculer l'estimateur des moments de $\theta$ si $n = 4$ et
    $x_1 = x_2 = x_3 = 3$ et $x_4 = 12$. Interpréter le résultat.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\hat{\theta} = 2 \bar{X} - 1$
    \item $9,5$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Il s'agit ici de trouver l'estimateur des moments du
      paramètre $\theta$ d'une distribution uniforme discrète sur $1,
      2, \dots, \theta$, c'est-à-dire que $\prob{X = x} = 1/\theta$
      pour $x = 1, \dots, \theta$. En posant
      \begin{align*}
        \esp{X} = \frac{\theta + 1}{2} = \bar{X},
      \end{align*}
      on trouve facilement que l'estimateur des moments de $\theta$
      est $\hat{\theta} = 2 \bar{X} - 1$.
    \item Avec $x_1 = x_2 = x_3 = 3$ et $x_4 = 12$, on a $\hat{\theta}
      = 2 (3 + 3 + 3 + 12)/4 - 1 = 9,5$. Or, cet estimateur est
      absurde puisque, en ayant roulé le résultat 12, on sait qu'il y a au
      moins douze faces sur le dé! En d'autres termes, $9,5$ est
      une valeur de $\theta$ impossible. On constate que l'estimateur
      obtenu à l'aide de la méthode des moments n'est pas toujours un
      estimateur possible.
    \end{enumerate}
  \end{sol}
\end{exercice}


% Méthode des quantiles

\begin{exercice}
Soit $X_1, \dots, X_n$ un échantillon aléatoire tiré d'une distribution log-normale. Les 20e et 80e quantiles empiriques sont respectivement de $\hat{\pi}_{0,20} = 18,25$ et $\hat{\pi}_{0,80} = 35,8$. Estimer les paramètres de la distribution en utilisant la méthode des quantiles, puis utiliser ces estimations pour estimer la probabilité d'observer une valeur excédant 30.

\begin{rep}
$\hat{\mu}=3,241$, $\hat{\sigma}=0,4$ et $\widehat{\Pr(X>30)}= 0,3446$
\end{rep}

\begin{sol}
Les équations à résoudre sont
\begin{align*}
0,2 &= F(18,25) = \Phi\left(\frac{\ln(18,25)-\hat{\mu}}{\hat{\sigma}}\right), \\
0,8 &= F(35,8) = \Phi\left(\frac{\ln(35,8)-\hat{\mu}}{\hat{\sigma}}\right).
\end{align*}
Les 20e et 80e quantiles de la loi normale standard sont -0,842 et 0,842, respectivement. Les équations deviennent
\begin{align*}
-0,842 &= \frac{2,904-\hat{\mu}}{\hat{\sigma}}, \\
0,842 &= \frac{3,578-\hat{\mu}}{\hat{\sigma}}.
\end{align*}
Diviser la première équation par la deuxième donne
$$
-1 = \frac{2,904-\hat{\mu}}{3,578-\hat{\mu}}.
$$
La solution est $\hat{\mu}=3,241$ et $\hat{\sigma}=0,4$. La probabilité d'excéder 30 est estimée par
\begin{align*}
\widehat{\Pr(X>30)} &= 1-F(30; \hat\mu,\hat\sigma) = 1-\Phi\left(\frac{\ln(30)-3,241}{0,4}\right) = 1-\Phi(0,4) \\
&= 1-0,6554 = 0,3446.
\end{align*}
\end{sol}
\end{exercice}

\begin{exercice}
Un échantillon aléatoire de réclamations est tiré d'une distribution log-logistique avec fonction de répartition
$$
F(x)=\frac{(x/\theta)^\tau}{1+(x/\theta)^\tau}, \quad x>0, \, \theta >0, \, \tau >0.
$$
Dans l'échantillon, 80\% des réclamations excèdent 100 et 20\% des réclamations excèdent 400. Estimer les paramètres $\theta$ et $\tau$ par la méthode des quantiles.

\begin{rep}
$\hat{\tau}=2$ et $\hat{\theta}=200$
\end{rep}

\begin{sol}
Les équations à résoudre sont
\begin{align*}
0,2 &= F(100) = \frac{(100/\hat{\theta})^{\hat{\tau}}}{1+(100/\hat{\theta})^{\hat{\tau}}}, \\
0,8 &= F(400) = \frac{(400/\hat{\theta})^{\hat{\tau}}}{1+(400/\hat{\theta})^{\hat{\tau}}}.
\end{align*}
Avec la première équation, on obtient
$$
0,2=0,8(100/\hat{\theta})^{\hat{\tau}} \text{ ou encore } \hat{\theta}^{\hat{\tau}}=4(100)^{\hat{\tau}}. 
$$
En insérant le résultat dans la deuxième équation, on obtient
$$
0,8=\frac{4^{\hat{\tau}-1}}{1+4^{\hat{\tau}-1}}.
$$
On résoud pour obtenir $\hat{\tau}=2$ et $\hat{\theta}=200$.
\end{sol}
\end{exercice}

\begin{exercice}
Les 20 pertes suivantes (en millions de dollars) ont été enregistrées sur une période d'un an:
\begin{center}
\begin{tabular}{cccccccccc}
1 & 1 & 1 & 1 & 1 & 2 & 2 & 3 & 3 & 4 \\
6 & 6 & 8 & 10 & 13 & 14 & 15 & 18 & 22 & 25
\end{tabular}
\end{center}
Déterminer le 75e quantile empirique par la méthode des quantiles empiriques lissés.

\begin{rep}
13,75
\end{rep}

\begin{sol}
On a besoin de la $0,75(21)=15,75$e plus petite observation. On obtient $0,25(13) + 0,75(14)= 13,75$.
\end{sol}
\end{exercice}

% Méthode du maximum de vraisemblance

\begin{exercice}
  \label{ex:ponctuelle:emv}
  Trouver l'estimateur du maximum de vraisemblance du paramètre
  $\theta$ de chacune des distributions de
  l'exercice~\ref{chap:ajustement}.\ref{ex:ponctuelle:emm}.
  \begin{rep}
    \begin{inparaenum}
    \item $\bar{X}$
    \item $-n/\ln(X_1 \cdots X_n)$
    \item $\bar{X}$
    \item $\text{med}(X_1, \dots, X_n)$
    \item $X_{(1)}$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Dans tous les cas, la fonction de vraisemblance est $L(\theta) =
    \prod_{i = 1}^n f(x_i; \theta)$ et la fonction de
    log-vraisemblance est $l(\theta) = \ln L(\theta) = \sum_{i = 1}^n
    \ln f(x_i; \theta)$. L'estimateur du maximum de vraisemblance du
    paramètre $\theta$ est la solution de l'équation $l^\prime(\theta)
    = 0$.
    \begin{enumerate}
    \item On a
      \begin{align*}
        L(\theta) &= \frac{e^{-n\theta} \theta^{\sum_{i=1}^n x_i}}{%
          \prod_{i=1}^n x_i!}, \\
        l(\theta) &= -n \theta  + \sum_{i = 1}^n x_i \ln(\theta)
        - \sum_{i=1}^n \ln(x_i!) \\
        \intertext{et}
        l^\prime(\theta) &= -n + \frac{\sum_{i = 1}^n x_i}{\theta}.
      \end{align*}
      En résolvant l'équation $l^\prime(\theta) = 0$ pour $\theta$, on
      trouve que l'estimateur du maximum de vraisemblance est
      $\hat{\theta} = \bar{X}$.
    \item On a
      \begin{align*}
        L(\theta) &= \theta^n
        \left(
          \prod_{i = 1}^n x_i
        \right)^{\theta - 1}, \\
        l(\theta) &= n \ln(\theta) +
        (\theta - 1) \sum_{i = 1}^n \ln(x_i) \\
        \intertext{et}
        l^\prime(\theta) &= \frac{n}{\theta} +
        \sum_{i = 1}^n \ln(x_i).
      \end{align*}
      On trouve donc que
      \begin{equation*}
      \hat{\theta} = -\frac{n}{\sum_{i=1}^n \ln(X_i)} =
      -\frac{n}{\ln(X_1 \cdots X_n)}.
      \end{equation*}
    \item  On a
      \begin{align*}
        L(\theta) &= \theta^{-n} e^{-\sum_{i=1}^n x_i/\theta}, \\
        l(\theta) &= -n \ln(\theta) - \frac{\sum_{i=1}^n x_i}{\theta} \\
        \intertext{et}
        l^\prime(\theta) &= -\frac{n}{\theta} +
        \frac{\sum_{i=1}^n x_i}{\theta^2}.
      \end{align*}
      On obtient que $\hat{\theta} = \bar{X}$.
    \item On a
      \begin{equation*}
        L(\theta) = \left( \frac{1}{2} \right)^n
        e^{-\sum_{i = 1}^n \abs{x_i - \theta}}
      \end{equation*}
      La présence de la valeur absolue rend cette fonction non
      différentiable en $\theta$. On remarque que la fonction de
      vraisemblance sera maximisée lorsque l'expression $\sum_{i=1}^n
      \abs{x_i - \theta}$ sera minimisée. On établit donc que $\hat{\theta} = \text{med}(X_1, \dots, X_n)$, puisqu'on connaît le résultat suivant sur la médiane.
      
      En général, si $X$ est une variable aléatoire continue et $a$ est une constante, on peut trouver le minimum de
    \begin{align*}
      \esp{\abs{X - a}}
      &= \int_{-\infty}^\infty \abs{x - a} f(x)\,dx \\
      &= \int_{-\infty}^a (a - x)f(x)\, dx
      + \int_a^\infty (x - a)f(x)\,dx.
    \end{align*}
    Or,
    \begin{align*}
      \frac{d}{da} \esp{\abs{X - a}}
      &= \int_{-\infty}^a f(x)\, dx + \int_a^\infty f(x)\, dx \\
      &= F_X(a) - (1 - F_X(a)) \\
      &= 2F_X(a) - 1
    \end{align*}
    Par conséquent, le minimum est atteint au point $a$ tel que
    $2F_X(a) - 1 = 0$, soit $F_X(a) = 1/2$. Par définition, cette
    valeur est la médiane de $X$.

    \item On remarque que le support de la densité dépend du paramètre
      $\theta$. La vraisemblance est
      \begin{equation*}
        L(\theta) = e^{n\theta - \sum_{i=1}^n x_i}\prod_{i=1}^n\mathbf{1}(x_i>\theta)
      \end{equation*}
      S'il y a une indicatrice qui est 0, alors la vraisemblance sera 0, elles doivent donc toutes être égales à 1 simultanément, ce qui est équivalent à
      \begin{equation*}
        L(\theta) = e^{n\theta - \sum_{i=1}^n x_i}\mathbf{1}(\min(x_1,\ldots,x_n)>\theta).
      \end{equation*}
      La fonction $e^{n\theta - \sum_{i=1}^n x_i}$ est strictement
      croissante en fonction de $\theta$, ce qui indique de choisir
      une valeur de $\theta$ la plus grande possible. Par contre, on a
      la contrainte $min(x_1,\ldots,x_n)>\theta$, c'est-à-dire que $\theta$ doit
      être plus inférieur ou égal à la plus petite valeur de
      l'échantillon. Par conséquent, $\hat{\theta} = \min(X_1, \dots,
      X_n) = X_{(1)}$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:ponctuelle:expon}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire de la distribution
  exponentielle translatée avec fonction de répartition
  \begin{align*}
    F(x; \mu, \lambda)
    &= 1 - e^{-\lambda (x - \mu)} \\
    \intertext{et densité}
    f(x; \mu, \lambda)
    &= \lambda e^{-\lambda (x - \mu)}, \quad x \geq \mu,
  \end{align*}
  où $-\infty < \mu < \infty$ et $\lambda > 0$.
  \begin{enumerate}
  \item Démontrer que la distribution exponentielle translatée est
    obtenue par la transformation $X = Z + \mu$, où $Z \sim
    \text{Exponentielle}(\lambda)$.
  \item Calculer l'espérance et la variance de cette distribution.
  \item Calculer les estimateurs du maximum de vraisemblance des
    paramètres $\mu$ et $\lambda$.
  \item Simuler 100 observations d'une loi Exponentielle translatée de
    paramètres $\mu = 1000$ et $\lambda = 0,001$ à l'aide de la
    fonction \texttt{rexp} de \textsf{R} et de la transformation en
    a). Calculer des estimations ponctuelles de $\mu$ et $\lambda$
    pour l'échantillon ainsi obtenu. Ces estimations sont-elles
    proches des vraies valeurs des échantillons? Répéter l'expérience
    plusieurs fois au besoin.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $\esp{X} = \mu + \lambda^{-1}$, $\var{X} = \lambda^{-2}$
    \item $\hat{\mu} = X_{(1)}$, $\hat{\lambda} = n/\sum_{i = 1}^n (X_i -
        X_{(1)})$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a $X = Z + \mu$ où $Z \sim
      \text{Exponentielle}(\lambda)$. Alors,
      \begin{align*}
        F_X(x) &= \prob{Z + \mu \leq x}\\
        &= F_Z(x - \mu) \\
        &= 1 - e^{-\lambda (x-\mu)}, \quad x > \mu \\
        \intertext{et}
        f_X(x) &= \lambda e^{-\lambda (x - \mu)}, \quad x > \mu.
      \end{align*}
    \item On a simplement
      \begin{align*}
        \Esp{X} &= \Esp{Z + \mu}\\
        &= \Esp{Z} + \mu\\
        &= \frac{1}{\lambda} + \mu\intertext{et}
        \Var{X} &= \Var{Z + \mu}\\
        &= \Var{Z}\\
        &= \frac{1}{\lambda^2}.
      \end{align*}
    \item On a
      \begin{align*}
        L(\mu,\lambda) &= \lambda^n e^{-\lambda\sum_{i=1}^n(x_i -\mu)}\prod_{i=1}^n\mathbf{1}(x_i\geq \mu)\\
        l(\mu,\lambda) &= n\ln(\lambda) -
        \lambda\sum_{i=1}^n (x_i - \mu) +\log(\mathbf{1}\{\min(x_1,\ldots,x_n)\geq \mu\}) \\
        \intertext{et}
        \frac{\partial l(\mu,\lambda)}{\partial \lambda} &=
        \frac{n}{\lambda} - \sum_{i=1}^n (x_i - \mu), \\
        \intertext{d'où}
        \lambda &= \frac{n}{\sum_{i=1}^n (x_i - \mu)}.
      \end{align*}
      On voit que la fonction $e^{-\lambda\sum_{i=1}^n(x_i -\mu)}$ est strictement
      croissante en fonction de $\mu$. Ainsi, il faut prendre la
      valeur de $\mu$ la plus grande possible telle que $\log(\mathbf{1}\{\min(x_1,\ldots,x_n)\geq \mu\})=0$. On a donc
      \begin{align*}
        \hat{\mu} &= X_{(1)} \\
        \hat{\lambda} &= \frac{n}{\sum_{i=1}^n (x_i - x_{(1)})}.
      \end{align*}
    \item On a, par exemple, les résultats suivants pour une
      exponentielle translatée de paramètres $\mu = \lambda^{-1} =
      \nombre{1000}$:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x} \hlkwb{<-} \hlkwd{rexp}\hlstd{(}\hlnum{100}\hlstd{,} \hlkwc{rate} \hlstd{=} \hlnum{0.001}\hlstd{)} \hlopt{+} \hlnum{1000}
\hlkwd{min}\hlstd{(x)}
\end{alltt}
\begin{verbatim}
## [1] 1001.475
\end{verbatim}
\begin{alltt}
\hlnum{100} \hlopt{/} \hlkwd{sum}\hlstd{(x} \hlopt{-} \hlkwd{min}\hlstd{(x))}
\end{alltt}
\begin{verbatim}
## [1] 0.001032499
\end{verbatim}
\end{kframe}
\end{knitrout}
      Les estimations obtenues sont près des vraies valeurs des
      paramètres, même pour un relativement petit échantillon de
      taille 100.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soient $X_{(1)} < \dots < X_{(n)}$ les statistiques d'ordre d'un
  échantillon aléatoire tiré d'une distribution uniforme sur
  l'intervalle $[\theta - \frac{1}{2}, \theta + \frac{1}{2}]$,
  $-\infty < \theta < \infty$. Démontrer que toute statistique $T(X_1,
  \dots, X_n)$ satisfaisant l'inégalité
  \begin{displaymath}
    X_{(n)} - \frac{1}{2} \leq T(X_1, \dots, X_n) \leq
    X_{(1)} + \frac{1}{2}
  \end{displaymath}
  est un estimateur du maximum de vraisemblance de $\theta$. Ceci est
  un exemple où l'estimateur du maximum de
  vraisemblance n'est pas unique.
  \begin{sol}
    Il est clair ici que, comme $f(x;\theta) = 1$, on ne pourra pas
    utiliser la technique habituelle pour calculer l'estimateur du
    maximum de vraisemblance. Il faut d'abord déterminer l'ensemble
    des valeurs de $\theta$ possibles selon l'échantillon obtenu.
    Comme toutes les données de l'échantillon doivent se trouver dans
    l'intervalle $[\theta - 1/2, \theta + 1/2]$, on a $\theta \geq
    X_{(n)} - 1/2$ et $\theta \leq X_{(1)} + 1/2$. De plus, puisque
    $X_{(n)} - X_{(1)} \leq 1$, on a que $X_{(n)} - 1/2 \leq \theta
    \leq X_{(1)} + 1/2$. Ainsi, toute statistique satisfaisant ces
    inégalités est un estimateur du maximum de vraisemblance de
    $\theta$. On a donc que
    \begin{displaymath}
      X_{(n)} - \frac{1}{2} \leq T(X_1, \dots, T_n) \leq X_{(1)} + \frac{1}{2}.
    \end{displaymath}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire issu de la
  distribution inverse gaussienne, dont la densité est
  \begin{displaymath}
    f(x; \mu,\lambda) =
    \left(
      \frac{\lambda}{2\pi x^3}
    \right)^{1/2}
    \exp\left\{
      -\frac{\lambda (x - \mu)^2}{2\mu^2 x}
    \right\}, \quad x > 0.
  \end{displaymath}
  Calculer les estimateurs du maximum de vraisemblance de $\mu$ et $\lambda$.
  \begin{rep}
    $\hat{\mu} = \bar{X}$, $\hat{\lambda} = n/\sum_{i = 1}^n
    (X_i^{-1} - \bar{X}^{-1})$
  \end{rep}
  \begin{sol}
    On a
    \begin{equation*}
      L(\mu ,\lambda) =
      \left(
        \frac{\lambda}{2\pi}
      \right)^{n/2}
      \left(
        \prod_{i = 1}^n \frac{1}{x_i^3}
      \right)^{1/2}
      \exp\left\{
        -\frac{\lambda}{2}
        \sum_{i=1}^n \frac{(x_i - \mu)^2}{\mu^2 x_i}
      \right\}.
    \end{equation*}
    Il est plus simple de trouver d'abord l'estimateur du maximum de
    vraisemblance du paramètre $\mu$. On constate qu'il s'agit de la
    valeur qui minimise la somme dans l'exponentielle. Or,
    \begin{equation*}
      \frac{\partial}{\partial \mu}\,
      \sum_{i=1}^n \frac{(x_i - \mu)^2}{\mu^2 x_i} =
      -\sum_{i=1}^n \frac{2}{\mu^2}
      \left( \frac{x_i}{\mu} - 1 \right).
    \end{equation*}
    En posant
    \begin{equation*}
      \sum_{i=1}^n \left( \frac{x_i}{\mu} - 1 \right) = 0,
    \end{equation*}
    on trouve que $\hat{\mu} = \bar{X}$. Pour trouver l'estimateur du
    maximum de vraisemblance de $\lambda$, on établit d'abord que
    \begin{displaymath}
      L(\hat{\mu}, \lambda) \propto \lambda^{n/2} e^{-\lambda H},
    \end{displaymath}
    où
    \begin{align*}
      H &= \sum_{i = 1}^n \frac{(x_i - \bar{x})^2}{2 \bar{x}^2 x_i} \\
      &= \frac{1}{2} \sum_{i = 1}^n
      \left(
        \frac{1}{x_i} - \frac{1}{\bar{x}}
      \right).
    \end{align*}
    On obtient donc
    \begin{align*}
      \hat{\lambda} &= \frac{n}{2H}\\
      &= \frac{n}{\sum_{i = 1}^n X_i^{-1} - \bar{X}^{-1}}.
    \end{align*}
  \end{sol}
\end{exercice}


\begin{exercice}
\label{ex:moment:pareto}
Soit $X_1, \ldots, X_n$ un échantillon aléatoire issu d'une distribution Pareto type II tel que pour tout $i \in \{ 1, \ldots , n\}$ et $x >0$, 
$$
F(x)=1-\left(\frac{\theta}{x+\theta}\right)^\alpha
\quad \mbox{ et } \quad 
f(x)=\frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}},
$$
avec paramètres inconnus $\alpha>0$ et $\theta>0$. 
\begin{enumerate}
\item Trouver $\ex[X_1]$ et $\vr(X_1)$.
\item On suppose $\alpha>2$. Construire des estimateurs pour les paramètres $\alpha$ et $\theta$ en utilisant la méthode des moments.
\item Écrire la vraisemblance, la log-vraisemblance et les deux équations qui doivent être résolues pour trouver les estimateurs du maximum de vraisemblance pour $\alpha$ et $\theta$.
\end{enumerate}
\begin{rep}
a) $\ex[X_1]= \theta/(\alpha-1)$ et $
\vr(X_1)=\alpha\theta^2/\{(\alpha-1)^2(\alpha-2)\}$ 

b) $\hat\alpha = 2S_n^2/(S_n^2-\bar X_n^2)$ et $\hat\theta=\bar X_n (S_n^2+\bar X_n^2)/(S_n^2-\bar X_n^2)$
\end{rep}
\begin{sol}
\begin{enumerate}
\item On a
\begin{align*}
\ex[X_1]&=\int_0^\infty x \frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}}\d x.
\end{align*}
On pose $t=x+\theta$,  
\begin{align*}
\ex[X_1] &= \int_\theta^\infty (t-\theta) \frac{\alpha\theta^\alpha}{t^{\alpha+1}}\mbox{d} t\\
&=\int_\theta^\infty \frac{\alpha\theta^\alpha}{t^{\alpha}}\d t-\int_\theta^\infty \frac{\alpha\theta^{\alpha+1}}{t^{\alpha+1}}\mbox{d} t\\
&= \left.\frac{-\alpha\theta^\alpha}{(\alpha-1)t^{\alpha-1}}\right|_\theta^\infty+ \left.\frac{\alpha\theta^{\alpha+1}}{\alpha t^{\alpha}}\right|_\theta^\infty, \quad \alpha>1\\
&=\frac{\alpha\theta}{\alpha-1}-\theta = \frac{\theta}{\alpha-1}.
\end{align*}

De même,
\begin{align*}
\ex[X_1^2]&=\int_0^\infty x^2 \frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}}\d x.
\end{align*}
On pose $t=x+\theta$,  
\begin{align*}
\ex[X_1^2]&=\int_\theta^\infty (t-\theta)^2 \frac{\alpha\theta^\alpha}{t^{\alpha+1}}\d t
\end{align*}
En intégrant par parties,
\begin{align*}
\ex[X_1^2]&= \left.\frac{-(t-\theta)^2\theta^\alpha}{t^{\alpha}}\right|_\theta^\infty+ \int_\theta^\infty 2(t-\theta)\frac{\theta^{\alpha}}{ t^{\alpha}}\d t\\
&=\left.\frac{-2(t-\theta)\theta^\alpha}{(\alpha-1)t^{\alpha-1}}\right|_\theta^\infty+ \int_\theta^\infty 2\frac{\theta^{\alpha}}{ (\alpha-1)t^{\alpha-1}}\d t\\
&=\left.\frac{-2\theta^\alpha}{(\alpha-1)(\alpha-2)t^{\alpha-2}}\right|_\theta^\infty, \quad \alpha>2\\
&=\frac{2\theta^\alpha}{(\alpha-1)(\alpha-2)\theta^{\alpha-2}}= \frac{2\theta^2}{(\alpha-1)(\alpha-2)}.
\end{align*}
Ainsi
$$
\vr(X_1)=\frac{2\theta^2}{(\alpha-1)(\alpha-2)}-\frac{\theta^2}{(\alpha-1)^2}=\frac{\alpha\theta^2}{(\alpha-1)^2(\alpha-2)}.
$$

\item On a $\bar X_n=\frac{1}{n}\sum_{i=1}^nX_i$ pour la moyenne échantillonnale et $S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X_n)^2$ pour la variance échantillonnale. Les estimateurs des moments sont tels que
\begin{align}
\bar X_n &= \frac{\hat\theta}{\hat\alpha-1} \label{eq:ajust3}\\
S_n^2 &= \frac{\hat\alpha\hat\theta^2}{(\hat\alpha-1)^2(\hat\alpha-2)}\label{eq:ajust4}
\end{align}
En utilisant \eqref{eq:ajust3}, on a $\hat\theta=\bar X_n (\hat\alpha -1)$. Aussi, on note que
\begin{align*}
S_n^2= \frac{\hat\alpha\hat\theta^2}{(\hat\alpha-1)^2(\hat\alpha-2)}&= \left(\frac{\hat\theta}{\hat\alpha-1}\right)^2\frac{\hat\alpha}{\hat\alpha-2}\\
& = \bar X_n^2\frac{\hat\alpha}{\hat\alpha-2}, \quad \mbox{ avec \eqref{eq:ajust3}}.
\end{align*}
On réarrange pour trouver
$$
\hat\alpha = \frac{2S_n^2}{S_n^2-\bar X_n^2}
$$
et
$$
\hat\theta=\bar X_n \left(\frac{2S_n^2}{S_n^2-\bar X_n^2} -1\right)=\bar X_n \frac{S_n^2+\bar X_n^2}{S_n^2-\bar X_n^2}.
$$

\item La vraisemblance est
$$
\mathcal{L}(\alpha,\theta)=\prod_{i=1}^n\frac{\alpha\theta^\alpha}{(x_i+\theta)^{\alpha+1}}=\frac{\alpha^n\theta^{n\alpha}}{\prod_{i=1}^n(x_i+\theta)^{\alpha+1}}
$$
et la log-vraisemblance est
\begin{align*}
\ell(\alpha,\theta)&=n\ln \alpha+n\alpha\ln \theta-\ln\left\{\prod_{i=1}^n(x_i+\theta)^{\alpha+1}\right\}\\
&=n\ln \alpha+n\alpha\ln \theta-(\alpha+1)\sum_{i=1}^n\ln(x_i+\theta).
\end{align*}
Les dérivées partielles sont
\begin{align*}
\frac{\partial\ell(\alpha,\theta)}{\partial \alpha}&=\frac{n}{ \alpha}+n\ln \theta-\sum_{i=1}^n\ln(x_i+\theta)\\
\frac{\partial\ell(\alpha,\theta)}{\partial \theta}&=\frac{n\alpha}{ \theta}-(\alpha+1)\sum_{i=1}^n\frac{1}{x_i+\theta}.
\end{align*}
Les estimateurs du maximum de vraisemblance sont tels que
\begin{align*}
\frac{n}{\hat\alpha}+n\ln \hat\theta-\sum_{i=1}^n\ln(x_i+\hat\theta)&=0\\
\frac{n\hat\alpha}{ \hat\theta}-(\hat\alpha+1)\sum_{i=1}^n\frac{1}{x_i+\hat\theta}&=0.
\end{align*}
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire issu d'une loi
  uniforme sur l'intervalle $(a, b)$ où $a$ et $b$ sont des constantes
  inconnues. Calculer l'estimateur du maximum de vraisemblance de $a$
  et $b$.
  \begin{rep}
    $\hat{a} = \min(X_1, \dots, X_n)$ et $\hat{b} = \max(X_1, \dots,
    X_n)$
  \end{rep}
  \begin{sol}
    La fonction de vraisemblance est
    \begin{equation*}
      L(a, b) = \left( \frac{1}{b - a} \right)^n\mathbf{1}(a < x_1, \dots, x_n < b).
    \end{equation*}
    Pour maximiser cette fonction, il
    faut minimiser la quantité $b - a$ en choisissant une valeur de
    $b$ la plus petite possible et une valeur de $a$ la plus grande
    possible. Étant donné le support de la distribution, on choisit
    donc $\hat{a} = \min(X_1, \dots, X_n)$ et $\hat{b} = \max(X_1,
    \dots, X_n)$.
  \end{sol}
\end{exercice}

\begin{exercice}
On suppose que $X_1, \ldots, X_n$ forment un échantillon aléatoire d'une loi uniforme sur l'intervalle $(0, 2\theta +1)$ pour un paramètre inconnu $\theta > -1/2$.
\begin{enumerate}
\item Calculer l'estimateur du maximum de vraisemblance pour $\theta$.
\item Calculer l'EMV de $\vr (X)$, où $X$ suit une distribution $\mathcal{U}(0, 2\theta+1)$.
\end{enumerate}
\begin{rep}
a) $\{\max(X_1,\dots,X_n) -1\}/2$
b) $\{\max(X_1,\dots,X_n)\}^2/12$
\end{rep}

\begin{sol}
\begin{enumerate}
\item La fonction de vraisemblance de $\theta$ pour $x_1,\dots, x_n$ est 
\begin{align*}
L(\theta) &= \left(\frac{1}{2 \theta + 1}\right)^n \boldsymbol{1}(x_1 \le 2 \theta + 1) \times \dots \times\boldsymbol{1}(x_n \le 2 \theta + 1)\\
& = \left(\frac{1}{2 \theta + 1}\right)^n \boldsymbol{1}\{\max(x_1,\dots,x_n) \le 2 \theta + 1\}\\
& = \left(\frac{1}{2 \theta + 1}\right)^n \boldsymbol{1}\left\{  \theta \ge \frac{\max(x_1,\dots,x_n) -1}{2}\right\}.
\end{align*}
Puisque $\{1/(2\theta+1)\}^n$ est décroissante en $\theta$, $L(\theta)$ est maximisée à 
$$
\hat \theta_n = \frac{\max(x_1,\dots,x_n) -1}{2}
$$
L'EMV de $\theta$ est donc
$$
\hat \theta_n = \frac{\max(X_1,\dots,X_n) -1}{2}.
$$

\item La variance peut être calculée comme suit~:
$$
\ex(X) = \int_0^{2\theta  +1} x \, \frac{1}{2\theta + 1} \, \mbox{d} x = \frac{2 \theta  +1}{2} 
$$
et
$$
\ex(X^2) =  \int_0^{2\theta  +1} x^2 \, \frac{1}{2\theta + 1} \, \mbox{d}x = \frac{(2\theta + 1)^2}{3} \, .
$$
Par conséquent,
$$
\vr(X) = \ex(X^2) - \{\ex(X)\}^2 = \frac{(2\theta + 1)^2}{3}- \frac{(2\theta + 1)^2}{4} = \frac{(2\theta + 1)^2}{12} \, .
$$
Puisque l'application $g : (0,\infty) \to (0,\infty)$ donnée par
$$
g(x) = \frac{(2x + 1)^2}{12}
$$
est strictement croissante (et donc un-pour-un), la propriété d'invariance de l'EMV donne que l'EMV de $\vr(X)$ est
\begin{align*}
\frac{(2\hat\theta_n + 1)^2}{12} = \frac{\{\max(X_1,\dots,X_n)\}^2}{12} \, .
\end{align*}

\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire issu d'une
  distribution dont la loi de probabilité est
  \begin{displaymath}
    \prob{X = x} = \theta^x (1 - \theta)^{1 - x}, \quad x = 0, 1,
    \quad 0 \leq \theta \leq \frac{1}{2}.
  \end{displaymath}
  \begin{enumerate}
  \item Calculer les estimateurs du maximum de vraisemblance et des
    moments de $\theta$.
  \item Calculer l'erreur quadratique moyenne pour les estimateurs
    développés en a).
  \item Lequel des estimateurs obtenus en a) est le meilleur?
    Justifier.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $\tilde{\theta} = \bar{X}$, $\hat{\theta} =
      \min(\bar{X}, 1/2)$
    \item EQM$(\tilde{\theta}) = \theta (1 - \theta)/n$,
      EQM$(\hat{\theta}) =
      \sum_{y = 0}^{[n/2]} (y/n - \theta)^2 \binom{n}{y} \theta^y (1 -
      \theta)^{n-y} + \sum_{y = [n/2] + 1}^n (1/2 - \theta)^2
      \binom{n}{y} \theta^y (1 - \theta)^{n - y}$
    \item EQM$(\hat{\theta}) \leq$ EQM$(\tilde{\theta})$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item La distribution de $X$ est une Bernoulli avec une
      restriction sur la valeur du paramètre $\theta$. On a donc que
      $\esp{X} = \theta$. L'estimateur des moments de $\theta$ est
      donc $\tilde{\theta} = \bar{X}$.

      Pour l'estimateur du maximum de vraisemblance, on a, en posant
      $y = \sum_{i = 1}^n x_i$,
      \begin{align*}
        L(\theta) &= \theta^{y} (1 - \theta)^{n - y}, \\
        l(\theta) &= y \ln(\theta) + (n - y) \ln(1 - \theta) \\
        \intertext{et}
        l^\prime(\theta) &= \frac{y}{\theta} - \frac{(n - y)}{1 - \theta}\\
        &= \frac{y - n\theta}{\theta (1 - \theta)}\\
        &= \frac{n \bar{x} - n \theta}{\theta (1 - \theta)}.
      \end{align*}
      Ainsi, la log-vraisemblance est croissante pour $\theta \leq
      \bar{x}$ et décroissante pour $\theta > \bar{x}$ (voir la
      figure~\ref{fig:ponctuelle:uniforme}). Le maximum est donc
      atteint en $\bar{x}$. Cependant, puisque $0 \leq \theta \leq
      1/2$ on doit avoir $\hat{\theta} \leq 1/2$. On a donc
      $\hat{\theta} = \min(\bar{X}, 1/2)$.
      \begin{figure}
        \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=0.6\textwidth]{figure/unnamed-chunk-23-1} 

\end{knitrout}
        \caption{Graphique de la fonction de log-vraisemblance de
          l'exercice
          \ref{chap:ajustement}.\ref{ex:ponctuelle:uniforme} pour $n =
          10$ et $y = 3$.}
        \label{fig:ponctuelle:uniforme}
      \end{figure}
    \item Premièrement, on remarque que $Y = \sum_{i = 1}^n X_i = n
      \bar{X} \sim \text{Binomiale}(n, \theta)$ avec $0 \leq \theta
      \leq 1/2$. Deuxièmement, on sait que $\mbox{EQM}(\hat{\theta}) =
      \var{\hat{\theta}} + b(\hat{\theta})^2$, où $\hat{\theta}$ est
      un estimateur quelconque d'un paramètre $\theta$ et
      $b(\hat{\theta}) = \esp{\hat{\theta}} - \theta$ est le biais de
      l'estimateur.

      Pour l'estimateur des moments $\tilde{\theta} = Y/n$, on a
      \begin{align*}
        \mbox{EQM}(\tilde{\theta}) &= \frac{\Var{Y}}{n^2} +
        b(Y/n)^2 \\
        &= \frac{\theta (1 - \theta)}{n},
      \end{align*}
      puique $\esp{Y/n} = \theta$.

      Pour l'estimateur du maximum de vraisemblance
      \begin{equation*}
        \hat{\theta} =
        \begin{cases}
          \frac{Y}{n}, & Y \leq \frac{n}{2} \\
          \frac{1}{2}, & Y > \frac{n}{2},
        \end{cases}
      \end{equation*}
      il est plus simple de développer l'erreur quadratique moyenne
      ainsi:
      \begin{align*}
        \mbox{EQM}(\hat{\theta}) &= \esp{(\hat{\theta} - \theta)^2} \\
        &= \sum_{y = 0}^n (\hat{\theta} - \theta)^2 \prob{Y = y} \\
        &= \sum_{y = 0}^{[n/2]}
        \left(
          \frac{y}{n} - \theta
        \right)^2 \binom{n}{y} \theta^y (1 - \theta)^{n - y} \\
        &\phantom{=} +
        \sum_{y = [n/2] + 1}^n
        \left(
          \frac{1}{2} - \theta
        \right)^2 \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
      \end{align*}
    \item On compare les erreurs quadratiques moyennes des deux
      estimateurs. Soit
      \begin{displaymath}
        \mbox{EQM}(\tilde{\theta}) = \sum_{y = 0}^n
        \left(
          \frac{y}{n} - \theta
        \right)^2 \binom{n}{y} \theta^y (1 - \theta)^{n - y},
      \end{displaymath}
      d'où
      \begin{align*}
        \mbox{EQM}(\tilde{\theta}) - \mbox{EQM}(\hat{\theta}) &=
        \sum_{y=[n/2] + 1}^n
        \left(
          \frac{y}{n} + \frac{1}{2} - 2 \theta
        \right)
        \left(
          \frac{y}{n} - \frac{1}{2}
        \right) \\
        &\phantom{=} \times
        \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
      \end{align*}
      Étant donné que $y/n > 1/2$ et que $\theta \leq 1/2$, tous les
      termes dans la somme sont positifs. On a donc que
      $\mbox{EQM}(\tilde{\theta}) - \mbox{EQM}(\hat{\theta}) > 0$ ou, de manière
      équivalente, $\mbox{EQM}(\hat{\theta}) < \mbox{EQM}(\tilde{\theta})$. En terme
      d'erreur quadratique moyenne, l'estimateur du maximum de
      vraisemblance est meilleur que l'estimateur des moments selon ce critère.
    \end{enumerate}
  \end{sol}
\end{exercice}

% Qualité de l'ajustement 

\begin{exercice}
Un assureur IARD américain souhaite modéliser les montants des sinistres pour une assurance automobile privée. Les données disponibles contiennent $n=6773$ réclamations, et $X_1,\ldots,X_n$ représentent les montants en dollars US. Les réclamations sont considérées indépendantes. Un sommaire et un histogramme des réclamations sont fournis ci-dessous. On a également
$$
\frac{1}{n}\sum_{i=1}^n x_i=1~853 \quad \mbox{ et }\quad \frac{1}{n}\sum_{i=1}^n x_i^2=10~438~832.
$$



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{summary}\hlstd{(PAID)}
\end{alltt}
\begin{verbatim}
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##     9.5   523.7  1001.7  1853.0  2137.4 60000.0
\end{verbatim}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-25-1} 

\end{knitrout}


\begin{enumerate}
\item En utilisant les informations fournies, argumenter qu'une loi normale ne devrait pas être utilisée pour modéliser les montants de réclamations. Proposer (au moins) une distribution qui semble appropriée et justifier.

\item On suppose que les montants de réclamations suivent une loi log-normale définie à l'exercice~\ref{chap:ajustement}.\ref{ex:moment:distln}. Trouver les estimateurs des moments des paramètres $\mu$ et $\sigma^2$.

\item On suppose que les montants de réclamations sont distribués selon une loi Pareto type II définie à l'exercice~\ref{chap:ajustement}.\ref{ex:moment:pareto}. Trouver les estimateurs des paramètres $\alpha$ et $\theta$ par la méthode des moments.

\item Les estimateurs du maximum de vraisemblance des distributions log-normale et Pareto peuvent être calculés par optimisation numérique en utilisant les estimateurs des moments trouvés en b) et c) comme valeurs de départ. Les résultats sont les suivants:
\begin{description}
\item[Log-normale]: Les estimateurs sont $\hat\mu_{MV}=6,95561$ et $\hat\sigma^2_{MV}=1,14698$ et la valeur de log-vraisemblance est $-57~185,11$.
\item[Pareto]: Les estimateurs sont $\hat\alpha_{MV}=4,71364$ et $\hat\theta_{MV}=6~819,891$ et la valeur de log-vraisemblance est $-57~500,12$.
\end{description}
Calculer l'AIC pour chacune des distributions. Quelle distribution semble être préférable selon ce critère?

\item Les diagrammes quantile-quantile de chacun des modèles sont présentés ci-dessous. Commenter sur l'ajustement des modèles. Quel modèle recommandez-vous à l'assureur? Pourquoi?

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-26-1} 

\end{knitrout}

\item L'assureur s'intéresse à la queue à droite de la distribution et souhaite savoir les quantiles de niveau 99~\% et 99,5~\% pour les distributions des montants. Donner une estimation de ces quantiles sous les modèles log-normal et Pareto ajustés en d). [\emph{Astuce: Observer qu'il est possible d'inverser explicitement la fonction de répartition de la Pareto, et que les quantiles de la log-normale peuvent être exprimés en termes de quantiles de la loi normale.}]

\end{enumerate}
\begin{rep}
b) $\hat \mu =6.969$ et $\hat\sigma^2 =1.112$ 
c) $\hat\alpha =3.922$ et 
$\hat\theta=5414.77$
d) Log-normale~: $114~374,2$, Pareto~: $115~004,2$
f) 11~296,76,  14~166,68,  12~720,54  et  16~537,1
\end{rep}
\begin{sol}
\begin{enumerate}

\item La loi normale est définie sur les nombres réels, alors que les montants sont positifs seulement. Le domaine d'une loi normale n'est donc pas le même que le domaine des montants de réclamation. La moyenne échantillonnale et la médiane ne sont pas égales, ce qui serait le cas si le modèle normal était approprié. L'histogramme des montants ne ressemblent pas à une cloche symétrique. De bonnes options seraient les distributions Gamma, log-normale ou Pareto puisqu'elles sont toutes définies sur les réels positifs et sont asymétriques.

\item On a $\bar x_n=1~853$ et $m_2=\frac{1}{n}\sum_{i=1}^n x_i^2 = 10~438~832$. Selon l'exercice~\ref{chap:ajustement}.\ref{ex:moment:distln}~b), on a
\begin{align*}
\hat \mu &= \ln\left\{\frac{\bar x_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2}} \right\}=\ln\left\{\frac{1~853^2}{\sqrt{10~438~832}} \right\}=6.969\\
\hat\sigma^2 &= \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n x_i^2}{\bar x_n^2} \right\}=  \ln\left\{\frac{10~438~832}{1~853^2} \right\}=1.112.
\end{align*}


\item On a $\bar x_n=1~853$ et $m_2=\frac{1}{n}\sum_{i=1}^n x_i^2 = 10~438~832$, donc 
\begin{align*}
s_n^2&=\frac{1}{n-1}\sum_{i=1}^n x_i^2 - \frac{n}{n-1}\bar x_n^2\\
& = \frac{n}{n-1}(m_2-\bar x_n^2)\\
&=\frac{6~773}{6~772}(10~438~832-1~853^2)=7~006~257.
\end{align*}
Selon l'exercice~\ref{chap:ajustement}.\ref{ex:moment:pareto}~b), on trouve
\begin{align*}
\hat\alpha &= \frac{2s_n^2}{s_n^2-\bar x_n^2}=3.922\\
\hat\theta&=\bar x_n \left(\frac{2s_n^2}{s_n^2-\bar x_n^2} -1\right)=5414.77.
\end{align*}


\item L'AIC est donné par $2(-\ell(\hat\theta)+k)$, où $\ell(\hat\theta)$ est la valeur maximale de la vraisemblance et $k$ le nombre de paramètres dans le modèle. Il y a deux paramètres dans chaque modèle. Les AIC sont donc
\begin{description}
\item[Log-normale]: $2\times(57~185,11+2)=114~374,2$.
\item[Pareto]: $2\times(57~500,12+2)=115~004,2$
\end{description}
Selon le critère AIC, le meilleur modèle est celui avec la valeur la plus petite, la distribution log-normale est donc préférable.

\item L'ajustement des deux modèles n'est pas parfait parce que les points ne sont pas exactement alignés, plus spécialement pour de grandes valeurs de $x$. Les points sont au-dessus de la ligne dans le graphique Quantiles-Quantiles de la Pareto, ce qui signifie que la queue de la distribution empirique est plus épaisse que celle de la Pareto. Cela devrait inquiéter l'assureur puisque ça signifie qu'il sous-estimerait les réclamations. L'ajustement de la log-normale n'est pas parfait non plus, mais il y a moins de sous-estimation, car plusieurs points se retrouvent sous la ligne et les extrêmes sont moins éloignés. Par conséquent, on recommande l'utilisation du modèle log-normal.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-27-1} 

\end{knitrout}

\item Les quantiles de la loi Pareto sont l'inverse de sa fonction de répartition. Si $\omega_\kappa$ est le $(1-\kappa)$e quantile, c'est-à-dire qu'il est tel que $F(\omega_\kappa)=1-\kappa$, alors en utilisant la fonction de répartition fournie à l'exercice~\ref{chap:ajustement}.\ref{ex:moment:pareto}, on a
\begin{align*}
1-\left(\frac{\theta}{\omega_\kappa+\theta}\right)^\alpha &= 1-\kappa\\
\frac{\theta}{\omega_\kappa+\theta} &= \kappa^{1/\alpha}\\
\omega_\kappa= \frac{\theta}{\kappa^{1/\alpha}}-\theta.
\end{align*}

Une estimation du quantile 99~\% est
$$
\hat\omega_{1\%}= \frac{\hat\theta}{0,01^{1/\hat\alpha}}-\hat\theta
$$
et en utilisant l'EMV donné en (d), on a 
$$
\hat\omega_{1\%}= \frac{6~819,891}{0,01^{1/ 4,71364}}-6~819,891=11~296,76
$$
et une estimation du quantile 99,5\% est
$$
\hat\omega_{0,5\%}= \frac{6~819,891}{0,005^{1/4,71364}}-6~819,891= 14~166,68
$$

Si $\nu_\kappa$ représente le $(1-\kappa)$e quantile de la loi log-normale,
$$
F(\nu_\kappa)=1-\kappa \Rightarrow \Phi\left(\frac{\ln\nu_\kappa-\mu}{\sigma}\right)=1-\kappa,
$$
où $\Phi$ est la fonction de répartition de $\mathcal{N}(0,1)$. Cela signifique que le $(1-\kappa)$e quantile de la loi normale centrée réduite est utilisé pour trouver
$$
\frac{\ln\nu_\kappa-\mu}{\sigma}=z_\kappa \Rightarrow \nu_\kappa = \exp(\sigma z_\kappa+\mu).
$$
Une estimation du quantile 99~\% est
$$
\hat\nu_{1\%}= \exp(\hat\sigma z_{1\%}+\hat\mu),
$$
où $z_{1\%}=2,33$. En utilisant l'EMV donné en d), on a 
$$
\hat\nu_{1\%}= \exp(\sqrt{1,14698}\times 2,33+6,95561)=12~720,54
$$
et une estimation du quantile 99,5\% est
$$
\hat\nu_{0,5\%}= \exp(\sqrt{1,14698}\times 2,575+6,95561)=16~537,1
$$
Tel attendu en regardant les diagrammes quantile-quantile, les quantiles estimés par la distribution log-normale sont supérieurs à ceux estimés par la distribution Pareto.
\end{enumerate}


\end{sol}
\end{exercice}

%
%\begin{exercice}
%Télécharger le fichier \texttt{contents.csv} sur le site de cours. Il contient les pertes de biens, dues à un incendie, de plus d'un million de couronnes danoises provenant de réclamations faites à la compagnie de réassurance Copenhagen Re entre 1980 et 1990. Les données peuvent être importées en \textsf{R} comme suit, après avoir changé l'environnement de travail de \textsf{R} au dossier dans lequel vous avez enregistré le fichier \texttt{contents.csv}:
%
%<<>>=
%ct <- read.csv("contents.csv",header=TRUE,row.names=1)
%attach(ct)
%data <- Contents
%@
%
%Les données peuvent aussi être importées par \textsf{RStudio} en cliquant sur ``Import Dataset''.
%
%\begin{enumerate}
%\item En utilisant la fonction \texttt{fitdistr} de la librairie \texttt{MASS}, ajuster divers modèles paramétriques en utilisant le maximum de vraisemblance: loi exponentielle, Gamma, log-normale et normale.
%
%\item Pour chacun de ces quatre modèles, afficher le diagramme quantile-quantile et commenter sur la qualité de l'ajustement.
%
%\item Sélectionner le meilleur modèle ajusté selon le critère AIC.
%
%\item Basé sur les résultats de a)--c), êtes-vous satisfaits du modèle sélectionné? Pensez-vous qu'il y a place à amélioration? Justifier.
%\end{enumerate}
%
%\begin{sol}
%\begin{enumerate}
%
%\item Premièrement, on importe les données en \texttt{R}:
%<<>>=
%ct <- read.csv("contents.csv",header=TRUE,row.names=1)
%attach(ct)
%data <- Contents
%@
%Ensuite, les divers modèles peuvent être ajustés comme suit:
%<<>>=
%library(MASS)
%m1 <- fitdistr(data,densfun="exponential")
%m2 <- fitdistr(data,densfun="lognormal")
%m3 <- fitdistr(data,densfun="gamma",start=list(shape=0.5,rate=0.2))
%m4 <- fitdistr(data,densfun="normal")
%@
%
%Les paramètres estimés sont:
%\begin{center}
%\begin{tabular}{l|r}
%\hline
%Distribution ajustée & Paramètres \\
%\hline
%Exponentielle & $\hat \beta_n = round(1/m1$estimate,3)$\\
%\hline
%Log-normale & $\hat \mu_n = round(m2$estimate[1],3)$\\
%& $\hat \sigma^2_n = round(m2$estimate[2],3)$\\
%\hline
%Gamma & $\hat \alpha_n = round(m3$estimate[1],3)$\\
%&  $\hat \beta_n = round(1/m3$estimate[2],3)$\\
%\hline
%Normale & $\hat \mu_n =  round(m4$estimate[1],3)$\\
%&  $\hat \sigma^2_n = round(m4$estimate[2],3)$\\
%\hline
%\end{tabular}
%\end{center}
%
%\item Les diagrammes quantiles-quantiles peuvent être codés comme suit: 
%<<fig.height=6>>=
%par(mfrow=c(2,2))
%n <- length(data)
%q <- 1:n/(n+1)
%plot(qexp(q,rate=m1$estimate), sort(data),
%     xlab="Quantiles th\u{E9}orique de l'exponentielle",
%     ylab="Quantiles empiriques")
%lines(sort(data), sort(data))
%
%plot(qlnorm(q,meanlog=m2$estimate[1], sdlog=m2$estimate[2]), 
%     sort(data), xlab="Quantiles th\u{E9}orique de la log-normale", 
%     ylab="Quantiles empiriques")
%lines(sort(data), sort(data))
%
%plot(qgamma(q,shape=m3$estimate[1],rate=m3$estimate[2]), sort(data),
%     xlab="Quantiles th\u{E9}orique de la Gamma", 
%     ylab="Quantiles empiriques")
%lines(sort(data),sort(data))
%
%plot(qnorm(q,mean=m4$estimate[1],sd=m4$estimate[2]), sort(data),
%     xlab="Quantiles th\u{E9}orique de la normale",
%     ylab="Quantiles empiriques")
%lines(sort(data),sort(data))
%@
%
%Les diagrammes quantiles-quantiles montrent que le modèle normal ne s'ajuste pas bien du tout. Le modèle exponentiel est légèrement mieux, mais s'ajuste tout de même mal, spécifiquement dans les quantiles élevés. Les modèles Gamma et log-normal sont meilleurs et l'ajustement est similaire pour les deux. Cependant, pour les deux modèles, les quantiles très élevés ne sont pas bien capturés: les quantiles prédits par les modèles sont trop bas. Le modèle log-normal prédit les quantiles élevés légèrement mieux que le modèle Gamma.
%
%\item La sélection de modèle selon l'AIC peut se faire comme suit:
%<<>>=
%AIC(m1,m2,m3,m4)
%@
%Le modèle avec le plus petit AIC est \texttt{m2}, i.e., le modèle log-normal.
%
%\item Le modèle sélectionné, i.e., le modèle log-normal, ne modélise pas adéquatement les quantiles élevés. Cependant, ceux-ci sont particulièrement importants pour une compagnie d'assurance puisque que ce sont eux qui causent les plus grandes pertes et qui doivent être modélisés avec précision. Par conséquent, on devrait tenter de trouver un meilleur modèle.
%\end{enumerate}
%\end{sol}
%\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}


%\section*{Exercices proposés dans \cite{Wackerly:mathstat:7e:2008}}
%
%\begin{trivlist}
%\item 3.145--3.151, 3.153, 3.155, 3.158, 3.159, 3.161, 3.162, 3.163
%\end{trivlist}


%%%
%%% Insérer les réponses
%%%
\input{reponses-ajustement}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_analyse_statistique"
%%% coding: utf-8-unix
%%% End:


\chapter{Estimation par intervalle}
\label{chap:intervalle}

\Opensolutionfile{reponses}[reponses-intervalle]
\Opensolutionfile{solutions}[solutions-intervalle]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:intervalle}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:intervalle}}

\end{Filesave}

%%%
%%% Début des exercices
%%%

\begin{exercice}
Soit $X_1, \ldots, X_n$ un échantillon aléatoire de taille $n$ tiré d'une distribution Gamma avec paramètres $\alpha=2$ et $\beta$ inconnu.
\begin{enumerate}
\item Montrer que $T = 2(X_1 +\cdots+X_n)/\beta$ est un pivot et que sa distribution est khi-carrée avec $4n$ degrés de liberté. [Astuce: utiliser les fonctions génératrices des moments.]
\item Utiliser le pivot $T$ pour construire un intervalle de confiance bilatéral de niveau 95~\% pour~$\beta$.
\item Si la moyenne échantillonnale est de $\bar x = 5,6$ et que $n=5$, utiliser le résultat en b) pour donner un intervalle de confiance de niveau 95~\% pour $\beta$.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item La fonction génératrice des moments d'une distribution Gamma avec paramètres $\alpha = 2$ et $\beta > 0$ est donnée par 
$$
M_X(t) = (1-\beta t)^{-2}, \quad t < 1/\beta.
$$
La fonction génératrice des moments de $T = 2(X_1 + \cdots + X_n)/\beta$ est donc
$$
M_T(t) = \ex (e^{(2t/\beta) \sum_{i=1}^n X_i}) = \prod_{i=1}^n \ex \left\{ e^{(2t/\beta) X_i} \right\} = \{ M_X(2t/\beta)\}^n = (1-2t)^{-2n}
$$
à condition que $2t/\beta < 1/\beta$, i.e., si $t < 1/2$. Sachant que $(1-2t)^{-2n}$ est la fonction génératrice des moments d'une distribution khi-carrée avec $4n$ degrés de liberté, $T \sim \chi^2_{(4n)}$.

Ainsi, $T$ est une fonction de l'échantillon aléatoire et du paramètre $\beta$, avec une distribution connue qui ne dépend d'aucun paramètre inconnu~: $T$ est donc un pivot.

\item Pour construire l'intervalle de confiance bilatéral à partir de $T$, on note $\chi^2_{0,975, \, 4n}$ et $\chi^2_{0,025, \, 4n}$ les quantiles $2,5$\% et $97,5$\% d'une distribution $\chi^2$ avec $4n$ degrés de liberté. On a donc
$$
\Pr( \chi^2_{0,975, \, 4n} \le T \le \chi^2_{0,025, \, 4n}) = 0,95.
$$
On résout les inégalités
$$
 \chi^2_{0,975, \, 4n} \le T \le \chi^2_{0,025, \, 4n} 
 $$
ou encore 
 $$
 \chi^2_{0,975, \, 4n} \le \frac{2}{\beta} \sum_{i=1}^n X_i \le \chi^2_{0,025, \, 4n},
$$
ce qui donne un l'intervalle de confiance bilatéral de niveau $95$~\% pour $\beta$~:
\begin{align*}
\left[ \frac{2n \bar X_n}{ \chi^2_{0,025, \, 4n}} \, , \frac{2n \bar X_n}{ \chi^2_{0,975, \, 4n}}\right].
\end{align*}

\item Avec $n = 5$, les quantiles requis d'une distribution khi-carrée avec $4n = 20$ degrés de liberté sont
$$
 \chi^2_{0,975, \, 20} = 9.59, \quad  \chi^2_{0,025, \, 20} = 34.17.
$$ 
Sachant que $\bar x_n = 5,6$, l'intervalle de confiance de niveau $95$~\% pour $\beta$ est 
\begin{align*}
\left[ \frac{2\times 5\times 5,6}{ 34.17} \, , \frac{2\times 5\times 5,6}{ 9.59}\right] = [ 1.64, 5.84].
\end{align*}
\end{enumerate}
	\end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:intervalle:moyenne}
  La valeur observée de la moyenne empirique $\bar{X}$ d'un
  échantillon aléatoire de taille $20$ tiré d'une $N(\mu, 80)$ est
  $81,2$. Déterminer un estimateur par intervalle de niveau $95~\%$
  pour $\mu$.
  \begin{rep}
    $(77,28, \, 85,12)$
  \end{rep}
  \begin{sol}
    On cherche deux statistiques $L$ et $U$ tel que
    \begin{equation*}
      \prob{L \leq \mu \leq U} = 1 - \alpha.
    \end{equation*}
    On sait que si $X_1, \dots, X_n$ est un échantillon aléatoire tiré
    d'une distribution $N(\mu, \sigma^2)$, alors $\bar{X} \sim N(\mu,
    \sigma^2/n)$ ou, de manière équivalente, que
    \begin{displaymath}
      \frac{\bar{X} - \mu}{\sigma / \sqrt{n}} \sim N(0, 1).
    \end{displaymath}
    Par conséquent,
    \begin{equation*}
      \Prob{-z_{\alpha/2} \leq \frac{\bar{X} - \mu}{\alpha/\sqrt{n}} \leq
        z_{\alpha/2}} = 1 - \alpha,
    \end{equation*}
    d'où
    \begin{equation*}
      \Prob{\bar{X} - \frac{\sigma}{\sqrt{n}}\, z_{\alpha/2} \leq \mu
        \leq \bar{X} + \frac{\sigma}{\sqrt{n}}\, z_{\alpha/2}} = 1 - \alpha.
    \end{equation*}
    Les statistiques $L$ et $U$ sont dès lors connues: $L = \bar{X} -
    \sigma z_{\alpha/2}/\sqrt{n}$ et $U = \bar{X} + \sigma
    z_{\alpha/2}/\sqrt{n}$. Un estimateur par intervalle de $\mu$ est
    donc
    \begin{equation*}
      (\bar{X} - \sigma z_{\alpha/2}/\sqrt{n}, \,
      \bar{X} + \sigma z_{\alpha/2}/\sqrt{n}).
    \end{equation*}
    Avec $n = 20$, $\sigma^2 = 80$ et $\bar{x} = 81,2$, on obtient
    l'intervalle $(77,28, \, 85,12)$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\bar{X}$ la moyenne d'un échantillon aléatoire de taille $n$
  d'une distribution normale de moyenne $\mu$ inconnue et de variance
  $9$. Trouver la valeur $n$ tel que, approximativement,
  $\prob{\bar{X} - 1 < \mu < \bar{X} + 1} = 0,90$.
  \begin{rep}
    $24$ ou $25$
  \end{rep}
  \begin{sol}
    On a $X \sim N(\mu, 9)$. Tel que démontré à
    l'exercice~\ref{chap:intervalle}.\ref{ex:intervalle:moyenne},
    \begin{equation*}
      \Prob{\bar{X} - \frac{\sigma}{\sqrt{n}}\, z_{0,05} \leq \mu
        \leq \bar{X} + \frac{\sigma}{\sqrt{n}}\, z_{0,05}} = 0,90.
    \end{equation*}
    Pour satisfaire la relation $\prob{\bar{X} - 1 < \mu < \bar{X} +
      1} = 0,90$, on doit donc choisir
    \begin{equation*}
      \frac{\sigma}{\sqrt{n}}\, z_{0,05} =
      \frac{3 (1,645)}{\sqrt{n}} = 1.
    \end{equation*}
    On trouve que $n = 24,35$. On doit donc choisir une taille
    d'échantillon de $24$ ou $25$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Un échantillon aléatoire comptant $17$ observations d'une
  distribution normale de moyenne et de variance inconnues a donné
  $\bar{x} = 4,7$ et $s^2 = 5,76$. Trouver des intervalles de
  confiance à $90~\%$ pour $\mu$ et pour $\sigma^2$.
  \begin{rep}
    $\mu \in (3,7, \, 5,7)$ et $\sigma^2 \in (3,50, \, 11,58)$
  \end{rep}
  \begin{sol}
    On sait que
    \begin{align*}
      \frac{\bar{X} - \mu}{S/\sqrt{n}} &\sim t(n-1) \\
      \intertext{et que}
      \frac{(n-1) S^2}{\sigma^2} &\sim \chi^2(n - 1).
    \end{align*}
    Ainsi, on peut établir que
    \begin{equation*}
      \Prob{\bar{X} - \frac{S}{\sqrt{n}}\, t_{\alpha/2} \leq \mu
        \leq \bar{X} + \frac{S}{\sqrt{n}}\, t_{\alpha/2}} = 1 - \alpha.
    \end{equation*}
    et qu'un intervalle de confiance de niveau $1 - \alpha$ pour $\mu$
    est
    \begin{equation*}
      (\bar{X} - S t_{\alpha/2}/\sqrt{n}, \,
      \bar{X} + S t_{\alpha/2}/\sqrt{n}).
    \end{equation*}
    Avec $n = 17$, $\bar{x} = 4,7$, $s^2 = 5,76$ et $\alpha = 0,10$,
    on trouve que $\mu \in (3,7, \, 5,7)$.

    Pour la variance, on cherche des valeurs $a$ et $b$, $a \leq b$
    tel que
    \begin{equation*}
      \Prob{a \leq \frac{(n-1)S^2}{\sigma^2} \leq b} =
      \Prob{\frac{(n-1)S^2}{b} \leq \sigma^2 \leq \frac{(n-1)S^2}{a}} =
      1 - \alpha.
    \end{equation*}
    Plusieurs valeurs de $a$ et $b$ satisfont cette relation. Le choix
    le plus simple est $a = \chi_{1 - \alpha/2}^2(n - 1)$ et $b =
    \chi_{\alpha/2}^2(n - 1)$. Ainsi, un intervalle de confiance de
    niveau $1 - \alpha$ pour $\sigma^2$ est
    \begin{equation*}
      \left(
        \frac{(n-1)S^2}{\chi_{\alpha/2}^2(n - 1)}, \,
        \frac{(n-1)S^2}{\chi_{1 - \alpha/2}^2(n - 1)}
      \right).
    \end{equation*}
    Dans une table de la loi khi carré, on trouve que
    $\chi_{0,05}^2(16) = 7,96$ et que $\chi_{0,95}^2(16) = 26,30$,
    d'où $\sigma^2 \in (3,50, \, 11,58)$.
  \end{sol}
\end{exercice}

\begin{exercice} 
  Lors d'une très sérieuse et importante analyse statistique de la
  taille des étudiantes en sciences et génie à l'Université Laval, on
  a mesuré un échantillon aléatoire d'étudiantes en actuariat et un
  autre en génie civil. Les résultats obtenus se trouvent résumés dans
  le tableau ci-dessous. On suppose que les deux échantillons
  aléatoires sont indépendants et que la taille des étudiantes est
  distribuée selon une loi normale.
  \begin{center}
    \begin{tabular}{lD{.}{,}{3.0}D{.}{,}{3.0}}
      \toprule
      Quantité &
      \multicolumn{1}{c}{Actuariat} &
      \multicolumn{1}{c}{Génie civil} \\
      \midrule
      Taille de l'échantillon     & 15 & 20 \\
      Taille moyenne (en cm)      & 152 & 154 \\
      Variance (en $\text{cm}^2$) & 101 & 112 \\
      \bottomrule
    \end{tabular}
  \end{center}
  \begin{enumerate}
  \item Déterminer un intervalle de confiance à $90~\%$ pour la taille
    moyenne des étudiantes de chacun des deux programmes en supposant
    que l'écart type de la distribution normale est 9~cm.
  \item Répéter la partie a) en utilisant plutôt les variances des
    échantillons.
  \item On suppose que les variances des deux populations sont égales. 
    Y a-t-il une différence significative, avec un niveau de
    confiance de 90~\%, entre la taille des étudiantes en actuariat et
    celles en génie civil? 
  \item Déterminer un intervalle de confiance à 90~\% pour la variance
    de la taille des étudiantes en actuariat.
  \item On suppose que les moyennes des deux populations sont égales.
    La différence observée entre les variances dans la taille des
    étudiantes des deux programmes est-elle significative? Utiliser un
    niveau de confiance de 90~\%.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $148,18 < \mu_1 < 155,82$ et $150,69 < \mu_2 < 157,31$
    \item $147,43 < \mu_1 < 156,57$ et $149,91 < \mu_2 < 158,09$
    \item $\mu_1 - \mu_2 \in -2 \pm 5,82$
    \item $59,71 < \sigma_1^2 < 215,22$
    \item $0,462 < \sigma_2^2/\sigma_1^2 < 2,502$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    On représente la taille des étudiantes en actuariat par la
    variable aléatoire $X$ et celle des étudiantes en génie civil par
    $Y$. On a
    \begin{align*}
      \bar{X} &\sim N(\mu_1, \sigma_1^2/15), &
      \bar{Y} &\sim N(\mu_2, \sigma_2^2/20), \\
      \frac{14 S_1^2}{\sigma_1^2} &\sim \chi^2(14), &
      \frac{19 S_2^2}{\sigma_2^2} &\sim \chi^2(19)
    \end{align*}
    et les valeurs des statistiques pour les deux échantillons sont
    \begin{align*}
      \bar{x} &= 152, & \bar{y} &= 154, \\
      s_1^2   &= 101, &  s_2^2  &= 112.
    \end{align*}
    \begin{enumerate}
    \item Si l'on suppose que $\sigma_1^2 = \sigma_2^2 = 81$, alors
      $\bar{X} \sim N(\mu_1, 5,4)$ et $\bar{Y} \sim N(\mu_2, 4,05)$.
      Par conséquent,
      \begin{gather*}
        \Prob{-1,645
          < \frac{\bar{X} - \mu_1}{\sqrt{5,4}} <
          1,645} = 0,90 \\
        \intertext{et}
        \Prob{-1,645
          < \frac{\bar{Y} - \mu_2}{\sqrt{4,05}} <
          1,645
        } = 0,90
      \end{gather*}
      d'où
      \begin{equation*}
        152 - 1,645 \sqrt{5,4} < \mu_1 < 152 + 1,645 \sqrt{5,4},
      \end{equation*}
      soit $148,18 < \mu_1 < 155,82$ et
      \begin{equation*}
        154 - 1,645 \sqrt{4,05} < \mu_2 < 154 + 1,645 \sqrt{4,05},
      \end{equation*}
      soit $150,69 < \mu_2 < 157,31$.
    \item Si la variance est inconnue, on a plutôt que
      \begin{equation*}
        \frac{\bar{X} - \mu_1}{S_1/\sqrt{15}} \sim t(14)
        \quad \text{et} \quad
        \frac{\bar{Y} - \mu_2}{S_2/\sqrt{20}} \sim t(19).
      \end{equation*}
      Or, $t_{0,05}(14) = 1,761$ et $t_{0,05}(19) =
      1,729$, d'où
      \begin{equation*}
        152 - 1,761 \sqrt{\frac{101}{15}} < \mu_1 < 152 + 1,761
        \sqrt{\frac{101}{15}},
      \end{equation*}
      soit $147,43 < \mu_1 < 156,57$ et
      \begin{equation*}
        154 - 1,729 \sqrt{\frac{112}{20}} < \mu_2 <
        154 + 1,729 \sqrt{\frac{112}{20}},
      \end{equation*}
      soit $149,91 < \mu_2 < 158,09$.
    \item On cherche un intervalle de confiance à $90~\%$ pour la
      différence $\mu_1 - \mu_2$. Si $0$ appartient à l'intervalle, on
      pourra dire que la différence entre les deux moyennes n'est pas
      significative à $90~\%$. Pour les besoins de la cause, on va
      supposer ici que $\sigma_1^2 = \sigma_2^2 = \sigma^2$. Or,
      puisque
      \begin{gather*}
        \frac{(\bar{X} - \bar{Y}) - (\mu_1 - \mu_2)}{%
          \sigma \sqrt{\frac{1}{15} + \frac{1}{20}}}
        \sim N(0, 1) \\
        \intertext{et que}
        \frac{14 S_1^2}{\sigma^2} + \frac{19 S_2^2}{\sigma^2}
        \sim \chi^2(33)
      \end{gather*}
      on établit que
      \begin{displaymath}
        \frac{(\bar{X} - \bar{Y}) - (\mu_1 - \mu_2)}{\sqrt{\frac{14
              S_1^2 + 19 S_2^2}{33} \left( \frac{1}{15} + \frac{1}{20}
            \right)}} \sim t(33).
      \end{displaymath}
      De plus, $t_{0,05}(33) \approx z_{0,05} = 1,645$, d'où
      l'intervalle de confiance à 90~\% pour $\mu_1 - \mu_2$ est
      \begin{displaymath}
        \mu_1 - \mu_2 \in -2 \pm 5,82.
      \end{displaymath}
      La différence de taille moyenne entre les deux groupes
      d'étudiantes n'est donc pas significative.
    \item Tel que mentionné précédemment,
      \begin{displaymath}
        Y = \frac{14 S_1^2}{\sigma_1^2} \sim \chi^2(14).
      \end{displaymath}
      Or, on trouve dans une table de la loi khi carré (ou avec la
      fonction \texttt{qchisq} dans \textsf{R}) que
      \begin{displaymath}
        \Prob{6,57 < Y < 23,68} = 0,90.
      \end{displaymath}
      Par conséquent,
      \begin{gather*}
        \Prob{
          6,57 < \frac{14 S_1^2}{\sigma_1^2} < 23,68
        } = 0,90 \\
        \intertext{ou, de manière équivalente,}
        \Prob{
          \frac{14 S_1^2}{23,68} < \sigma_1^2 < \frac{14 S_1^2}{6,57}
        } = 0,90.
      \end{gather*}
      Puisque $s_1^2 = 101$ dans cet exemple, un intervalle de
      confiance à $90~\%$ pour $\sigma_1^2$ est $(59,71, \, 215,22)$.
    \item Un peu comme en c), on détermine un intervalle de confiance
      pour le ratio $\sigma_2^2/\sigma_1^2$ et on conclut que la
      différence entre la variance des étudiantes en génie civil n'est
      pas significativement plus grande que celle des étudiantes en
      actuariat si cet intervalle contient la valeur $1$. À la suite
      des conclusions en c), il est raisonnable de supposer que les
      moyennes des deux populations sont identiques, soit $\mu_1 =
      \mu_2 = \mu$. On a que
      \begin{displaymath}
        F = \frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}
        \sim F(14, 19).
      \end{displaymath}
      On trouve dans une table de la loi $F$ (ou avec la fonction
      \texttt{qf} dans \textsf{R}) que
      \begin{displaymath}
        \Prob{0,417 < F < 2,256} = 0,90.
      \end{displaymath}
      Par conséquent,
      \begin{displaymath}
        \Prob{
          \frac{0,417 S_2^2}{ S_1^2}
          < \frac{\sigma_2^2}{\sigma_1^2} <
          \frac{2,256 S_2^2}{ S_1^2}
        } = 0,90
      \end{displaymath}
      et un intervalle de confiance à $90~\%$ pour
      $\sigma_2^2/\sigma_1^2$ est $(0,462, \, 2,502)$. La variance
      $\sigma_2^2$ n'est donc pas significativement plus grande que
      $\sigma_1^2$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire tiré d'une
  population normale de moyenne et variance inconnues. Développer la
  formule d'un estimateur par intervalle de niveau $1 - \alpha$ pour
  $\sigma$, l'écart type de la distribution normale.
  \begin{rep}
    $(\sqrt{(n-1) S^2/b}, \sqrt{(n-1) S^2/a})$.
  \end{rep}
  \begin{sol}
    On sait que
    \begin{displaymath}
      \frac{(n-1)S^2}{\sigma^2} \sim \chi^2(n-1).
    \end{displaymath}
    Ainsi, pour des constantes $a$ et $b$, $a \leq b$, on a
    \begin{equation*}
      \Prob{a \leq \frac{(n-1)S^2}{\sigma^2} \leq b} =
      \Prob{\sqrt{\frac{(n-1)S^2}{b}} \leq \sigma \leq \sqrt{\frac{(n-1)S^2}{a}}} =
      1 - \alpha.
    \end{equation*}
    Un estimateur par intervalle de $\sigma$ est donc $(\sqrt{(n-1)
      S^2/b}, \sqrt{(n-1) S^2/a})$, où $a$ et $b$ satisfont la relation
    $\prob{a \leq Y \leq b} = 1 - \alpha$, avec $Y \sim \chi^2(n - 1)$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, X_2, \dots, X_n$ un échantillon aléatoire d'une
  distribution normale de moyenne $\mu$ et de variance $\sigma^2 =
  25$. Déterminer la taille de l'échantillon nécessaire pour que la
  longueur de l'intervalle de confiance de niveau $0,90$ pour la
  moyenne ne dépasse pas $0,05$.
  \begin{rep}
    \nombre{108 241}
  \end{rep}
  \begin{sol}
    On sait que
    \begin{align*}
      \mu &\in \bar{X} \pm z_{0,05} \frac{\sigma}{\sqrt{n}} \\
      &\in \bar{X} \pm 1,645 \frac{5}{\sqrt{n}}.
    \end{align*}
    La longueur de l'intervalle de confiance est $2 (1,645) (5) /
    \sqrt{n} = 16,45/\sqrt{n}$. Si l'on souhaite que $16,45/\sqrt{n}
    \leq 0,05$, alors $n \geq \nombre{108 241}$.
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $S^2_n$ la variance échantillonnale d'un échantillon aléatoire de taille $n$ issu
  d'une distribution $\mathcal{N}(\mu, \sigma^2)$ où $\mu$ et $\sigma^2$ sont
  des paramètres inconnus. On sait que $Y = (n-1)S^2_n/\sigma^2 \sim
  \chi^2(n - 1)$. Soit $g(y)$ la fonction de densité de $Y$ et $G(y)$
  la fonction de répartition. Soit $a$ et $b$ des constantes telles
  que $((n-1) s^2/b, (n-1) s^2/a)$ est un intervalle de confiance de niveau $1
  - \alpha$ pour $\sigma^2$. La longueur de cet intervalle est donc $(n-1)
  s^2 (b - a)/(ab)$. Démontrer que la longueur de l'intervalle de
  confiance est minimale si $a$ et $b$ satisfont la condition $a^2
  g(a) = b^2 g(b)$. (\emph{Astuce}: minimiser la longueur de
  l'intervalle sous la contrainte que $G(b) - G(a) = 1 - \alpha$.)
  \begin{sol}
    On cherche à minimiser la longueur de l'intervalle de confiance
    $h(a, b) = (n-1) s^2 (b - a)/(ab)$ sous la contrainte que la
    probabilité dans cet intervalle est $1 - \alpha$, c'est-à-dire que
    $G(b) - G(a) = 1 - \alpha$. En utilisant la méthode des
    multiplicateurs de Lagrange, on pose
    \begin{displaymath}
      L(a, b, \lambda) = \frac{(n-1) s^2}{a} - \frac{(n-1) s^2}{b} +
      \lambda(G(b) - G(a) - 1 + \alpha).
    \end{displaymath}
    Les dérivées de cette fonction par rapport à chacune de ses
    variables sont:
    \begin{align*}
      \frac{\partial}{\partial a}\, L(a, b, \lambda)
      &= - \frac{(n-1) s^2}{a^2} + \lambda g(a) \\
      \frac{\partial}{\partial b}\, L(a, b, \lambda)
      &= - \frac{-(n-1) s^2}{b^2} + \lambda g(b) \\
      \frac{\partial}{\partial \lambda}\, L(a, b, \lambda)
      &= G(b) - G(a) - 1 + \alpha.
    \end{align*}
    En posant ces dérivées égales à zéro et en résolvant, on trouve
    que
    \begin{equation*}
      \frac{g(a)}{g(b)} = \frac{b^2}{a^2}
    \end{equation*}
    ou, de manière équivalente, que $b^2 g(b) = a^2 g(a)$.
  \end{sol}
\end{exercice}

\begin{exercice}
Le \emph{Scholastic Assessment Test} (SAT) est un test d'aptitudes standardisé largement utilisé dans les admissions aux universités américaines. Les résultats au test, qui avaient lentement diminués au fil des ans depuis son implantation, ont commencé à remonter. Initialement, un résultat de 500 était considéré dans la moyenne. En 2005, les résultats moyens étaient approximativement de 508 pour le test de langue et 520 pour le test de mathématiques. Un échantillon aléatoire de résultats de 20 étudiants d'une école secondaire a produit les moyennes et écart-types listés ci-dessous. On suppose que les résultats aux tests sont normalement distribués.

\begin{center}
\begin{tabular}{lcc}
& Langue & Mathématiques\\ \hline
Moyenne échantillonnale & 505 & 495\\
Écart-type échantillonnal & 55 & 70\\ \hline
\end{tabular}
\end{center}

\begin{enumerate}
\item Trouver un intervalle de confiance bilatéral à 90~\% pour la moyenne des résultats du test de langue des étudiants de l'école secondaire.
\item Est-ce que l'intervalle trouvé en a) inclut la valeur 508, la moyenne exacte du test de langue en 2005? Que peut-on en conclure?
\item Trouver un intervalle de confiance bilatéral à 90~\% pour la moyenne des résultats du test de mathématiques des étudiants de l'école secondaire. Est-ce que l'intervalle inclut la valeur 520, la moyenne du test de mathématiques en 2005? Que peut-on en conclure?
\item Peut-on utiliser la méthode discutée en classe pour un construire un intervalle de confiance bilatéral pour la différence entre les moyennes des deux tests (langue et mathématiques), en supposant que leur variance est égale? Expliquer.
\item Construire un intervalle de confiance bilatéral de niveau 90~\% pour la variance $\sigma^2$ des résultats au test de mathématiques. Comment peut-on construire un intervalle de confiance bilatéral à 90~\% pour $\sigma$? Expliquer.
\end{enumerate}

\begin{rep}
\begin{enumerate}
\item $(483.734, \, 526.266)$
\item Oui
\item $(467.935, \, 522.065)$
\item Non
\item $(55.575, \, 95.929)$
\end{enumerate}
\end{rep}

\begin{sol}
\begin{enumerate}
\item
L'intervalle de confiance bilatéral pour $\mu$ a la forme
$$
\left[ \bar X_n - t_{n-1,\alpha/2} \frac{S_n}{\sqrt{n}}, \bar X_n + t_{n-1,\alpha/2} \frac{S_n}{\sqrt{n}} \right].
$$
Sachant que $n=20$ et $\alpha =0,1$, le quantile approprié de la distribution de Student est
$$
t_{19, \, 0,05} = 1.729.
$$
Avec $\bar x_n = 505$ et $S_n = 55$, l'intervalle devient
\begin{align*}
\left[ 505 -  1.729 \, \frac{55}{\sqrt{20}} \, , 505 + 1.729 \, \frac{55}{\sqrt{20}} \right] = [ 483.734 ,526.266].
\end{align*}

\item Puisque l'intervalle inclut $508$, il n'y a pas de preuve à l'effet que la moyenne du test de langue a significativement changé depuis 2005, à un niveau de confiance $90$~\%.  

\item
L'intervalle a la même forme qu'en a) et le même quantile. Avec la moyenne échantillonnale et l'écart-type échantillonnal, l'intervalle peut être calculé comme suit
\begin{align*}
\left[ 495 -  1.729 \, \frac{70}{\sqrt{20}}\, ,495 + 1.729 \, \frac{70}{\sqrt{20}} \right] = [ 467.935, \,522.065].
\end{align*}
L'intervalle inclut $520$, il n'y a donc pas une différence significative dans la moyenne du test de mathématiques depuis 2005, à un niveau de confiance $90$~\%.

\item
Non, la méthode ne peut pas être utilisée car les échantillons ne sont pas indépendants, les tests ont été effectués par les 20 mêmes étudiants dans les deux cas.

\item
L'intervalle de confiance a la forme
$$
\left[\frac{n-1}{\chi^2_{n-1,\alpha/2}} \, S_n^2,\frac{n-1}{\chi^2_{n-1,1-\alpha/2}} \, S_n^2\right].
$$
Les quantiles appropriés d'une distribution khi-carrée avec $n-1=19$ degrés de liberté et $\alpha=0,1$ sont 
$$
\chi^2_{n-1,1-\alpha/2} = 10.117, \quad \chi^2_{n-1,\alpha/2} = 30.144.
$$
Avec la variance échantillonnale $S_n^2 = 70^2 = 4900$, on obtient
\begin{align*}
\left[\frac{19}{30.144}\times4900,\frac{19}{10.117}\times4900\right] = [3088.557,9202.321].
\end{align*}
Une possibilité d'intervalle de confiance bilatéral à $100 \times (1-\alpha)$~\% pour $\sigma$ est
$$
\left[\sqrt{\frac{n-1}{\chi^2_{n-1,\alpha/2}}} \, S_n,\sqrt{\frac{n-1}{\chi^2_{n-1,1-\alpha/2}}} \, S_n\right]
$$
car
\begin{multline*}
\Pr\left(\sqrt{\frac{n-1}{\chi^2_{n-1,\alpha/2}}} \, S_n \le \sigma \le \sqrt{\frac{n-1}{\chi^2_{n-1,1-\alpha/2}}} \, S_n \right)\\ = \Pr\left(\frac{n-1}{\chi^2_{n-1,\alpha/2}} \, S_n^2 \le \sigma^2 \le \frac{n-1}{\chi^2_{n-1,1-\alpha/2}} \, S_n^2 \right) = 1-\alpha.
\end{multline*}
L'intervalle devient
\begin{align*}
[\sqrt{3088.557},\sqrt{9202.321}] = [55.575,95.929].
\end{align*}
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Le cuivre solide produit par frittage (chauffage sans fusion) d'une poudre dans des conditions environnementales spécifiques est ensuite mesuré pour la porosité (la fraction volumique due aux vides) en laboratoire. Un échantillon de $n=4$ mesures de porosité indépendantes a une moyenne $\bar x_n = 0,22$ et une variance $s_n^2 = 0,001$. Un second laboratoire répète le même processus et obtient $m=5$ mesures de porosité indépendantes avec $\bar y_m = 0,17$ and $s_m^2 = 0,002$.
\begin{enumerate}
\item Énumérer les hypothèses nécessaires pour construire un intervalle de confiance bilatéral \emph{exact} pour la différence de moyennes entre les populations des deux laboratoires.
\item Construire un intervalle de confiance bilatéral \emph{exact} à $95$~\% pour la différence des moyennes des populations des deux laboratoires.
\item Est-ce que la différence entre les moyennes des deux laboratoires est significative?
\end{enumerate}

\begin{rep}
b) $(-0.01288, \,0.11288)$
\end{rep}

\begin{sol}
\begin{enumerate}
\item Les échantillons doivent être indépendants, normalement distribués et avoir la même variance $\sigma^2$.

\item L'estimation combinée de la variance est donnée par
$$
s^2 = \frac{(n-1) s_n^2 + (m-1)s_m^2}{n+m-2} = \frac{3\times 0,001 + 4 \times 0,002}{4+5-2} = 0.00157.
$$
Le quantile $97,5$\% de la loi de Student avec $n+m-2=7$ degrés de liberté est
$$
t_{0,025, \, 7} = 2.365.
$$
L'intervalle de confiance à 95~\% est donc donné par
\begin{multline*}
\left[\bar x_n - \bar y_m - t_{0,025, \, 7} s \sqrt{\frac{1}{n} + \frac{1}{m}} \, , \bar x_n - \bar y_m + t_{0,025, \, 7} s \sqrt{\frac{1}{n} + \frac{1}{m}}\right] \\ = \left[0,22 - 0,17 - t_{0,025, \, 7} s \sqrt{\frac{1}{4} + \frac{1}{5}}, 0,22 - 0,17 + t_{0,025, \, 7} s \sqrt{\frac{1}{4} + \frac{1}{5}}\right] \\= [-0.01288 \, ,0.11288].
\end{multline*}

\item Puisque $0$ est inclus dans l'intervalle de confiance bilatéral calculé en b), les moyennes ne semblent pas différer à un seuil de 5~\%.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Dans une étude sur la relation entre l'ordre de naissance et la réussite scolaire, un chercheur a trouvé que $126$ étudiants gradués sur un échantillon de $180$ étaient enfants ainés. Dans un échantillon de $100$ personnes non graduées d'âge et de situation socio-économique comparable, le nombre d'ainés étaient de $54$.
\begin{enumerate}
\item Construire un intervalle de confiance bilatéral approximatif à $90$~\% pour la différence entre les proportions d'ainés pour les deux populations desquelles proviennent les échantillons.
\item Basé sur l'intervalle en a), est-ce que la différence entre les proportions d'ainés des deux populations semble significative?
\item Si les chercheurs souhaitent questionner un nombre égal $n$ d'étudiants gradués et de personnes non graduées, quelle est la valeur minimale de $n$ requise pour estimer la différence entre les proportions avec une erreur inférieure ou égale à $\pm 0.05$ avec un niveau de confiance de $90$~\% ?
\end{enumerate}

\begin{rep}
\begin{enumerate}
\item $(0.06062, \, 0.25938)$
\item Non
\item $n \geq 542$
\end{enumerate}
\end{rep}

\begin{sol}
\begin{enumerate}
\item Les proportions échantillonnales sont 
$$
p_n = \frac{126}{180}=0,7, \quad q_m = \frac{54}{100}=0,54.
$$
Les tailles d'échantillons sont respectivement de $n=180$ et $m=100$. Puisque les deux sont assez grandes, une approximation d'un intervalle de confiance à $90$~\% peut être utilisée pour $p-q$. On trouve
$$
\left[ p_n - q_m - z_{0,05} \sqrt{\frac{p_n(1-p_n)}{n} + \frac{q_m(1-q_m)}{m}},  p_n - q_m + z_{0,05} \sqrt{\frac{p_n(1-p_n)}{n} + \frac{q_m(1-q_m)}{m}}\right].
$$
Le quantile $95$~\% d'une loi $\mathcal{N}(0,1)$ est
$$
z_{0,05} = 1.645.
$$
L'intervalle de confiance devient
\begin{multline*}
\left [0.16 - 1.645\sqrt{\frac{0.7 \times 0.3}{180} + \frac{0.54\times 0.46}{100}} \, , 0.16 + 1.645\sqrt{\frac{0.7 \times 0.3}{180} + \frac{0.54\times 0.46}{100}}\right] \\ = [0.06062,0.25938].
\end{multline*}

\item Puisque l'intervalle de confiance calculé en a) ne contient pas $0$, la proportion d'enfants ainés semble être significativement plus grande dans la population d'étudiants gradués au niveau de confiance $90$~\%.

\item En supposant $n=m$, la taille d'échantillon de chaque groupe peut être calculée en résolvant l'équation
$$
1.645\sqrt{\frac{p(1-p)}{n} + \frac{q(1-q)}{n}} = 0,05.
$$
On trouve
$$
n = \left(\frac{1.645}{0.05}\right)^2 \{ p(1-p)+ q(1-q)\}.
$$
Sachant que pour tout $p\in(0,1)$, $p(1-p) \le 1/4$, une estimation conservatrice des tailles d'échantillon requises est
$$
n = m=  \left(\frac{1.645}{0.05}\right)^2 \times \left(\frac{1}{4} + \frac{1}{4} \right) \approx 541.205.
$$
Ainsi, pour atteindre la précision requise, chaque groupe doit comprendre au moins $542$ personnes.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
Soit $X_1,\ldots,X_n$ un échantillon aléatoire d'une distribution exponentielle avec densité
$$
f(x)=\lambda e^{-\lambda x}, \quad x>0.
$$
\begin{enumerate}
\item Écrire la vraisemblance, la log-vraisemblance et l'EMV de $\lambda$.
\item Calculer l'information de Fisher $I(\lambda)$.
\item Si $m=100$ et $\bar x=105,2$, utiliser la distribution limite de l'EMV pour obtenir un intervalle de confiance approximatif à $95$\% pour $\lambda$.
\item Basé sur l'intervalle trouvé en c), obtenir un intervalle de confiance approximatif de niveau $95$~\% pour la probabilité $\Pr[X>300]$, où $X$ est indépendant de $X_1,\ldots,X_n$ et a la même distribution.
\end{enumerate}
\begin{rep}
c) $[0.00764,0.01137]\quad$ 
d) $[0.03302,0.10099]$
\end{rep}

\begin{sol}
\begin{enumerate}
\item La vraisemblance est
$$
L(\lambda)=\prod_{i=1}^n \lambda e^{-\lambda x_i} = \lambda^n e^{-\lambda\sum_{i=1}^n x_i}
$$
et la log-vraisemblance est
$$
\ell(\lambda)=n\ln\lambda-\lambda\sum_{i=1}^n x_i.
$$
Si on dérive par rapport à $\lambda$, on trouve
$$
\ell'(\lambda)=\frac{n}{\lambda}-\sum_{i=1}^n x_i
$$
et $\ell''(\lambda)=-n/\lambda^2<0$, donc l'estimateur du maximum de vraisemblance est
$$
\frac{n}{\hat\lambda}-\sum_{i=1}^n x_i=0 \quad \Rightarrow \hat\lambda=\frac{1}{\bar x_n}.
$$ 

\item On a $\ln f(X;\lambda) = \ln\lambda-\lambda X$ donc l'information de Fisher est
\begin{align*}
I(\lambda)&=\ex\left[-\frac{\partial^2}{\partial\lambda^2} \ln f(X;\lambda) \right]=\ex\left[-\frac{\partial^2}{\partial\lambda^2} \left(\ln\lambda-\lambda X \right)\right]=\ex\left[\frac{1}{\lambda^2} \right] =\frac{1}{\lambda^2}.
\end{align*}

\item La distribution limite de l'EMV est
$$
\frac{\hat\lambda_n-\lambda}{\sqrt{1/\{nI(\lambda)\}}}
$$
est asymptotiquement $\mathcal{N}(0,1)$. On peut estimer $nI(\lambda)$ au dénominateur par $nI(\hat\lambda)=\frac{n}{\hat\lambda^2}$, pour obtenir, quand $n\to\infty$,
$$
\frac{\hat\lambda-\lambda}{\hat\lambda/\sqrt{n}}
$$
est asymptotiquement $\mathcal{N}(0,1)$. D'une autre façon, on peut estimer $nI(\lambda)$ par
$$
-\left.\frac{\partial^2}{\partial\lambda^2} \ell(\lambda)\right|_{\lambda=\hat\lambda}=\frac{n}{\hat\lambda^2},
$$
ce qui revient à la même réponse. Sachant que $z_{2,5\%}=1.96$ et $\hat\lambda=1/\bar x=1/105,2$, un intervalle de confiance approximatif de niveau 95~\% pour $\lambda$ est
\begin{align*}
\left[\hat\lambda-z_{\alpha/2}\sqrt{\frac{\hat\lambda^2}{n}},\hat\lambda+z_{\alpha/2}\sqrt{\frac{\hat\lambda^2}{n}}\right]&=\left[\frac{1}{\bar x_n}-z_{\alpha/2}\frac{1}{\sqrt{n}\bar x_n}, \frac{1}{\bar x_n}+z_{\alpha/2}\frac{1}{\sqrt{n}\bar x_n}\right]\\
&=\left[\frac{1}{105.2}-\frac{1.96}{10\times105.2},\frac{1}{105.2}+\frac{1.96}{10\times105.2}\right]\\
&=\left[0.00764,0.01137\right].
\end{align*}

\item Le paramètre d'intérêt est $\theta=\Pr[X>300]=e^{-300\lambda}$. On a
\begin{align*}
\Pr\left[\frac{1}{\bar X_n}-z_{\alpha/2}\frac{1}{\sqrt{n}\bar X_n}\leq \lambda \leq \frac{1}{\bar X_n}+z_{\alpha/2}\frac{1}{\sqrt{n}\bar X_n}\right]\approx 0,95.
\end{align*}
Multiplier l'inégalité par $-300$ change le signe d'inégalité, alors que l'exponentielle est une fonction croissante, donc
\begin{align*}
\Pr\left[\exp\left\{-300\left(\frac{1}{\bar X_n}-z_{\alpha/2}\frac{1}{\sqrt{n}\bar X_n}\right)\right\}\geq e^{-300\lambda} \geq \exp\left\{-300\left(\frac{1}{\bar X_n}+z_{\alpha/2}\frac{1}{\sqrt{n}\bar X_n}\right)\right\}\right]\approx 0,95.
\end{align*}
Donc, un intervalle de confiance de niveau approximatif $95$~\% pour $\Pr[X>300]$ est
$$
\left[0.03302,0.10099\right].
$$
\end{enumerate}
\end{sol}
\end{exercice}


\Closesolutionfile{solutions}
\Closesolutionfile{reponses}
%
%\section*{Exercices proposés dans \cite{Wackerly:mathstat:7e:2008}}
%
%\begin{trivlist}
%\item 8.39, 8.41, 8.43, 8.47, 8.57, 8.59, 8.63, 8.70, 8.72, 8.75, 8.77
%\end{trivlist}


%%%
%%% Insérer les réponses
%%%
\input{reponses-intervalle}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_analyse_statistique"
%%% coding: utf-8-unix
%%% End:


\chapter{Tests}
\label{chap:tests}

\Opensolutionfile{reponses}[reponses-tests]
\Opensolutionfile{solutions}[solutions-tests]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:tests}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:tests}}

\end{Filesave}




%%%
%%% Début des exercices
%%%

\begin{exercice}
Soit une proportion $p$ inconnue d'objets défectueux dans une grande population d'objets. On souhaite tester les hypothèses suivantes:
\begin{align*}
        \mathcal{H}_0 &: p = 0,2 \\
        \mathcal{H}_1 &: p \ne 0,2.
\end{align*}
On suppose qu'un échantillon aléatoire de $n=20$ objets est tiré de la population. Soit $X$ le nombre d'objets défectueux dans l'échantillon aléatoire, on considère un test d'hypothèse qui rejette l'hypothèse nulle si $X \ge 7$ ou $X \le 1$. 
\begin{enumerate}
\item Est-ce que les hypothèses sont simples ou composites? Expliquer.
\item Quelle est la région critique du test?
\item Calculer la taille du test et la probabilité de faire une erreur de type I.
\item Calculer la valeur de la fonction de puissance $\Pi(p)$ aux points
$$
p\in\{0,\,\, 0,1,\,\, 0,2,\,\, 0,3,\,\, 0,4,\,\, 0,5,\,\, 0,6,\,\, 0,7,\,\, 0,8,\,\, 0,9,\,\, 1\}
$$ 
et tracer la fonction de puissance en utilisant R ou Excel. Quel est le lien entre la fonction de puissance et les probabilités d'erreur de type I et II?
\item Refaire (d) avec une taille d'échantillon de $n=50$ et $n=100$. Est-ce que le test semble bon?
\item Suggérer une modification au test pour qu'il demeure significatif avec une taille d'échantillon différente de $20$.
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item L'hypothèse nulle est simple étant donné que $\Theta_0 = \{ 0,2\}$ ne contient qu'une seule valeur. La contre-hypothèse est composite étant donné que $\Theta_1 = [0,\, 0,2) \cup (0,2,\, 1]$ contient plus d'une valeur de $\theta$.

\item Les mesures $X_1, \ldots , X_{20}$ sont des variables aléatoires Bernoulli mutuellement indépendantes. La région critique est donc donnée par
$$
\mathcal{C} = \left\{ (x_1,\dots, x_{20}) \in \{0,1\}^n  :  x_1 + \dots + x_n \in \{ 0,1\} \cup \{7,\dots,20 \}\right \}.
$$

\item Parce que l'hypothèse nulle est simple, la taille du test et la probabilité d'erreur de type I sont les mêmes.
$$
\alpha =\Pr(X  \le 1, X \ge 7  |  p = 0,2). 
$$
Sous l'hypothèse nulle, $X = X_1 + \cdots + X_{20}$ est une distribution binomiale de taille $n=20$ et probabilité $p=0,2$. Ainsi, $\alpha$ peut être calculé comme suit:
\begin{align*}
\Pr(X  \le 1, X \ge 7) &= 1 - \Pr(2 \le X \ge 6) \\
&= 1 - \sum_{x=2}^6 \Pr(X=x) \\
&= 1 - 0,8441322 \\
&= 0,1558678
\end{align*}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pbinom}\hlstd{(}\hlnum{1}\hlstd{,}\hlkwc{size}\hlstd{=}\hlnum{20}\hlstd{,}\hlkwc{prob}\hlstd{=}\hlnum{0.2}\hlstd{)} \hlopt{+} \hlnum{1}\hlopt{-}\hlkwd{pbinom}\hlstd{(}\hlnum{6}\hlstd{,}\hlkwc{size}\hlstd{=}\hlnum{20}\hlstd{,}\hlkwc{prob}\hlstd{=}\hlnum{0.2}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.1558678
\end{verbatim}
\end{kframe}
\end{knitrout}

\item La fonction de puissance à $p_1\in [0,1]$ arbitraire est donnée par
$$
\Pi(p_1) = \Pr(X  \le 1, X \ge 7  |  p =p_1)
$$
et peut être calculée par le fait que $X \sim Bin(n=20, p=p_1)$. On répète la même démarche qu'en (c) en changeant la probabilité $p$ de la loi Binomiale pour chacune des valeurs de $p_1$. Avec \textsf{R}, on trouve
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{p} \hlkwb{<-} \hlkwd{seq}\hlstd{(}\hlkwc{from}\hlstd{=}\hlnum{0}\hlstd{,}\hlkwc{to}\hlstd{=}\hlnum{1}\hlstd{,}\hlkwc{by}\hlstd{=}\hlnum{0.1}\hlstd{)}
\hlstd{power} \hlkwb{<-} \hlkwd{pbinom}\hlstd{(}\hlnum{1}\hlstd{,}\hlkwc{size}\hlstd{=}\hlnum{20}\hlstd{,}\hlkwc{prob}\hlstd{=p)} \hlopt{+} \hlnum{1}\hlopt{-}\hlkwd{pbinom}\hlstd{(}\hlnum{6}\hlstd{,}\hlkwc{size}\hlstd{=}\hlnum{20}\hlstd{,}\hlkwc{prob}\hlstd{=p)}
\hlstd{power}
\end{alltt}
\begin{verbatim}
##  [1] 1.0000000 0.3941331 0.1558678 0.3996274 0.7505134
##  [6] 0.9423609 0.9935345 0.9997390 0.9999982 1.0000000
## [11] 1.0000000
\end{verbatim}
\end{kframe}
\end{knitrout}
La courbe de la fonction de puissance peut être tracée avec les points $(\Pi(p_1), p_1)$. En \textsf{R}, elle peut être tracée comme suit:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(p,power,}\hlkwc{type}\hlstd{=}\hlstr{"b"}\hlstd{)}
\hlkwd{points}\hlstd{(p,power,}\hlkwc{pch}\hlstd{=}\hlnum{16}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-32-1} 

\end{knitrout}
La puissance à $p=0,2$ est égale à la probabilité d'erreur de type I $\alpha$, alors que pour $ p \ne 0,2$, la probabilité d'erreur de type II $\beta$ est donnée par $1 - \Pi(p)$.

\item La seule différence avec (d) est que maintenant $Y$ est une distribution binomiale de taille $n \in \{50,100\}$. La fonction de puissance à $$
p\in\{0,\,\, 0,1,\,\, 0,2,\,\, 0,3,\,\, 0,4,\,\, 0,5,\,\, 0,6,\,\, 0,7,\,\, 0,8,\,\, 0,9,\,\, 1\}
$$ 
pour $n=50$ et $n=100$ est calculée de la même façon qu'en (d) en changeant la taille $n$ de la loi Binomiale pour $50$ et $100$. En \textsf{R}, on trouve:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(power.50} \hlkwb{<-} \hlkwd{pbinom}\hlstd{(}\hlnum{1}\hlstd{,}\hlkwc{size}\hlstd{=}\hlnum{50}\hlstd{,}\hlkwc{prob}\hlstd{=p)} \hlopt{+} \hlnum{1}\hlopt{-}\hlkwd{pbinom}\hlstd{(}\hlnum{6}\hlstd{,}\hlkwc{size}\hlstd{=}\hlnum{50}\hlstd{,}\hlkwc{prob}\hlstd{=p))}
\end{alltt}
\begin{verbatim}
##  [1] 1.0000000 0.2635590 0.8967945 0.9975067 0.9999860
##  [6] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
## [11] 1.0000000
\end{verbatim}
\begin{alltt}
\hlstd{(power.100} \hlkwb{<-} \hlkwd{pbinom}\hlstd{(}\hlnum{1}\hlstd{,}\hlkwc{size}\hlstd{=}\hlnum{100}\hlstd{,}\hlkwc{prob}\hlstd{=p)} \hlopt{+} \hlnum{1}\hlopt{-}\hlkwd{pbinom}\hlstd{(}\hlnum{6}\hlstd{,}\hlkwc{size}\hlstd{=}\hlnum{100}\hlstd{,}\hlkwc{prob}\hlstd{=p))}
\end{alltt}
\begin{verbatim}
##  [1] 1.0000000 0.8831661 0.9999220 1.0000000 1.0000000
##  [6] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000
## [11] 1.0000000
\end{verbatim}
\end{kframe}
\end{knitrout}
De la même façon qu'en (d), le graphique ci-dessous trace les fonctions de puissance pour $n=5$ (en rouge) et $n=100$ (en bleu), avec la puissance pour $n=20$ (en noir).
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{plot}\hlstd{(p,power,}\hlkwc{type}\hlstd{=}\hlstr{"b"}\hlstd{)}
\hlkwd{points}\hlstd{(p,power.50,}\hlkwc{type}\hlstd{=}\hlstr{"b"}\hlstd{,}\hlkwc{col}\hlstd{=}\hlstr{"red"}\hlstd{)}
\hlkwd{points}\hlstd{(p,power.100,}\hlkwc{type}\hlstd{=}\hlstr{"b"}\hlstd{,}\hlkwc{col}\hlstd{=}\hlstr{"blue"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-34-1} 

\end{knitrout}
Bien que la puissance se comporte bien si $p \ne 0,2$ (i.e., elle s'approche de 1), l'erreur de type I est complètement inacceptable: elle est aussi grande que $0,999$ quand $n=100$. Le test n'est pas du tout utile pour ces tailles d'échantillon.

\item La région de rejet devra dépendre de $n$, puisque plus la taille d'échantillon augmente, plus d'objets défectueux seront présents sous $\mathcal{H}_0$, simplement car plus d'objets sont inspectés. Sous l'hypothèse nulle, le nombre total d'objets défectueux va être une distribution binomiale de taille $n$ avec probabilité $p=0,2$. Les valeurs critiques pourraient être choisies comme des quantiles de la distribution binomiale.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:tests:x1x2}
  Soit $X$ une variable aléatoire dont la fonction de densité de
  probabilité est
  \begin{displaymath}
    f(x; \theta) = \theta x^{\theta - 1}, \quad 0 < x < 1.
  \end{displaymath}
  On suppose que $\theta$ peut prendre exclusivement les valeurs
  $\theta = 1$ ou  $\theta = 2$.
  \begin{enumerate}
  \item Trouver une statistique exhaustive pour le paramètre $\theta$
    de cette distribution.
  \item On teste l'hypothèse $ \mathcal{H}_0: \theta = 1$ versus $ \mathcal{H}_1: \theta =
    2$ à partir d'un échantillon aléatoire $X_1, X_2$. Si la région
    critique est $C = \{(x_1, x_2); x_1 x_2 \geq 3/4\}$, calculer la
    probabilité de faire une erreur de type I ($\alpha$) et la
    probabilité de faire une erreur de type II ($\beta$).
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $\prod_{i = 1}^n X_i$
    \item $\alpha = 0,034$, $\beta = 0,886$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a
      \begin{align*}
        f(x_1, \dots, x_n;\theta) &= \prod_{i=1}^n f(x_i;\theta) \\
        &= \theta^n\prod_{i=1}^nx_i^{\theta-1} \\
        &= \left(
          \theta^n \prod_{i=1}^n x_i^\theta
        \right)
        \left(
          \prod_{i=1}^n x_i^{-1}
        \right) \\
        &= g(t(x_1, \dots, x_n); \theta) h(x_1, \dots, x_n),
      \end{align*}
      où
      \begin{align*}
        g(y; \theta) &= \theta^n y^n \\
        t(x_1, \dots, x_n) &= \prod_{i=1}^n x_i \\
        h(x_1, \dots, x_n) &= \prod_{i=1}^n x_i^{-1}.
      \end{align*}
      Ainsi, par le théorème de factorisation de Fisher--Neyman, $T(X_1, \dots, X_n) = \prod_{i = 1}^n X_i$ est une
      statistique exhaustive pour $\theta$.
    \item D'une part, l'erreur de type I consiste à rejeter
      l'hypothèse $ \mathcal{H}_0$ alors qu'elle est vraie. La probabilité de
      faire ce type d'erreur, notée $\alpha$, correspond donc à la probabilité que
      la statistique du test se retrouve dans la région critique
      lorsque l'hypothèse $ \mathcal{H}_0$ est vraie. On a donc
      \begin{align*}
        \alpha &= \Prob{X_1X_2 \geq \frac{3}{4}; \theta = 1} \\
        &= \iint_C f_{X_1 X_2}(x_1, x_2; 1)\, dx_2 dx_1,
      \end{align*}
      où $C = \{(x_1, x_2); x_1 x_2 \geq 3/4\}$. Or, 
      $$
      f_{X_1 X_2}(x_1,
      x_2; \theta) = f_{X_1}(x_1; \theta) f_{X_2}(x_2; \theta) =
      \theta^2 x_1^{\theta - 1} x_2^{\theta - 1},
      $$ 
      d'où $f_{X_1
        X_2}(x_1, x_2; 1) = 1$, $0 < x_1, x_2 < 1$. La région critique
      $C$ est représentée à la figure~\ref{fig:tests:x1x2}.
      \begin{figure}
        \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=0.6\textwidth]{figure/unnamed-chunk-35-1} 

\end{knitrout}
        \caption{Domaine de définition de la densité conjointe des
          variables $X_1$ et $X_2$ de
          l'exercice~\ref{chap:tests}.\ref{ex:tests:x1x2}. La zone
          hachurée est la région critique $C = \{(x, y); x y \geq
          3/4\}$ du test d'hypothèse.}
        \label{fig:tests:x1x2}
      \end{figure}
      Ainsi,
      \begin{align*}
        \alpha
        &= \int_{3/4}^1 \int_{3/(4 x_1)}^1 \,dx_2\,dx_1 \\
        &= \int_{3/4}^1
        \left(
          1 - \frac{3}{4x_1}
        \right)\, dx_1 \\
        &= \frac{1}{4} + \frac{3}{4} \ln \frac{3}{4} \\
        &\approx 0,034.
      \end{align*}

      D'autre part, l'erreur de type II consiste à ne pas rejeter
      l'hypothèse $ \mathcal{H}_0$ alors qu'elle est fausse. La valeur $\beta$
      est donc la probabilité que la statistique se retrouve à
      l'extérieur de la région critique lorsque l'hypothèse $ \mathcal{H}_0$ est
      fausse, c'est-à-dire
      \begin{align*}
        \beta &= \Prob{X_1 X_2 < \frac{3}{4}; \theta = 2} \\
        &= 1 - \Prob{X_1 X_2 \geq \frac{3}{4}; \theta = 2}\\
        &= 1 - \iint_C f_{X_1 X_2}(x_1, x_2; 2)\, dx_2 dx_1.
      \end{align*}
      Dès lors, puisque $f_{X_1 X_2}(x_1, x_2; 2) = 4 x_1 x_2$,
      \begin{align*}
        \beta
        &= 1 - \int_{3/4}^1 \int_{3/(4x_1)}^1 4 x_1 x_2\, dx_2\, dx_1\\
        &= \frac{9}{16} - \frac{9}{8}\ln \frac{3}{4} \\
        &\approx 0,886.
      \end{align*}
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice} 
Soit $X_1, \dots, X_n$ un échantillon aléatoire tiré d'une distribution Poisson avec paramètre $\lambda$ inconnu. Soit $\lambda_0$ et $\lambda_1$ des valeurs spécifiques telles que $0 < \lambda_0 < \lambda_1$ et on souhaite tester les hypothèses suivantes:
\begin{align*}
\mathcal{H}_0 \: : \: \lambda & = \lambda_0, \\
\mathcal{H}_1 \: : \: \lambda & = \lambda_1.
\end{align*}
[\emph{On sait que si $Z_1 \sim \mathcal{P}(\mu_1)$ est indépendante de $Z_2 \sim \mathcal{P}(\mu_2)$, alors $Z_1 +Z_2 \sim \mathcal{P}(\mu_1 + \mu_2)$.}]

\begin{enumerate}
\item Montrer que le test optimal au seuil $\alpha$ rejette $\mathcal{H}_0$ quand $\bar X_n > c$. Spécifier la valeur de $c$.

\item Montrer que le test $\delta$ qui minimise $\alpha(\delta) + \beta(\delta)$ rejette $\mathcal{H}_0$ quand $\bar X_n > c^\star$. Trouver la valeur de $c^\star$.

\item On suppose que $n=20$, $\lambda_0 = 1/20$ et $\lambda_1=1/10$. Calculer la valeur de la constante $c$ en a) quand $\alpha = 0,08$ et calculer les probabilités d'erreur de type I et II.

\item On suppose que $n=20$, $\lambda_0 = 1/20$ et $\lambda_1=1/10$. Calculer la valeur de $c^\star$ en b) et déterminer la valeur minimale que peut atteindre $\alpha(\delta) + \beta(\delta)$. Quelles sont les probabilités d'erreur de type I et II?
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item Ici, la fonction de distribution de l'échantillon est donnée par
$$
f(\boldsymbol{x}; \lambda) = e^{-\lambda n} \frac{\lambda^{x_1 + \dots + x_n}}{x_1! \times \dots \times x_n!}
$$
et le rapport de vraisemblance est
$$
\frac{f(\boldsymbol{x}; \lambda_1)}{f(\boldsymbol{x}; \lambda_0)} = e^{-\lambda_1 n + \lambda_0 n} \left(\frac{\lambda_1}{\lambda_0}\right)^{x_1 + \dots + x_n} = \exp\left\{ -(\lambda_1 - \lambda_0)n + n \bar x_n \ln\left(\frac{\lambda_1}{\lambda_0}\right) \right\}.
$$
Parce que les hypothèses sont simples, le Lemme de Neyman--Pearson dit que le test optimal rejette l'hypothèse nulle si pour une valeur critique $k>0$,
$$
\frac{f(\boldsymbol{x}; \lambda_1)}{f(\boldsymbol{x}; \lambda_0)} > \frac{1}{k}
$$
ce qui est équivalent à
$$ 
\exp\left\{ -(\lambda_1 - \lambda_0)n + n \bar x_n \ln\left(\frac{\lambda_1}{\lambda_0}\right) \right\} > \frac{1}{k}.
$$
Prendre le logarithme des deux côtés de l'inégalité donne
$$
\bar x_n > \underbrace{\frac{-\ln(k)/n + (\lambda_1 -\lambda_0)}{\ln(\lambda_1) - \ln(\lambda_0)}}_{=c}.
$$
Sous l'hypothèse nulle, on note que $n\bar X_n = X_1 + \dots + X_n$ est une distribution Poisson avec moyenne $n\lambda_0$. Ainsi, la valeur $c$ devrait être choisie telle que
$$
\Pr( n \bar X_n > nc  |  \lambda = \lambda_0) = \alpha
$$
pour un seuil $\alpha$, c'est-à-dire que $nc$ est le quantile $1-\alpha$ de la distribution Poisson avec moyenne $n\lambda_0$. À noter que le seuil de signification ne sera peut-être pas exact étant donné que la distribution Poisson est discrète.

\item Ici, le test optimal rejette l'hypothèse nulle si
$$
\frac{f(\boldsymbol{x}; \lambda_1)}{f(\boldsymbol{x}; \lambda_0)} = \exp\left\{ -(\lambda_1 - \lambda_0)n + n \bar x_n \ln\left(\frac{\lambda_1}{\lambda_0}\right) \right\} > 1,
$$
c'est-à-dire lorsque 
$$
\bar x_n > \underbrace{\frac{\lambda_1 -\lambda_0}{\ln(\lambda_1) - \ln(\lambda_0)}}_{=c^*}.
$$

\item Si $n=20$ et $\lambda_0 = 1/20$, $n\bar X_n$ est une loi de Poisson avec moyenne $1$ sous l'hypothèse nulle. Le quantile de niveau $1-0,08 = 0,92$ de cette distribution est $2$ puisque
\begin{center}
\begin{tabular}{lcc}
$x$ & $\Pr(n\bar X_n =x)$ & $\Pr(n\bar X_n \leq x)$ \\
$0$ & $0,368$ & $0,368$ \\
$1$ & $0,368$ & $0,736$ \\
$2$ & $0,184$ & $0,920$ \\
\end{tabular}
\end{center}
%<<>>=
%qpois(0.932,lambda=5)
%@
Parce que 
\begin{align*}
\Pr( n \bar X_n > nc  |  \lambda = \lambda_0) &= \alpha \\
1-\Pr( n \bar X_n \leq nc  |  \lambda = \lambda_0) &= \alpha 
\end{align*}
%<<>>=
%round(1- ppois(8,lambda=5),3)
%@
la valeur critique $c$ égale  $2/20 = 0,1$ et le test rejette $\mathcal{H}_0$ si $n \bar X_n > 2$. La probabilité d'erreur de type I est donc
$$
\Pr(n\bar X_n > 2  |  \lambda = 1/20) = 0,08.
$$
La probabilité d'erreur de type II est donnée par
$$
\Pr(n \bar X_n \le 2  |  \lambda =1/10) = 0,6766764
$$
et peut être calculée par le fait que quand $\lambda =1/10$, $n\bar X_n$ est une loi de Poisson avec moyenne $2$.

%<<>>=
%round(ppois(8,lambda=10),3)
%@

\item Ici, la valeur de $c^\star$ est donnée par
$$
\frac{\lambda_1 -\lambda_0}{\ln(\lambda_1) - \ln(\lambda_0)} = \frac{1/20}{\ln(1/10) - \ln(1/20)} = 0.07213,
$$
donc le test rejette l'hypothèse nulle si
$$
n\bar X_n > nc^* = 20\times 0.07213 = 1.4427,
$$
i.e. si $n \bar X_n > 1$. La probabilité d'erreur de type I est
$$
\Pr(n \bar X_n > 1  |  \lambda =1/20) = 0.264.
$$
La probabilité d'erreur de type II est
$$
\Pr(n \bar X_n \le 1  |  \lambda =1/10) = 0.406,
$$
et la valeur minimale que peut atteindre $\alpha(\delta) + \beta(\delta)$ est
$$
\alpha(\delta) + \beta(\delta) = 0.264 + 0.406 = 0.67.
$$
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  On suppose que la durée de vie d'un pneu en kilomètres a une
  distribution normale de moyenne $\nombre{30000}$ et d'écart type
  $\nombre{5000}$. Le fabricant du nouveau pneu \emph{Super Endurator
    X24} prétend que la durée de vie moyenne de ce pneu est bien
  supérieure à $\nombre{30000}$~km. Afin de vérifier les prétentions
  du fabricant, on testera $ \mathcal{H}_0: \mu \leq \nombre{30000}$ versus
  la contre-hypothèse $ \mathcal{H}_1: \mu > \nombre{30000}$ à partir de $n$
  observations indépendantes $x_1, \dots, x_n$. On rejettera $ \mathcal{H}_0$ si
  $\bar{x} \geq c$. Trouver les valeurs de $n$ et de $c$ de sorte que
  la probabilité de faire une erreur de type I en $\mu =
  \nombre{30000}$ est $0,01$ et que la probabilité de faire une erreur
  de type II en $\mu = \nombre{35000}$ est $0,02$.
  \begin{rep}
    $n = 19 \text{ ou } 20$, $c = \nombre{32658}$
  \end{rep}
  \begin{sol}
    On sait que $\bar{X} \sim \mathcal{N}(\mu, 5000^2/n)$. Or,
    \begin{align*}
      \alpha &= \prob{\bar{X} \geq c; \mu = \nombre{30 000}} \\
      &= 1 -
      \Phi\left(
        \frac{\sqrt{n}(c - \nombre{30000})}{\nombre{5000}}
      \right) \\
      \intertext{d'où}
      z_\alpha &=
      \frac{\sqrt{n}(c-\nombre{30000})}{\nombre{5000}}.
    \end{align*}
    De même,
    \begin{align*}
      \beta &= \prob{\bar{X} < c; \mu = \nombre{35000}}\\
      &= \Phi\left(
        \frac{\sqrt{n}(c - \nombre{35000})}{\nombre{5000}}
      \right), \\
      \intertext{d'où}
      z_{1 - \beta} &=
      \frac{\sqrt{n}(c-\nombre{35000})}{\nombre{5000}}.
    \end{align*}
    On trouve dans une table de la loi normale que $z_\alpha =
    z_{0,01} = 2,326$ et que $z_{1 - \beta} = z_{0,98} = -2,05$. En
    résolvant pour $n$ et $c$ le système à deux équations et deux
    inconnues, on obtient $n = 19,15$ et $c = \nombre{32658}$. Aux
    fins du test, on choisira donc une taille d'échantillon de $n =
    19$ ou $n = 20$.
  \end{sol}
\end{exercice}


\begin{exercice}
  \label{ex:tests:filles}
  On suppose que le poids en grammes des bébés à la naissance au Canada est distribué selon une loi normale de moyenne $\mu = \nombre{3315}$ et de variance $\sigma^2 = 525^2$, garçons et filles confondus. Soit $X$ le poids d'une fillette née au Québec. On suppose $X \sim \mathcal{N}(\mu_X, \sigma_X^2)$.
 \begin{enumerate}
 \item Donner l'expression de la statistique du test $ \mathcal{H}_0: \mu_X \le \nombre{3315}$ versus $ \mathcal{H}_1: \mu_X > \nombre{3315}$ (les bébés sont en moyenne plus gros au Québec) si $n = 11$ et $\alpha = 0,01$. (La valeur de $\sigma_X^2$ est inconnue ici.)
 \item Calculer la valeur de la statistique et tirer une conclusion  si l'échantillon de poids de 11 fillettes nées au Québec est le suivant:
    \begin{center}
      \begin{tabular}{llllll}
        \nombre{3119}, &
        \nombre{2657}, &
        \nombre{3459}, &
        \nombre{3629}, &
        \nombre{3345}, &
        \nombre{3629}, \\
        \nombre{3515}, &
        \nombre{3856}, &
        \nombre{3629}, &
        \nombre{3345}, &
        \nombre{3062}.
      \end{tabular}
    \end{center}
 %\item À quel niveau de confiance maximal rejette-t-on $ \mathcal{H}_0$?
 \item Énoncer la statistique du test et la région critique du test $ \mathcal{H}_0: \sigma_X^2 \ge 525^2$ versus $ \mathcal{H}_1: \sigma_X^2 < 525^2$ (moins de variation dans le poids des bébés nés au Québec) si $\alpha =  0,05$.
 \item Calculer la statistique du test à partir des données de la partie b). Quelle est la conclusion à tirer de ce résultat?
% \item Calculer le seuil observé du test sur la variance.
 \end{enumerate}

  \begin{rep}
    \begin{inparaenum}
      \stepcounter{enumi}
    \item $t = 0,699 < t_{10, \, 0,01}$
      \stepcounter{enumi}
    \item $y = 4,104 > \chi_{10, \,0,05}^2$
    \end{inparaenum}
  \end{rep}
  
 \begin{sol}
   \begin{enumerate}
   \item Il s'agit d'un simple test sur une moyenne. La statistique
      pour un petit échantillon est
      \begin{align*}
        T &= \frac{\bar{X} - \mu_X}{\sqrt{S^2/n}} = \frac{\bar{X} - \nombre{3315}}{\sqrt{S^2/11}} \sim t_{10}.
      \end{align*}
      On rejette $ \mathcal{H}_0$ si $t > t_{10, \,0,01} = 2,764$.
   \item On commence par calculer les statistiques $\bar{X}$ et $S_X^2$,
   $$
   \bar{X} = 3385,91 \quad \mbox{ et } \quad S_X^2 = \nombre{113108,49}.
   $$
   On trouve que $t = 0,699 < t_{10, \,0,01} = 2,764$, on ne rejette donc pas $ \mathcal{H}_0$ à un seuil de signification de $1$~\%.
 %\item Le niveau de confiance auquel on rejetterait $ \mathcal{H}_0$ est $1 - p = format(1 - tx$p.value, dig = 4, dec = ",")$. %$
 
    \item On a un test unilatéral à gauche sur une variance pour lequel la statistique est
      \begin{align*}
        Y = \frac{(n-1) S^2}{\sigma^2} = \frac{(10) S^2}{525^2} \sim \chi^2_{10}.
      \end{align*}
      On rejette $ \mathcal{H}_0$ si $y < \chi_{10, \, 0,95}^2 = 3,94$.
    \item Ici, $y = 4,104 > 3,94$. On ne rejette donc pas $ \mathcal{H}_0$.
%   \item On a $p = \prob{Y < 4,103}$, où $Y \sim \chi^2(10)$. Or,
%<<echo=TRUE>>=
%pchisq(4.103, 10)
%@
%d'où le seuil observé du test sur la variance est %
%   $format(pchisq(4.103, 10), dig = 3, dec = ",")$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $Y$ le poids en grammes d'un garçon né au Québec et
  supposons que $Y \sim \mathcal{N}(\mu_Y, \sigma_Y^2)$. On a les
  observations suivantes:
  \begin{center}
    \begin{tabular}{llllll}
      \nombre{4082}, &
      \nombre{3686}, &
      \nombre{4111}, &
      \nombre{3686}, &
      \nombre{3175}, &
      \nombre{4139}, \\
      \nombre{3686}, &
      \nombre{3430}, &
      \nombre{3289}, &
      \nombre{3657}, &
      \nombre{4082}.
    \end{tabular}
  \end{center}
  Refaire les questions de
  l'exercice~\ref{chap:tests}.\ref{ex:tests:filles}. Les réponses
  obtenues dans ces deux exercices suggèrent-elles d'autres hypothèses
  à explorer? Faire les tests appropriés le cas échéant.

  \begin{rep}
    \begin{inparaenum}
      \stepcounter{enumi}
    \item $t = 4,028 > t_{10, \,0,01}$
      \stepcounter{enumi}
    \item $y = 4,223 > \chi_{10, \, 0,95}^2$
    \item tests sur les moyennes et les variances
    \end{inparaenum}
  \end{rep}
  
 \begin{sol}
   \begin{enumerate}
    \item La statistique à utiliser est la même qu'à l'exercice~\ref{chap:tests}.\ref{ex:tests:filles}.
    \item On commence par calculer les statistiques $\bar{Y}$ et $S_Y^2$,
   $$
   \bar{Y} = 3729,36 \quad \mbox{ et } \quad S_Y^2 = \nombre{116388,85}.
   $$
   On trouve que $t = 4,028 > t_{10, \,0,01} = 2,764$, on rejette donc $ \mathcal{H}_0$ à un seuil de signification de $1$~\%.
    \item La statistique à utiliser est la même qu'à l'exercice~\ref{chap:tests}.\ref{ex:tests:filles}.
    \item On a $y = 4,223 > 3,94$. On ne rejette donc pas $ \mathcal{H}_0$.
    \item Les moyennes semblent être différentes entre les deux groupes, mais les variances égales. On commence par vérifier si le ratio des variances est près de $1$. On teste 
       \begin{align*}
         \mathcal{H}_0 &: \sigma_X^2 / \sigma_Y^2 = 1 \\
         \mathcal{H}_1 &: \sigma_X^2 / \sigma_Y^2 \ne 1
      \end{align*}
      La statistique à utiliser pour ce test est
      \begin{align*}
      F = \frac{S_Y^2 / \sigma_Y^2}{S_X^2 / \sigma_X^2} \sim \mathcal{F}_{m-1, n-1}
      \end{align*}
      Avec les données des deux numéros, on trouve que $f = 0,97$. À un seuil de signification de $\alpha = 10$\%, on trouve $\mathcal{F}_{10, 10, \, 0,05} = 2,98$ et $\mathcal{F}_{10, 10, \, 0,95} = 0,34$. On ne rejette donc pas $\mathcal{H}_0$ au seuil de $10$\%. Ainsi, on peut tester
      \begin{align*}
         \mathcal{H}_0 &: \mu_X = \mu_Y \\
         \mathcal{H}_1 &: \mu_X \ne \mu_Y
      \end{align*}
en supposant les variances égales. La statistique utilisée pour ce test est
\begin{align*}
W = \frac{(\bar{X} - \bar{Y}) - (\mu_X - \mu_Y)}{\sqrt{S_X^2/n+S_Y^2/m}} \sim t_{n+m-2}.
\end{align*}

Avec les données des deux numéros, on trouve que $w = -2,378$. À un seuil de signification de $\alpha = 5$\%, on trouve $t_{20, \, 0,025} = -2,086$ et $t_{20, \, 0,975} = 2,086$. On rejette donc $\mathcal{H}_0$ au seuil de $5$\%. On établit donc que les fillettes et les garçons nés au Québec ont un poids moyen différent les uns des autres.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Parmi les statistiques relevées par l'Organisation mondiale de
  la santé (OMS) on compte la concentration en $\mu g/m^3$ de
  particules en suspension dans l'air. Soit $X$ et $Y$ les
  concentrations en $\mu g/m^3$ de particules en suspension dans l'air
  aux centres-villes de Melbourne (Australie) et Houston (Texas),
  respectivement. À partir de $n = 13$ observations de la variable
  aléatoire $X$ et $m = 16$ observations de la variable aléatoire $Y$,
  on souhaite tester $ \mathcal{H}_0: \mu_X \ge \mu_Y$ versus $ \mathcal{H}_1: \mu_X < \mu_Y$.
  \begin{enumerate}
  \item Définir la statistique du test et la région critique en
    supposant égales les variances des distributions de $X$ et
    $Y$. Utiliser un seuil de signification de $5$~\%.
  \item Si $\bar{x} = 72,9$, $s_X = 25,6$, $\bar{y} = 81,7$ et $s_Y =
    28,3$, quelle est la conclusion pour ce test?
  \item Calculer la valeur $p$ de ce test. Est-elle conforme à la
    conclusion en b)?
  \item Tester si l'hypothèse de variances égales faite en a) est
    valide avec un niveau de confiance de $90$~\%.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $T = [(\bar{X} - \bar{Y}) - (\mu_X - \mu_Y)]/\sqrt{((n-1)S_X^2 +
            (m-1)S_Y^2) (n^{-1} + m^{-1})/(n + m - 2)}$, $T \sim
        t(n + m - 2)$
    \item $t = -0,838 > t_{0,05}(27)$
    \item $p = 0,2047$
    \item $f = 0,8311 < f_{0,05}(12, 15)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Nous avons un test unilatéral à gauche sur la différence entre deux moyennes. En supposant égales les variances des deux populations, on a $X \sim \mathcal{N}(\mu_X, \sigma^2)$ et $Y \sim  \mathcal{N}(\mu_Y, \sigma^2)$. On sait que $\bar{X} \sim \mathcal{N}(\mu_X, \sigma^2/n)$ et $\bar{Y} \sim \mathcal{N}(\mu_Y, \sigma^2/m)$, d'où un estimateur de $\mu_X - \mu_Y$ sur lequel baser un test est
      \begin{equation*}
        \bar{X} - \bar{Y} \sim
        \mathcal{N}\left(
          \mu_X - \mu_Y, \frac{\sigma^2}{n} + \frac{\sigma^2}{m} \right).
      \end{equation*}
      La variance $\sigma^2$ est toutefois inconnue. Un estimateur de ce paramètre est l'estimateur combiné, soit la moyenne pondérée des estimateurs de chaque échantillon:
      \begin{equation*}
      S_p^2 = \frac{(n - 1) S_X^2 + (m - 1) S_Y^2}{n + m - 2}.
      \end{equation*}
      Or, $(n + m - 2) S_p^2/\sigma^2 \sim \chi^2(n + m - 2)$. Par conséquent,
   \begin{align*}
          T &= \frac{\D \frac{(\bar{X} - \bar{Y}) - (\mu_X-\mu_Y)}{%
            \sigma \sqrt{n^{-1} + m^{-1}}}}{%
          \D \sqrt{\frac{(n + m - 2) S_p^2}{\sigma^2}}} \\[6pt]
        &= \frac{(\bar{X} - \bar{Y}) - (\mu_X-\mu_Y)}{%
          \D\sqrt{\frac{(n-1) S_X^2 + (m-1) S_Y^2}{n + m - 2}
            \left(
              \frac{1}{n} + \frac{1}{m}
            \right)}} \sim t(m + n - 2).
      \end{align*}
On rejette $ \mathcal{H}_0: \mu_X \ge \mu_Y$ en faveur de $ \mathcal{H}_1: \mu_X < \mu_y$ si $t \leq -t_{0,05}(n + m - 2)$.
  \item Avec les données de l'énoncé, la valeur de la statistique développée en a) est $t = -0,838$, alors que le 95{\ieme} centile d'une loi $t$ avec $13 + 16 - 2 = 27$ degrés de liberté est $t_{0,05}(27) = 1,703$. Puisque $\abs{t} < 1,703$, on ne rejette pas $ \mathcal{H}_0$.

\item On a $p = \prob{T < -0,838}$, où $T \sim t(27)$. À l'aide de \textsf{R}, on trouve
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pt}\hlstd{(}\hlopt{-}\hlnum{0.838}\hlstd{,} \hlkwc{df} \hlstd{=} \hlnum{27}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.204694
\end{verbatim}
\end{kframe}
\end{knitrout}
      Puisque
      $p = 0,2047 > 0,05$, on ne rejette pas $ \mathcal{H}_0$. La conclusion est évidemment la même qu'en a).

 \item On souhaite tester l'égalité de deux variances, c'est-à-dire $ \mathcal{H}_0: \sigma_X^2 = \sigma_Y^2$ versus $ \mathcal{H}_1: \sigma_X^2 \ne \sigma_Y^2$. Pour ce faire, on se base sur le fait que 
 $(n-1) S_X^2/\sigma_X^2 \sim \chi^2(n - 1)$ et que $(m-1) S_Y^2/\sigma_Y^2 \sim \chi^2(m - 1)$. Ainsi, sous $ \mathcal{H}_0$ (c'est-à-dire lorsque  $\sigma_X^2 = \sigma_Y^2$),
      \begin{displaymath}
        F = \frac{(n-1) S_X^2/(n - 1)}{(m-1) S_Y^2/(m - 1)} \sim F(n - 1, m - 1).
      \end{displaymath}
      On rejette $ \mathcal{H}_0$ si la valeur de la statistique est supérieure
      au $100(1 - \alpha/2)${\ieme} centile d'une loi $F$ avec $n - 1$
      et $m - 1$ degrés de liberté. Ici, on a $f = 0,8311$ et
      $f_{0,05}(12, 15) = 2,48$, et pour l'autre côté, la valeur de $f_{0,95}(12, 15)$ (ou de $f_{0,05}(15, 12)$) n'est pas donnée dans la table, mais on peut la trouver avec \textsf{R} comme suit:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{qf}\hlstd{(}\hlnum{0.05}\hlstd{,}\hlnum{12}\hlstd{,}\hlnum{15}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 0.3821387
\end{verbatim}
\end{kframe}
\end{knitrout}
     Puisque $0.38 < 0,8311 < 2,48$,  on ne rejette donc pas $ \mathcal{H}_0$.
    L'hypothèse des variances égales est donc raisonnable.
    \end{enumerate}
  \end{sol}
\end{exercice}


\begin{exercice}
  Un fabricant de dentifrice prétend que $75~\%$ de tous les dentistes
  recommandent son produit à ses patients. Sceptique, un groupe de
  protection des consommateurs décide de tester $ \mathcal{H}_0: \theta = 0,75$
  contre $ \mathcal{H}_1: \theta \ne 0,75$, où $\theta$ est la proportion de
  dentistes recommandant le dentifrice en question. Un sondage auprès
  de $390$ dentistes a révélé que $273$ d'entre eux recommandent
  effectivement ce dentifrice.
  \begin{enumerate}
  \item Quelle est la conclusion du test avec un seuil de signification
    de $\alpha = 0,05$?
  \item Quelle est la conclusion du test avec un seuil de signification
    de $\alpha = 0,01$?
  \item Quel est le seuil observé du test?
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $z = -2,28$
      \stepcounter{enumi}
    \item $0,0226$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Soit $X$ la variable aléatoire du nombre de dentistes qui
      recommande le dentifrice parmi un groupe-test de 390 dentistes.
      On a donc que $X \sim \text{Binomiale}(390, \theta)$ et on teste
      \begin{align*}
         \mathcal{H}_0 &: \theta = 0,75 \\
         \mathcal{H}_1 &: \theta \ne 0,75.
      \end{align*}
      Il s'agit d'un test bilatéral sur une proportion avec un grand
      échantillon. La statistique du test est
      \begin{displaymath}
        Z = \frac{\hat{\theta} - 0,75}{%
          \sqrt{0,75 (1 - 0,75)/390}}
      \end{displaymath}
      et on rejette $ \mathcal{H}_0$ si $\abs{z} > z_{\alpha/2}$. Ici, on a
      $\alpha = 0,05$, $\hat{\theta} = 273/390$, d'où $z = -2,28$.
      Puisque $\abs{z} > z_{0,025} = 1,96$, on juge, à un seuil de
      signification de 5~\%, que la proportion de dentistes qui
      recommandent le dentifrice est suffisamment éloignée de la
      prétention du fabricant pour rejetter l'hypothèse $ \mathcal{H}_0$.

%      On peut vérifier les résultats ci-dessus avec la fonction
%      \texttt{prop.test} de \textsf{R}:
%<<echo=TRUE>>=
%prop.test(273, n = 390, p = 0.75, correct = FALSE)
%@
%      On constate que la valeur test $\theta = 0,75$ ne se trouve pas
%      dans l'intervalle de confiance à 95~\%, d'où le rejet de
%      l'hypothèse $ \mathcal{H}_0$.
    \item Puisque $z_{0,005} = 2,576 > 2,28$, on ne rejette pas
      l'hypothèse  $ \mathcal{H}_0$ à un seuil de signification de 1~\%.
%      D'ailleurs un intervalle de confiance à 99~\% nous est fourni
%      par la fonction \texttt{prop.test}:
%<<echo=TRUE>>=
%prop.test(273, n = 390, p = 0.75, conf.level = 0.99,
%          correct = FALSE)
%@
%      On constate que cet intervalle comprend la valeur $\theta =
%      0,75$.
    \item Le seuil observé est le plus grand seuil de signification auquel on
      rejette $ \mathcal{H}_0$. Ainsi,
      \begin{equation*}
        p = 2 \prob{Z > 2,28} \approx 0,0226.
      \end{equation*}
      On rejette donc $ \mathcal{H}_0$ avec un niveau de confiance maximal de
      $97,74$~\%. %Cette valeur $p$ apparaît dans les résultats de la fonction \texttt{prop.test} en a) et b).
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $\theta$ la proportion de bonbons rouges dans une boîte de
  Smarties. On prétend que $\theta = 0,20$.
  \begin{enumerate}
  \item Définir la statistique de test et la région critique avec un
    seuil de signification de $5~\%$ pour le test
    \begin{align*}
        \mathcal{H}_0 &: \theta = 0,2 \\
        \mathcal{H}_1 &: \theta \ne 0,2.
	\end{align*}
  \item Pour faire le test, les $20$ membres de la section locale des
    Amateurs de Smarties Associés (ASA) ont chacun compté le nombres
    de bonbons rouges dans une boîte de $50$ grammes de Smarties. Ils
    ont obtenu les proportions suivantes:
    \begin{gather*}
      \frac{8}{56},  \quad
      \frac{13}{55}, \quad
      \frac{12}{58}, \quad
      \frac{13}{56}, \quad
      \frac{14}{57}, \quad
      \frac{5}{54},  \quad
      \frac{14}{56}, \quad
      \frac{15}{57}, \quad
      \frac{11}{54}, \quad
      \frac{13}{55}, \\[12pt]
      \frac{10}{57}, \quad
      \frac{8}{59},  \quad
      \frac{10}{54}, \quad
      \frac{11}{55}, \quad
      \frac{12}{56}, \quad
      \frac{11}{57}, \quad
      \frac{6}{54},  \quad
      \frac{7}{58},  \quad
      \frac{12}{58}, \quad
      \frac{14}{58}.
    \end{gather*}
    Si chaque membre de l'ASA fait le test mentionné en a), quelle
    proportion des membres rejette l'hypothèse $ \mathcal{H}_0$?
  \item En supposant vraie l'hypothèse $ \mathcal{H}_0$, à quelle proportion de
    rejets de l'hypothèse $ \mathcal{H}_0$ peut-on s'attendre?
  \item Pour chacun des ratios en b) on peut construire un intervalle
    de confiance à $95~\%$ pour $\theta$. Quelle proportion de ces
    intervalles de confiance contiennent $\theta = 0,20$?
  \item Si les $20$ résultats en b) sont agrégés de sorte que l'on a
    compté un total de $219$ bonbons rouges parmi $\nombre{1124}$ Smarties,
    rejette-t-on l'hypothèse $ \mathcal{H}_0$, toujours avec $\alpha = 0,05$?
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
      \item $Z = (\hat{\theta} - 0,20)/\sqrt{0,20 (1 - 0,20)/n}$
      \item $0$~\%
      \item $0,05$
      \item $100$~\%
      \item $p = 0,6654$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Il s'agit d'un test bilatéral sur une proportion avec un grand
      échantillon. La statistique du test est
      \begin{displaymath}
        Z = \frac{\hat{\theta} - 0,20}{%
          \sqrt{0,20 (1 - 0,20)/n}}
      \end{displaymath}
      et on rejette $ \mathcal{H}_0$ si $\abs{z} > z_{0,025} = 1,96$.
    \item En calculant la valeur de la statistique pour chacune des
      proportions données, on constate que toutes les statistiques
      sont plus inférieures à $1,96$. Auncun membre ne rejette donc
      l'hypothèse $ \mathcal{H}_0$.
    \item Étant donné que le seuil de signification est $5~\%$, si
      l'hypothèse $ \mathcal{H}_0$ est vraie, on peut s'attendre à un taux de
      rejet de $5$~\%.
    \item Si, en b), l'on n'a jamais rejeté l'hypothèse $ \mathcal{H}_0$, c'est
      que $100$~\% des intervalles de confiance à 95~\% pour $\theta$
      contiennent la valeur $0,20$.
    \item La valeur de la statistique est
      \begin{equation*}
        z = \frac{219/1124 - 0,20}{\sqrt{(0,20)(1 - 0,20)/1124}}
        = -0,4325
      \end{equation*}
      et $\abs{z} < 1,96$. Donc, on ne rejette pas $ \mathcal{H}_0$ à un seuil de
      signification de $5$~\%. La valeur $p$ associée est:
      \begin{align*}
        p &= \prob{\abs{Z} > -0,4325}\\
        &= 2 \prob{Z > 0,4325}\\
        &= 0,6654,
      \end{align*}
      ce qui représente le seuil de signification minimal auquel il
      est possible de rejeter $ \mathcal{H}_0$. %Ces résultats sont confirmés par
%      la fonction \texttt{prop.test} de \textsf{R}:
%<<echo=TRUE>>=
%prop.test(219, 1124, p = 0.2, correct = FALSE)
%@
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Lors d'un sondage mené auprès de $800$ adultes dont $605$
  non-fumeurs, on a posé la question suivante: \emph{Devrait-on introduire
  une nouvelle taxe sur le tabac pour aider à financer le système de
  santé au pays?} Soit $\theta_1$ et $\theta_2$ la proportion de
  non-fumeurs et de fumeurs, respectivement, qui ont répondu par
  l'affirmative à cette question. Les résultats du sondage sont les
  suivants: $x_1 = 351$ non-fumeurs ont répondu oui, contre $x_2 = 41$
  fumeurs.
  \begin{enumerate}
  \item Tester l'hypothèse nulle que $\theta_1 = \theta_2$ versus la contre-hypothèse $\theta_1 \ne \theta_2$ avec un seuil de signification de $5~\%$.
  \item Trouver un intervalle de confiance à $95~\%$ pour $\theta_1 -\theta_2$. Cet intervalle permet-il d'obtenir la même conclusion qu'en a)?
  \item Trouver un intervalle de confiance à $95~\%$ pour la proportion de la population totale en faveur de l'introduction d'une nouvelle taxe sur le tabac.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $z = 10,45 > 1,96$
    \item $(0,3005, \, 0,4393)$
    \item $(0,4555, \, 0,5246)$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Il s'agit d'un test sur la différence entre deux
      proportions. Il faut commencer par construire la statistique. On
      a
      \begin{align*}
        X &\sim \text{Binomiale}(n, \theta_1)\\
        Y &\sim \text{Binomiale}(m, \theta_2).
      \end{align*}
      Pour $n$ et $m$ grands, on a, approximativement,
      \begin{align*}
        X &\sim N(n\theta_1, n\theta_1(1 - \theta_1)) \\
        Y &\sim N(m\theta_2, m\theta_2(1 - \theta_2),
      \end{align*}
      et donc, toujours approximativement,
      \begin{align*}
        \hat{\theta}_1 &= \frac{X}{n}
        \sim N\left(\theta_1,
          \frac{\theta_1(1-\theta_1)}{n}\right) \\
        \hat{\theta}_2 &= \frac{Y}{m}
        \sim N\left(\theta_2, \frac{\theta_2(1-\theta_2)}{m}\right).
      \end{align*}
      Par conséquent,
      \begin{displaymath}
        \frac{(\hat{\theta}_1 - \hat{\theta}_2) - (\theta_1 -
          \theta_2)}{\sqrt{\theta_1(1-\theta_1)/n +
            \theta_2(1-\theta_2)/m}} \sim N(0, 1).
      \end{displaymath}
      Pour pouvoir calculer la valeur de cette statistique pour un
      échantillon aléatoire, on remplace $\theta_1$ et $\theta_2$ dans
      le radical par $\hat{\theta}_1 = X/n$ et $\hat{\theta}_2 = Y/m$,
      dans l'ordre. Un intervalle de confiance de niveau $1 - \alpha$
      pour $\theta_1 - \theta_2$ est donc
      \begin{displaymath}
        (\hat{\theta}_1 - \hat{\theta}_2) \pm
        z_{\alpha/2} \sqrt{\frac{\hat{\theta}_1 (1 - \hat{\theta}_1)}{n} +
          \frac{\hat{\theta}_2 (1 - \hat{\theta}_2)}{m}}.
      \end{displaymath}
      De manière similaire, la statistique utilisée pour tester la
      différence entre les deux proportions $\theta_1$ et $\theta_2$ est
      \begin{displaymath}
        Z =  \frac{(\hat{\theta}_1 - \hat{\theta}_2) - (\theta_1 -
          \theta_2)}{\sqrt{\hat{\theta}_1 (1 - \hat{\theta}_1)/n +
            \hat{\theta}_2 (1 - \hat{\theta}_2)/m}},
      \end{displaymath}
      et on rejette $ \mathcal{H}_0: \theta_1 = \theta_2$ en faveur de $ \mathcal{H}_1:
      \theta_1 \ne \theta_2$ si $\abs{z} > z_{\alpha/2}$.

      Ici, on a $x = 351$, $y = 41$, $n = 605$ et $m = 800 - 605 =
      195$. Ainsi, $\hat{\theta}_1 = 0,5802$, $\hat{\theta}_2 =
      0,2103$ et $\abs{z} = 10,44 > 1,96$. On rejette donc $ \mathcal{H}_0$ à un
     seuil de signification de $5$~\%. %La fonction \texttt{prop.test}
%      de \textsf{R} corrobore ces résultats:
%<<echo=TRUE>>=
%prop.test(c(351, 41), c(605, 195), correct = FALSE)
%@
    \item L'intervalle de confiance est
      \begin{displaymath}
        (\theta_1 - \theta_2) \in (0,3005, \, 0,4393).
      \end{displaymath}
      Comme $0$ n'appartient pas à cet intervalle, on rejette $ \mathcal{H}_0$.
    \item On cherche maintenant un intervalle de confiance pour la
      proportion de la population en faveur de l'introduction du taxe
      sur le tabac. On a une observation $x = 351 + 41 = 392$ d'une
      distribution Binomiale$(800, \theta)$, d'où $\hat{\theta} =
      392/800 = 0,49$. Un intervalle de confiance à 95~\% pour
      $\theta$ est
      \begin{align*}
        \theta &\in \hat{\theta} \pm 1,96 \sqrt{\frac{\hat{\theta} (1 -
            \hat{\theta})}{800}} \\
        &\in 0,49 \pm 1,96 \sqrt{\frac{0,49 (0,51)}{800}} \\
        &\in (0,4555, \, 0,5246).
      \end{align*}
      Vérification avec \textsf{R}:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{prop.test}\hlstd{(}\hlnum{351} \hlopt{+} \hlnum{41}\hlstd{,} \hlnum{800}\hlstd{,} \hlkwc{correct} \hlstd{=} \hlnum{FALSE}\hlstd{)}\hlopt{$}\hlstd{conf.int}
\end{alltt}
\begin{verbatim}
## [1] 0.4554900 0.5246056
## attr(,"conf.level")
## [1] 0.95
\end{verbatim}
\end{kframe}
\end{knitrout}
    \end{enumerate}
  \end{sol}
\end{exercice}





\begin{exercice}
Télécharger le fichier \texttt{contents.csv} sur le site de cours. Il contient les pertes de biens, dues à un incendie, de plus d'un million de couronnes danoises provenant de réclamations faites à la compagnie de réassurance Copenhagen Re entre 1980 et 1990. Les données peuvent être importées en \textsf{R} comme suit, après avoir changé l'environnement de travail de \textsf{R} au dossier dans lequel vous avez enregistré le fichier \texttt{contents.csv}:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ct} \hlkwb{<-} \hlkwd{read.csv}\hlstd{(}\hlstr{"contents.csv"}\hlstd{,}\hlkwc{header}\hlstd{=}\hlnum{TRUE}\hlstd{,}\hlkwc{row.names}\hlstd{=}\hlnum{1}\hlstd{)}
\hlkwd{attach}\hlstd{(ct)}
\hlstd{data} \hlkwb{<-} \hlstd{Contents}
\end{alltt}
\end{kframe}
\end{knitrout}

Les données peuvent aussi être importées par \textsf{RStudio} en cliquant sur ``Import Dataset''.
On se rappelle également que la distribution exponentielle est un cas spécial de la distribution Gamma quand le paramètre $\alpha$ égal $1$.
\begin{enumerate}
\item Ajuster la distribution Gamma aux données et construire un intervalle de confiance à $95$\% pour $\alpha$. Est-ce qu'il contient la valeur $\alpha=1$? Que peut-on en conclure?
\item Tester l'hypothèse que $\alpha=1$ à un niveau de $5$\% en utilisant le test de Wald. Comparer la conclusion avec celle en (a).
\item Tester l'hypothèse que le modèle exponentiel est une simplification adéquate du modèle Gamma en utilisant un test du rapport de vraisemblance au niveau $5$\%. Comparer la conclusion avec celle en (b).
\end{enumerate}

\begin{sol}
\begin{enumerate}
\item Premièrement, on ajuste le modèle Gamma aux données.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{library}\hlstd{(MASS)}
\hlstd{m3} \hlkwb{<-} \hlkwd{fitdistr}\hlstd{(data,}\hlkwc{densfun}\hlstd{=}\hlstr{"gamma"}\hlstd{,}\hlkwc{start}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{shape}\hlstd{=}\hlnum{0.5}\hlstd{,}\hlkwc{rate}\hlstd{=}\hlnum{0.2}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}
L'estimation du paramètre $\alpha$ et l'estimation de son écart-type sont
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m3}\hlopt{$}\hlstd{estimate[}\hlnum{1}\hlstd{]}
\end{alltt}
\begin{verbatim}
##     shape 
## 0.5199489
\end{verbatim}
\begin{alltt}
\hlstd{m3}\hlopt{$}\hlstd{sd[}\hlnum{1}\hlstd{]}
\end{alltt}
\begin{verbatim}
##      shape 
## 0.02679359
\end{verbatim}
\end{kframe}
\end{knitrout}
Le quantile $97,5$\% de la loi normale standard est
$$
z_{0,025} = 1.96.
$$
On trouve l'intervalle de confiance bilatéral approximatif pour $\alpha$ comme suit:
\begin{multline*}
[\hat \alpha - z_{0,025} \hat \sigma_{\hat \alpha}, \hat \alpha + z_{0,025} \hat \sigma_{\hat \alpha}] = [0.51995 - 1.96 \times 0.02679, 0.51995 + 1.96 \times 0.02679]\\
= [0.46743, 0.57246].
\end{multline*}
L'intervalle ne contient pas la valeur $\alpha=1$. Il y a donc évidence, au seuil $5$\%, que $\alpha \ne 1$, c'est-à-dire que la distribution exponentielle n'est pas une bonne simplification du modèle Gamma.

\item Les hypothèses de test sont:
$$
\mathcal{H}_0  :  \alpha = 1, \mathcal{H}_1 :  \alpha \ne 1
$$
et la statistique de Wald est donnée par
$$
w_n = \frac{\hat \alpha - 1}{\hat \sigma_{\hat \alpha}}  = \frac{0.51995-1}{0.02679} = -17.91664.
$$
Clairement, $w_n$ est plus petite que 
$$
- z_{0,025} = -1.96.
$$ 
Ainsi, $\mathcal{H}_0$ est rejetée à un niveau de confiance $5$\%. La conclusion est la même qu'en (a).

\item Pour calculer la statistique de test, on ajuste d'abord le modèle exponentiel aux données:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m2} \hlkwb{<-} \hlkwd{fitdistr}\hlstd{(data,}\hlkwc{densfun}\hlstd{=}\hlstr{"exponential"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}
La log-vraisemblance des deux modèles est donnée par
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{m3}\hlopt{$}\hlstd{loglik}
\end{alltt}
\begin{verbatim}
## [1] -865.6958
\end{verbatim}
\begin{alltt}
\hlstd{m2}\hlopt{$}\hlstd{loglik}
\end{alltt}
\begin{verbatim}
## [1] -963.2785
\end{verbatim}
\end{kframe}
\end{knitrout}
ce qui donne la statistique du rapport de vraisemblance
$$
w= 2\{\ell(\hat \alpha,\hat \beta)-\ell(\hat \beta)\} = 2 (-865.6958 + 963.2785) = 195.1653.
$$
Clairement, l'hypothèse que $\alpha=1$ est rejetée parce que la valeur de la statistique du rapport de vraisemblance est beaucoup plus grande que le quantile $97,5$\% de la distribution khi-carrée avec $1$ degré de liberté.
$$
\chi^2_{1,0.025} = 5.024.
$$
La conclusion est donc la même qu'en b): le modèle exponentiel n'est pas une simplification adéquare du modèle Gamma pour l'ajustement des données.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
\label{ex:test:titanic}
Le tableau de contingence suivant montre le destin des passagers du Titanic en fonction de la classe de tarification du billet, représentant le statut socio-économique.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##         Class
## Survived 1st 2nd 3rd Crew
##      No  122 167 528  673
##      Yes 203 118 178  212
\end{verbatim}
\end{kframe}
\end{knitrout}
\begin{enumerate}
\item Peut-on conclure que la probabilité de survie au naufrage du Titanic était différente selon la classe tarifaire au seuil de 5~\%~? 
\item Avec \textsf{R}, calculer le seuil observé du test utilisé en a).
\item Avec \textsf{R}, visualiser les probabilités de survie estimées avec un diagramme en mosaïque.
\end{enumerate}
\begin{rep}
a) oui \quad b) $\ensuremath{5.0276183\times 10^{-41}}$
\end{rep}
\begin{sol}
\begin{enumerate}
\item L'hypothèse nulle est $\mathcal{H}_0 :$ les lignes et les colonnes sont indépendantes. On doit donc faire un test du $\chi^2$. On calcule les totaux des lignes:
\begin{align*}
r_1 &= 122+167+528+673 = 1490\\
r_2 &= 203+118+178+212 = 711
\end{align*}
 On calcule les totaux des colonnes:
\begin{align*}
c_1 &= 122+203 = 325\\
c_2 &= 167+118 = 285\\
c_3 &= 528+178 = 706\\
c_4 &= 673+212 = 885\\
\end{align*}
On a $n=2201$ et les nombre espérés dans les cellules sont calculés comme suit.
\begin{align*}
\widehat{\esp{n_{ij}}} &= r_ic_j/n\\
\widehat{\esp{n_{11}}} &= 1490*325/2201= 220.0136302.
\end{align*}
On trouve donc les compte espérés
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{verbatim}
##        [,1]   [,2]   [,3]   [,4]
## [1,] 220.01 192.94 477.94 599.11
## [2,] 104.99  92.06 228.06 285.89
\end{verbatim}
\end{kframe}
\end{knitrout}
La statistique du $\chi^2$ est
\begin{align*}
X^2 =&\sum_{i=1}^2\sum_{j=1}^4 \frac{\{n_{ij}-\widehat{\esp{n_{ij}}}\}^2}{\widehat{\esp{n_{ij}}}}\\
& = \frac{(122-220.01)^2}{220.01}+\cdots+\frac{(212-285.89)^2}{285.89}\\
&= 190.39
\end{align*}
Le nombre de degrés de liberté est $1\times 3 = 3$ et la valeur critique donnée dans la table est $\chi^2_{3,95\%} = 7.81473$. Puisque $190.39>7.81473$, on rejette l'hypothèse nulle au seuil de 5~\%. La probabilité de survie dépend de la classe tarifaire.

\item On trouve
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pchisq}\hlstd{(}\hlnum{190.39}\hlstd{,}\hlnum{3}\hlstd{,}\hlkwc{lower.tail}\hlstd{=}\hlnum{FALSE}\hlstd{)}
\end{alltt}
\begin{verbatim}
## [1] 5.027618e-41
\end{verbatim}
\end{kframe}
\end{knitrout}

\item On peut tracer le diagramme en mosaïque avec les commandes suivantes. Le résultat se trouve dans la Figure~\ref{fig:test:Titanic}.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{tableau} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlkwd{c}\hlstd{(}\hlnum{122}\hlstd{,}\hlnum{203}\hlstd{,}\hlnum{167}\hlstd{,}\hlnum{118}\hlstd{,}\hlnum{528}\hlstd{,}\hlnum{178}\hlstd{,}\hlnum{673}\hlstd{,}\hlnum{212}\hlstd{),}\hlkwc{nrow}\hlstd{=}\hlnum{2}\hlstd{,}
              \hlkwc{dimnames}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{Survived}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"No"}\hlstd{,}\hlstr{"Yes"}\hlstd{),}
                         \hlkwc{Class}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"1st"}\hlstd{,}\hlstr{"2nd"}\hlstd{,}\hlstr{"3rd"}\hlstd{,}\hlstr{"Crew"}\hlstd{)))}
\hlkwd{mosaicplot}\hlstd{(}\hlkwd{t}\hlstd{(tableau),} \hlkwc{color}\hlstd{=}\hlnum{TRUE}\hlstd{,} \hlkwc{main}\hlstd{=}\hlstr{""}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{figure}
        \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=0.6\textwidth]{figure/unnamed-chunk-48-1} 

\end{knitrout}
        \caption{Diagramme en mosaïque des survivants du Titanic par classe tarifaire de
          l'exercice
          \ref{chap:tests}.\ref{ex:test:titanic}.}
        \label{fig:test:Titanic}
      \end{figure}
      
\end{enumerate}
\end{sol}
\end{exercice}


%\begin{exercice}
%  On considère $W_S$, la statistique de Wilcoxon dans dans une
%  expérience comportant $40$ sujets divisés en deux groupes: un groupe
%  \emph{contrôle} de $23$ sujets et un groupe \emph{traitement} de
%  $17$ sujets. On considère le test
%  \begin{align*}
%     \mathcal{H}_0 &: \text{le traitement n'est pas efficace} \\
%     \mathcal{H}_1 &: \text{le traitement est efficace}.
%  \end{align*}
%  \begin{enumerate}
%  \item Calculer la plus petite valeur possible de $W_S$.
%  \item Calculer la plus grande valeur possible de $W_S$.
%  \item Calculer l'espérance de $W_S$ lorsque l'hypothèse nulle est vraie.
%  \end{enumerate}
%  \begin{rep}
%    \begin{inparaenum}
%    \item $153$
%    \item $544$
%    \item $348,5$
%    \end{inparaenum}
%  \end{rep}
%  \begin{sol}
%    \begin{enumerate}
%    \item La plus petite valeur de la statistique de Wilcoxon
%      s'obtiendra lorsque tous les sujets du groupe traitement
%      recevront les plus petits rangs, c'est-à-dire les rangs de $1$ à
%      $17$. La statistique est alors
%      \begin{displaymath}
%        W_S = \sum_{i=1}^{17} i = \frac{(17)(18)}{2} = 153.
%      \end{displaymath}
%    \item La plus grande valeur de la statistique de Wilcoxon
%      s'obtiendra lorsque tous les sujets du groupe traitement
%      recevront les plus grands rangs, c'est-à-dire les rangs de $27$
%      à $40$. La statistique est alors
%      \begin{displaymath}
%        W_S = \sum_{i=1}^{40} i - \sum_{i=1}^{23} i =
%        \frac{(40)(41)}{2} - \frac{(23)(24)}{2} = 544.
%      \end{displaymath}
%    \item Comme la distribution de la statistique de Wilcoxon sous
%      l'hypothèse nulle est symétrique, la moyenne est donc la valeur
%      centrale, c'est-à-dire
%      \begin{displaymath}
%        153 + \frac{544 - 153}{2} = 348,5.
%      \end{displaymath}
%    \end{enumerate}
%  \end{sol}
%\end{exercice}
%
%\begin{exercice}
%  On considère $W_S$, la statistique de Wilcoxon dans dans une
%  expérience comportant $7$ sujets divisés en deux groupes avec $4$
%  sujets dans le groupe \emph{traitement} et $3$ sujets dans le groupe
%  \emph{contrôle}.  On teste les hypothèses
%  \begin{align*}
%     \mathcal{H}_0 &: \text{le traitement n'est pas efficace} \\
%     \mathcal{H}_1 &: \text{le traitement est efficace}.
%  \end{align*}
%  On suppose qu'un traitement est efficace s'il fait augmenter la
%  valeur de la statistique.
%  \begin{enumerate}
%  \item On obtient pour les sujets du groupe \emph{traitement} les
%    rangs $\{1, 3, 5, 6\}$. Calculer la valeur de la statistique de
%    Wilcoxon.
%  \item Calculer le nombre de configurations possibles.
%  \item Calculer la valeur $p$ du test.
%  \end{enumerate}
%  \begin{rep}
%    \begin{inparaenum}
%    \item $15$
%    \item $35$
%    \item $24/35$
%    \end{inparaenum}
%  \end{rep}
%  \begin{sol}
%    \begin{enumerate}
%    \item Simplement $W_S = 1 + 3 + 5 + 6 = 15$.
%    \item Il y a $\binom{7}{4} = \binom{7}{3} = 35$ configurations
%      possibles.
%    \item Il faut trouver les 35 configurations possibles et calculer
%      les $35$ valeurs de $W_S$ correspondantes. On obtient:
%      \begin{inparablank}
%        \item 1 fois 10;
%        \item 1 fois 11;
%        \item 2 fois 12;
%        \item 3 fois 13;
%        \item 4 fois 14;
%        \item 4 fois 15;
%        \item 5 fois 16;
%        \item 4 fois 17;
%        \item 4 fois 18;
%        \item 3 fois 19;
%        \item 2 fois 20;
%        \item 1 fois 21 et
%        \item 1 fois 22.
%      \end{inparablank}
%      La valeur $p$ est donc
%      \begin{displaymath}
%        p = (3)\left(\frac{4}{35}\right) +
%        (1)\left(\frac{5}{35}\right) (1)\left(\frac{3}{35}\right) +
%        (1)\left(\frac{2}{35}\right) + (2)\left(\frac{1}{35}\right) = \frac{24}{35}.
%      \end{displaymath}
%  \end{enumerate}
%\end{sol}
%\end{exercice}


\Closesolutionfile{solutions}
\Closesolutionfile{reponses}


%\section*{Exercices proposés dans \cite{Wackerly:mathstat:7e:2008}}
%
%\begin{trivlist}
%\item 3.145--3.151, 3.153, 3.155, 3.158, 3.159, 3.161, 3.162, 3.163
%\end{trivlist}


%%%
%%% Insérer les réponses
%%%
\input{reponses-tests}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_analyse_statistique"
%%% coding: utf-8-unix
%%% End:


\appendix
\include{tables}

\include{solutions}

\nocite{Hogg:mathstat:6e:2005,
  Hogg:probstat:2001,
  Mood:mathstat:1974,
  Freund:mathstat:1992}
\bibliography{stat}

\cleardoublepage
\cleartoverso

\pagestyle{empty}

\bandeverso
\begin{textblock*}{71mm}(135mm, -50mm)
  \textblockcolor{}
  % \includegraphics{codebarre}
\end{textblock*}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
