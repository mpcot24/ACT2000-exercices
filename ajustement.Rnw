\chapter{Ajustement de modèles}
\label{chap:ajustement}

\Opensolutionfile{reponses}[reponses-ajustement]
\Opensolutionfile{solutions}[solutions-ajustement]

\begin{Filesave}{reponses}
\bigskip
\section*{Réponses}

\end{Filesave}

\begin{Filesave}{solutions}
\section*{Chapitre \ref{chap:ajustement}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:ajustement}}

\end{Filesave}

<<echo=FALSE>>=
options(width = 55)
@


%%%
%%% Début des exercices
%%%

% Méthode des moments

\begin{exercice}
  \label{ex:ponctuelle:emm}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire issu des
  distributions ci-dessous. Dans chaque cas, trouver l'estimateur du
  paramètre $\theta$ à l'aide de la méthode des moments.
  \begin{enumerate}
  \item $f(x; \theta) = \theta^x e^{-\theta}/x!$, $x = 0, 1, \dots$,
    $\theta > 0$.
  \item $f(x; \theta) = \theta x^{\theta - 1}$, $0 < x < 1$, $\theta >
    0$.
  \item $f(x; \theta) = \theta^{-1} e^{-x/\theta}$, $\theta > 0$.
  \item $f(x; \theta) = e^{-|x - \theta|}/2$, $-\infty < x <
    \infty$, $-\infty < \theta < \infty$.
  \item $f(x; \theta) = e^{-(x - \theta)}$, $x \geq \theta$, $\theta >
    0$.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\bar{X}$
    \item $\bar{X}/(1 - \bar{X})$
    \item $\bar{X}$
    \item $\bar{X}$
    \item $\bar{X} - 1$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a une une distribution de Poisson de paramètre $\theta$,
      d'où $\esp{X} = \theta$. L'estimateur des moments est donc
      $\hat{\theta} = \bar{X}$.
    \item La densité est celle d'une distribution bêta de paramètres
      $\theta$ et $1$. Ainsi, $\esp{X} = \theta/(\theta + 1)$ et en posant
      \begin{equation*}
        \frac{\theta}{\theta + 1} = \bar{X}
      \end{equation*}
      on trouve que l'estimateur des moments de $\theta$ est
      \begin{equation*}
        \hat{\theta} = \frac{\bar{X}}{1 - \bar{X}}.
      \end{equation*}
    \item On reconnaît la densité d'une distribution Gamma de
      paramètres $1$ et $\theta$. Ainsi, on sait que $\esp{X} =
      \theta$, d'où l'estimateur des moments est $\hat{\theta} =
      \bar{X}$.
    \item Cette densité est celle de la loi de Laplace. On remarque d'abord que 
    \begin{align*}
    -|x - \theta| &=
    \begin{cases}
    -x + \theta &\text{pour} \;  x \geq \theta \\
    x - \theta &\text{pour} \; x \leq \theta \\
    \end{cases}.
    \end{align*}
    Ainsi, on divise l'intégrale en deux selon les cas pour la valeur absolue:
      \begin{align*}
        \esp{X} &= \frac{1}{2}
        \left(
          \int_{-\infty}^\theta x e^{x-\theta}\, dx +
          \int_{\theta}^\infty xe^{-x + \theta}\, dx
        \right).
        \end{align*}
        On intègre ensuite par parties pour obtenir
        \begin{align*}
        \esp{X} &= \frac{1}{2} \left( \left[x e^{x - \theta}   \right]_{-\infty}^{\theta} -  \int_{-\infty}^\theta e^{x-\theta}\, dx 
        - \left[ x e^{-x + \theta} \right]_{\theta}^{\infty} +
        \int_{\theta}^\infty e^{-x + \theta}\, dx \right) \\
        &= \frac{1}{2}(2\theta)\\
        &= \theta.
      \end{align*}
      L'estimateur des moments de $\theta$ est donc $\hat{\theta} =
      \bar{X}$.
    \item On a la densité d'une exponentielle de paramètre 1
      translatée de $\theta$ vers la droite. Par conséquent,
      $\esp{X} = \theta + 1$, un résultat facile à vérifier en
      intégrant. En posant $\theta + 1 = \bar{X}$, on trouve
      facilement que $\hat{\theta} = \bar{X} - 1$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
\label{ex:moment:distln}
Si $Z\sim \mathcal{N}(\mu,\sigma^2)$ et $Y=e^{Z}$, alors $Y$ a une distribution log-normale avec paramètres $\mu$ et $\sigma^2$. La fonction de répartition est
$$
F(y)=\Phi\left(\frac{\ln(y)-\mu}{\sigma}\right), \quad y>0,
$$
où $\Phi$ est la fonction de répartition de la loi normale standard. 
\begin{enumerate}
\item Utiliser la fonction génératrice des moments de la loi normale pour montrer que les deux premiers moments de la loi log-normale sont
$$
\ex[Y]=e^{\mu+\sigma^2/2}\quad \mbox{ et } \ex[Y^2]=e^{2\mu+2\sigma^2}.
$$
\item Utiliser la méthode des moments pour construire un estimateur de $\mu$ et $\sigma^2$ si $Y_1, \ldots, Y_n$ forment un échantillon aléatoire d'une loi log-normale.

\item Montrer que les estimateurs trouvés en (b) sont convergents.
\end{enumerate}
\begin{rep}
b) $\hat \mu = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\}$ et $\hat\sigma^2 = \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\}$
\end{rep}
\begin{sol}
\begin{enumerate}
\item La fonction génératrice des moments de $Z\sim\mathcal{N}(\mu,\sigma^2)$ est
$$
M_{Z}(t)=\ex[e^{tZ}]=\exp(\mu t +\sigma^2 t^2/2).
$$
Ainsi,
\begin{align*}
\ex[Y]&= \ex[e^{Z}]=M_Z(1)=\exp(\mu +\sigma^2/2),\\
\ex[Y^2]&= \ex[e^{2Z}]=M_Z(2)=\exp(2\mu +\sigma^2 2^2/2).
\end{align*}

\item On doit résoudre les deux équations où $m_1=\bar Y_n$ est égal à $\ex[Y]$ et $m_2=\frac{1}{n}\sum_{i=1}^n Y_i^2$ est égal à $\ex[Y^2]$:
\begin{align}
\exp(\hat\mu +\hat\sigma^2/2)&=\bar Y_n \label{eq:ajust1}\\
\exp(2\hat\mu +2\hat\sigma^2 )&=\frac{1}{n}\sum_{i=1}^n Y_i^2.\label{eq:ajust2}
\end{align}
On note que
 \begin{align*}
 \exp(2\hat\mu +2\hat\sigma^2 )&=\left\{\exp(\hat\mu +\hat\sigma^2/2)\right\}^2\exp(\hat\sigma^2)\\
 &=\bar Y_n^2 \exp(\hat\sigma^2),\quad \mbox{ avec \eqref{eq:1}}.
 \end{align*}
 En remplaçant dans \eqref{eq:ajust2}, on obtient que
 $$
 \bar{Y}_n^2 \exp(\hat\sigma^2) = \frac{1}{n}\sum_{i=1}^n Y_i^2 
 $$
 est équivalent à 
 $$
 \hat\sigma^2 = \ln\left\{\frac{\sum_{i=1}^n Y_i^2/n}{\bar{Y}_n^2} \right\}.
 $$
 En utilisant \eqref{eq:ajust1}, $\hat\mu +\hat\sigma^2/2=\ln(\bar Y_n )$, donc
\begin{align*}
\hat\mu & = \ln(\bar Y_n )-\frac{1}{2}\ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\}\\
& = \ln(\bar Y_n )+\ln\left\{\frac{\bar Y_n}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\}\\
& = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\}.
\end{align*}
Ainsi, les estimateurs des moments sont
$$
\hat \mu = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\} \quad \mbox{ et }\quad \hat\sigma^2 = \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\}.
$$

\item Pour montrer la convergence, on note premièrement que, selon la Loi faible des grands nombres
\begin{align*}
\bar Y &\stackrel{P}{\rightarrow} \ex[Y]=e^{\mu+\sigma^2/2}\\
\frac{1}{n}\sum_{i=1}^n Y_i^2 &\stackrel{P}{\rightarrow} \ex[Y^2]=e^{2\mu+2\sigma^2}\\
\end{align*}
Puisque $\ln$, le carré et la racine carrée sont toutes des fonctions continues, on trouve que, quand $n\to\infty$,
\begin{align*}
\hat\mu = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\} &\stackrel{P}{\rightarrow} \ln\left\{\frac{ e^{2\mu+\sigma^2}}{\sqrt{e^{2\mu+2\sigma^2}}} \right\}=\ln\left\{ e^{2\mu+\sigma^2-\mu-\sigma^2} \right\}=\mu\\
\hat\sigma^2 = \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\} &\stackrel{P}{\rightarrow} \ln\left\{\frac{e^{2\mu+2\sigma^2}}{e^{2\mu+\sigma^2}} \right\}=\ln\left\{e^{2\mu+2\sigma^2-2\mu-\sigma^2} \right\}=\sigma^2.
\end{align*}
Ainsi, les estimateurs sont convergents.
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Considérer la distribution géométrique avec fonction de masse de
  probabilité
  \begin{displaymath}
    \prob{X = x} = \theta (1 - \theta)^x, \quad x = 0, 1, \dots.
  \end{displaymath}
  On a obtenu l'échantillon aléatoire suivant de cette distribution:
  \begin{center}
    5\; 7\; 4\; 11\; 0\; 9\; 1\; 1\; 3\; 2\; 1\; 0\; 6\; 0\; 1\; 1\;
    1\; 9\; 2\; 0.
  \end{center}
  Utiliser la méthode des moments pour obtenir une estimation
  ponctuelle du paramètre $\theta$.
  \begin{rep}
    $0,2381$.
  \end{rep}
  \begin{sol}
    Pour obtenir l'estimateur des moments de $\theta$, on pose
    \begin{equation*}
      \esp{X} = \frac{1 - \theta}{\theta} = \bar{X},
    \end{equation*}
    d'où
    \begin{equation*}
      \hat{\theta} = \frac{1}{\bar{X} + 1}.
    \end{equation*}
    La moyenne de l'échantillon est $\bar{x} = 64/20 = 3,2$. On a donc
    \begin{equation*}
      \hat{\theta} = \frac{1}{4,2} = 0,2381.
    \end{equation*}
  \end{sol}
\end{exercice}

\begin{exercice}
  On a un dé régulier avec $\theta$ faces numérotées de $1$ à
  $\theta$, où $\theta$ est un entier inconnu. Soit $X_1, \dots,
  X_n$ un échantillon aléatoire composé de $n$ lancers de dés indépendants.
  \begin{enumerate}
  \item Déterminer l'estimateur des moments de $\theta$.
  \item Calculer l'estimateur des moments de $\theta$ si $n = 4$ et
    $x_1 = x_2 = x_3 = 3$ et $x_4 = 12$. Interpréter le résultat.
  \end{enumerate}
  \begin{rep}
    \begin{inparaenum}
    \item $\hat{\theta} = 2 \bar{X} - 1$
    \item $9,5$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item Il s'agit ici de trouver l'estimateur des moments du
      paramètre $\theta$ d'une distribution uniforme discrète sur $1,
      2, \dots, \theta$, c'est-à-dire que $\prob{X = x} = 1/\theta$
      pour $x = 1, \dots, \theta$. En posant
      \begin{align*}
        \esp{X} = \frac{\theta + 1}{2} = \bar{X},
      \end{align*}
      on trouve facilement que l'estimateur des moments de $\theta$
      est $\hat{\theta} = 2 \bar{X} - 1$.
    \item Avec $x_1 = x_2 = x_3 = 3$ et $x_4 = 12$, on a $\hat{\theta}
      = 2 (3 + 3 + 3 + 12)/4 - 1 = 9,5$. Or, cet estimateur est
      absurde puisque, en ayant roulé le résultat 12, on sait qu'il y a au
      moins douze faces sur le dé! En d'autres termes, $9,5$ est
      une valeur de $\theta$ impossible. On constate que l'estimateur
      obtenu à l'aide de la méthode des moments n'est pas toujours un
      estimateur possible.
    \end{enumerate}
  \end{sol}
\end{exercice}


% Méthode des quantiles

\begin{exercice}
Soit $X_1, \dots, X_n$ un échantillon aléatoire tiré d'une distribution log-normale. Les 20e et 80e quantiles empiriques sont respectivement de $\hat{\pi}_{0,20} = 18,25$ et $\hat{\pi}_{0,80} = 35,8$. Estimer les paramètres de la distribution en utilisant la méthode des quantiles, puis utiliser ces estimations pour estimer la probabilité d'observer une valeur excédant 30.

\begin{rep}
$\hat{\mu}=3,241$, $\hat{\sigma}=0,4$ et $\widehat{\Pr(X>30)}= 0,3446$
\end{rep}

\begin{sol}
Les équations à résoudre sont
\begin{align*}
0,2 &= F(18,25) = \Phi\left(\frac{\ln(18,25)-\hat{\mu}}{\hat{\sigma}}\right), \\
0,8 &= F(35,8) = \Phi\left(\frac{\ln(35,8)-\hat{\mu}}{\hat{\sigma}}\right).
\end{align*}
Les 20e et 80e quantiles de la loi normale standard sont -0,842 et 0,842, respectivement. Les équations deviennent
\begin{align*}
-0,842 &= \frac{2,904-\hat{\mu}}{\hat{\sigma}}, \\
0,842 &= \frac{3,578-\hat{\mu}}{\hat{\sigma}}.
\end{align*}
Diviser la première équation par la deuxième donne
$$
-1 = \frac{2,904-\hat{\mu}}{3,578-\hat{\mu}}.
$$
La solution est $\hat{\mu}=3,241$ et $\hat{\sigma}=0,4$. La probabilité d'excéder 30 est estimée par
\begin{align*}
\widehat{\Pr(X>30)} &= 1-F(30; \hat\mu,\hat\sigma) = 1-\Phi\left(\frac{\ln(30)-3,241}{0,4}\right) = 1-\Phi(0,4) \\
&= 1-0,6554 = 0,3446.
\end{align*}
\end{sol}
\end{exercice}

\begin{exercice}
Un échantillon aléatoire de réclamations est tiré d'une distribution log-logistique avec fonction de répartition
$$
F(x)=\frac{(x/\theta)^\tau}{1+(x/\theta)^\tau}, \quad x>0, \, \theta >0, \, \tau >0.
$$
Dans l'échantillon, 80\% des réclamations excèdent 100 et 20\% des réclamations excèdent 400. Estimer les paramètres $\theta$ et $\tau$ par la méthode des quantiles.

\begin{rep}
$\hat{\tau}=2$ et $\hat{\theta}=200$
\end{rep}

\begin{sol}
Les équations à résoudre sont
\begin{align*}
0,2 &= F(100) = \frac{(100/\hat{\theta})^{\hat{\tau}}}{1+(100/\hat{\theta})^{\hat{\tau}}}, \\
0,8 &= F(400) = \frac{(400/\hat{\theta})^{\hat{\tau}}}{1+(400/\hat{\theta})^{\hat{\tau}}}.
\end{align*}
Avec la première équation, on obtient
$$
0,2=0,8(100/\hat{\theta})^{\hat{\tau}} \text{ ou encore } \hat{\theta}^{\hat{\tau}}=4(100)^{\hat{\tau}}. 
$$
En insérant le résultat dans la deuxième équation, on obtient
$$
0,8 = \frac{\frac{400^{\hat{\tau}}}{\hat{\theta}^{\hat{\tau}}}}{1+\frac{400^{\hat{\tau}}}{\hat{\theta}^{\hat{\tau}}}}
=\frac{\frac{(4 \times 100)^{\hat{\tau}}}{(4 \times (100)^{\hat{\tau}})}}{1+\frac{(4 \times 100)^{\hat{\tau}}}{(4 \times (100)^{\hat{\tau}})}}
= \frac{4^{\hat{\tau}-1}}{1+4^{\hat{\tau}-1}}.
$$
On résoud pour obtenir $\hat{\tau}=2$ et $\hat{\theta}=200$.
\end{sol}
\end{exercice}

\begin{exercice}
Les 20 pertes suivantes (en millions de dollars) ont été enregistrées sur une période d'un an:
\begin{center}
\begin{tabular}{cccccccccc}
1 & 1 & 1 & 1 & 1 & 2 & 2 & 3 & 3 & 4 \\
6 & 6 & 8 & 10 & 13 & 14 & 15 & 18 & 22 & 25
\end{tabular}
\end{center}
Déterminer le 75e quantile empirique par la méthode des quantiles empiriques lissés.

\begin{rep}
13,75
\end{rep}

\begin{sol}
On a besoin de la $0,75(21)=15,75$e plus petite observation. On obtient $0,25(13) + 0,75(14)= 13,75$.
\end{sol}
\end{exercice}

% Méthode du maximum de vraisemblance

\begin{exercice}
  \label{ex:ponctuelle:emv}
  Trouver l'estimateur du maximum de vraisemblance du paramètre
  $\theta$ de chacune des distributions de
  l'exercice~\ref{chap:ajustement}.\ref{ex:ponctuelle:emm}.
  \begin{rep}
    \begin{inparaenum}
    \item $\bar{X}$
    \item $-n/\ln(X_1 \cdots X_n)$
    \item $\bar{X}$
    \item $\text{med}(X_1, \dots, X_n)$
    \item $X_{(1)}$
    \end{inparaenum}
  \end{rep}
  \begin{sol}
    Dans tous les cas, la fonction de vraisemblance est $L(\theta) =
    \prod_{i = 1}^n f(x_i; \theta)$ et la fonction de
    log-vraisemblance est $l(\theta) = \ln L(\theta) = \sum_{i = 1}^n
    \ln f(x_i; \theta)$. L'estimateur du maximum de vraisemblance du
    paramètre $\theta$ est la solution de l'équation $l^\prime(\theta)
    = 0$.
    \begin{enumerate}
    \item On a
      \begin{align*}
        L(\theta) &= \frac{e^{-n\theta} \theta^{\sum_{i=1}^n x_i}}{%
          \prod_{i=1}^n x_i!}, \\
        l(\theta) &= -n \theta  + \sum_{i = 1}^n x_i \ln(\theta)
        - \sum_{i=1}^n \ln(x_i!) \\
        \intertext{et}
        l^\prime(\theta) &= -n + \frac{\sum_{i = 1}^n x_i}{\theta}.
      \end{align*}
      En résolvant l'équation $l^\prime(\theta) = 0$ pour $\theta$, on
      trouve que l'estimateur du maximum de vraisemblance est
      $\hat{\theta} = \bar{X}$.
    \item On a
      \begin{align*}
        L(\theta) &= \theta^n
        \left(
          \prod_{i = 1}^n x_i
        \right)^{\theta - 1}, \\
        l(\theta) &= n \ln(\theta) +
        (\theta - 1) \sum_{i = 1}^n \ln(x_i) \\
        \intertext{et}
        l^\prime(\theta) &= \frac{n}{\theta} +
        \sum_{i = 1}^n \ln(x_i).
      \end{align*}
      On trouve donc que
      \begin{equation*}
      \hat{\theta} = -\frac{n}{\sum_{i=1}^n \ln(X_i)} =
      -\frac{n}{\ln(X_1 \cdots X_n)}.
      \end{equation*}
    \item  On a
      \begin{align*}
        L(\theta) &= \theta^{-n} e^{-\sum_{i=1}^n x_i/\theta}, \\
        l(\theta) &= -n \ln(\theta) - \frac{\sum_{i=1}^n x_i}{\theta} \\
        \intertext{et}
        l^\prime(\theta) &= -\frac{n}{\theta} +
        \frac{\sum_{i=1}^n x_i}{\theta^2}.
      \end{align*}
      On obtient que $\hat{\theta} = \bar{X}$.
    \item On a
      \begin{equation*}
        L(\theta) = \left( \frac{1}{2} \right)^n
        e^{-\sum_{i = 1}^n \abs{x_i - \theta}}
      \end{equation*}
      La présence de la valeur absolue rend cette fonction non
      différentiable en $\theta$. On remarque que la fonction de
      vraisemblance sera maximisée lorsque l'expression $\sum_{i=1}^n
      \abs{x_i - \theta}$ sera minimisée. On établit donc que $\hat{\theta} = \text{med}(X_1, \dots, X_n)$, puisqu'on connaît le résultat suivant sur la médiane.
      
      En général, si $X$ est une variable aléatoire continue et $a$ est une constante, on peut trouver le minimum de
    \begin{align*}
      \esp{\abs{X - a}}
      &= \int_{-\infty}^\infty \abs{x - a} f(x)\,dx \\
      &= \int_{-\infty}^a (a - x)f(x)\, dx
      + \int_a^\infty (x - a)f(x)\,dx.
    \end{align*}
    Or,
    \begin{align*}
      \frac{d}{da} \esp{\abs{X - a}}
      &= \int_{-\infty}^a f(x)\, dx + \int_a^\infty f(x)\, dx \\
      &= F_X(a) - (1 - F_X(a)) \\
      &= 2F_X(a) - 1
    \end{align*}
    Par conséquent, le minimum est atteint au point $a$ tel que
    $2F_X(a) - 1 = 0$, soit $F_X(a) = 1/2$. Par définition, cette
    valeur est la médiane de $X$.

    \item On remarque que le support de la densité dépend du paramètre
      $\theta$. La vraisemblance est
      \begin{equation*}
        L(\theta) = e^{n\theta - \sum_{i=1}^n x_i}\prod_{i=1}^n\mathbf{1}(x_i>\theta)
      \end{equation*}
      S'il y a une indicatrice qui est 0, alors la vraisemblance sera 0, elles doivent donc toutes être égales à 1 simultanément, ce qui est équivalent à
      \begin{equation*}
        L(\theta) = e^{n\theta - \sum_{i=1}^n x_i}\mathbf{1}(\min(x_1,\ldots,x_n)>\theta).
      \end{equation*}
      La fonction $e^{n\theta - \sum_{i=1}^n x_i}$ est strictement
      croissante en fonction de $\theta$, ce qui indique de choisir
      une valeur de $\theta$ la plus grande possible. Par contre, on a
      la contrainte $min(x_1,\ldots,x_n)>\theta$, c'est-à-dire que $\theta$ doit
      être plus inférieur ou égal à la plus petite valeur de
      l'échantillon. Par conséquent, $\hat{\theta} = \min(X_1, \dots,
      X_n) = X_{(1)}$.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  \label{ex:ponctuelle:expon}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire de la distribution
  exponentielle translatée avec fonction de répartition
  \begin{align*}
    F(x; \mu, \lambda)
    &= 1 - e^{-\lambda (x - \mu)} \\
    \intertext{et densité}
    f(x; \mu, \lambda)
    &= \lambda e^{-\lambda (x - \mu)}, \quad x \geq \mu,
  \end{align*}
  où $-\infty < \mu < \infty$ et $\lambda > 0$.
  \begin{enumerate}
  \item Démontrer que la distribution exponentielle translatée est
    obtenue par la transformation $X = Z + \mu$, où $Z \sim
    \text{Exponentielle}(\lambda)$.
  \item Calculer l'espérance et la variance de cette distribution.
  \item Calculer les estimateurs du maximum de vraisemblance des
    paramètres $\mu$ et $\lambda$.
  \item Simuler 100 observations d'une loi Exponentielle translatée de
    paramètres $\mu = 1000$ et $\lambda = 0,001$ à l'aide de la
    fonction \texttt{rexp} de \textsf{R} et de la transformation en
    a). Calculer des estimations ponctuelles de $\mu$ et $\lambda$
    pour l'échantillon ainsi obtenu. Ces estimations sont-elles
    proches des vraies valeurs des échantillons? Répéter l'expérience
    plusieurs fois au besoin.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $\esp{X} = \mu + \lambda^{-1}$, $\var{X} = \lambda^{-2}$
    \item $\hat{\mu} = X_{(1)}$, $\hat{\lambda} = n/\sum_{i = 1}^n (X_i -
        X_{(1)})$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item On a $X = Z + \mu$ où $Z \sim
      \text{Exponentielle}(\lambda)$. Alors,
      \begin{align*}
        F_X(x) &= \prob{Z + \mu \leq x}\\
        &= F_Z(x - \mu) \\
        &= 1 - e^{-\lambda (x-\mu)}, \quad x > \mu \\
        \intertext{et}
        f_X(x) &= \lambda e^{-\lambda (x - \mu)}, \quad x > \mu.
      \end{align*}
    \item On a simplement
      \begin{align*}
        \Esp{X} &= \Esp{Z + \mu}\\
        &= \Esp{Z} + \mu\\
        &= \frac{1}{\lambda} + \mu\intertext{et}
        \Var{X} &= \Var{Z + \mu}\\
        &= \Var{Z}\\
        &= \frac{1}{\lambda^2}.
      \end{align*}
    \item On a
      \begin{align*}
        L(\mu,\lambda) &= \lambda^n e^{-\lambda\sum_{i=1}^n(x_i -\mu)}\prod_{i=1}^n\mathbf{1}(x_i\geq \mu)\\
        l(\mu,\lambda) &= n\ln(\lambda) -
        \lambda\sum_{i=1}^n (x_i - \mu) +\log(\mathbf{1}\{\min(x_1,\ldots,x_n)\geq \mu\}) \\
        \intertext{et}
        \frac{\partial l(\mu,\lambda)}{\partial \lambda} &=
        \frac{n}{\lambda} - \sum_{i=1}^n (x_i - \mu), \\
        \intertext{d'où}
        \hat{\lambda} &= \frac{n}{\sum_{i=1}^n (x_i - \hat{\mu})}.
      \end{align*}
      On voit dans la fonction de vraisemblance que la fonction $e^{-\lambda\sum_{i=1}^n(x_i -\mu)}$ est strictement
      croissante en fonction de $\mu$. Ainsi, il faut prendre la
      valeur de $\hat{\mu}$ la plus grande possible telle que $\log(\mathbf{1}\{\min(x_1,\ldots,x_n)\geq \hat{\mu}\})=0$. On a donc
      \begin{align*}
        \hat{\mu} &= x_{(1)} \\
        \hat{\lambda} &= \frac{n}{\sum_{i=1}^n (x_i - x_{(1)})}.
      \end{align*}
    \item On a, par exemple, les résultats suivants pour une
      exponentielle translatée de paramètres $\mu = \lambda^{-1} =
      \nombre{1000}$:
<<echo=TRUE>>=
x <- rexp(100, rate = 0.001) + 1000
min(x)
100 / sum(x - min(x))
@
      Les estimations obtenues sont près des vraies valeurs des
      paramètres, même pour un relativement petit échantillon de
      taille 100.
    \end{enumerate}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soient $X_{(1)} < \dots < X_{(n)}$ les statistiques d'ordre d'un
  échantillon aléatoire tiré d'une distribution uniforme sur
  l'intervalle $[\theta - \frac{1}{2}, \theta + \frac{1}{2}]$,
  $-\infty < \theta < \infty$. Démontrer que toute statistique $T(X_1,
  \dots, X_n)$ satisfaisant l'inégalité
  \begin{displaymath}
    X_{(n)} - \frac{1}{2} \leq T(X_1, \dots, X_n) \leq
    X_{(1)} + \frac{1}{2}
  \end{displaymath}
  est un estimateur du maximum de vraisemblance de $\theta$. Ceci est
  un exemple où l'estimateur du maximum de
  vraisemblance n'est pas unique.
  \begin{sol}
    Il est clair ici que, comme $f(x;\theta) = 1$, on ne pourra pas
    utiliser la technique habituelle pour calculer l'estimateur du
    maximum de vraisemblance. Il faut d'abord déterminer l'ensemble
    des valeurs de $\theta$ possibles selon l'échantillon obtenu.
    Comme toutes les données de l'échantillon doivent se trouver dans
    l'intervalle $[\theta - 1/2, \theta + 1/2]$, on a $\theta \geq
    X_{(n)} - 1/2$ et $\theta \leq X_{(1)} + 1/2$. De plus, puisque
    $X_{(n)} - X_{(1)} \leq 1$, on a que $X_{(n)} - 1/2 \leq \theta
    \leq X_{(1)} + 1/2$. Ainsi, toute statistique satisfaisant ces
    inégalités est un estimateur du maximum de vraisemblance de
    $\theta$. On a donc que
    \begin{displaymath}
      X_{(n)} - \frac{1}{2} \leq T(X_1, \dots, T_n) \leq X_{(1)} + \frac{1}{2}.
    \end{displaymath}
  \end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire issu de la
  distribution inverse gaussienne, dont la densité est
  \begin{displaymath}
    f(x; \mu,\lambda) =
    \left(
      \frac{\lambda}{2\pi x^3}
    \right)^{1/2}
    \exp\left\{
      -\frac{\lambda (x - \mu)^2}{2\mu^2 x}
    \right\}, \quad x > 0.
  \end{displaymath}
  Calculer les estimateurs du maximum de vraisemblance de $\mu$ et $\lambda$.
  \begin{rep}
    $\hat{\mu} = \bar{X}$, $\hat{\lambda} = n/\sum_{i = 1}^n
    (X_i^{-1} - \bar{X}^{-1})$
  \end{rep}
  \begin{sol}
    On a
    \begin{equation*}
      L(\mu ,\lambda) =
      \left(
        \frac{\lambda}{2\pi}
      \right)^{n/2}
      \left(
        \prod_{i = 1}^n \frac{1}{x_i^3}
      \right)^{1/2}
      \exp\left\{
        -\frac{\lambda}{2}
        \sum_{i=1}^n \frac{(x_i - \mu)^2}{\mu^2 x_i}
      \right\}.
    \end{equation*}
    Il est plus simple de trouver d'abord l'estimateur du maximum de
    vraisemblance du paramètre $\mu$. On constate qu'il s'agit de la
    valeur qui minimise la somme dans l'exponentielle. Or,
    \begin{align*}
      \frac{\partial}{\partial \mu}\,
      \sum_{i=1}^n \frac{(x_i - \mu)^2}{\mu^2 x_i} &= \sum_{i = 1}^{n} \left[ \frac{-2 (x_i - \mu)}{\mu^2 x_i} + \frac{(x_i - \mu)^2}{x_i} \frac{-2}{\mu^3} \right] \\
      &= \sum_{i = 1}^{n} \left[ \frac{-2 \mu(x_i - \mu) - 2 (x_i - \mu)^2}{\mu^3 x_i} \right] \\
      &= -2 \sum_{i = 1}^{n} \left[\frac{\mu x_i - \mu^2 + x_i^2 - 2 x_i \mu + \mu^2}{\mu^3 x_i} \right] \\ 
      &= -2 \sum_{i = 1}^{n} \left[\frac{x_i^2 - x_i \mu}{\mu^3 x_i} \right] \\
      &= - \sum_{i = 1}^{n} \frac{2}{\mu^2} \left[ \frac{x_i}{\mu} - 1 \right]
    \end{align*}
    En posant
    \begin{equation*}
      \sum_{i=1}^n \left( \frac{x_i}{\mu} - 1 \right) = 0,
    \end{equation*}
    on trouve que $\hat{\mu} = \bar{X}$. Pour trouver l'estimateur du
    maximum de vraisemblance de $\lambda$, on établit d'abord que
    \begin{displaymath}
      L(\hat{\mu}, \lambda) \propto \lambda^{n/2} e^{-\lambda H},
    \end{displaymath}
    où
    \begin{align*}
      H &= \sum_{i = 1}^n \frac{(x_i - \bar{x})^2}{2 \bar{x}^2 x_i} \\
      &= \frac{1}{2} \sum_{i = 1}^n
      \left(
        \frac{1}{x_i} - \frac{1}{\bar{x}}
      \right).
    \end{align*}
    On obtient donc
    \begin{align*}
      \hat{\lambda} &= \frac{n}{2H}\\
      &= \frac{n}{\sum_{i = 1}^n X_i^{-1} - \bar{X}^{-1}}.
    \end{align*}
  \end{sol}
\end{exercice}


\begin{exercice}
\label{ex:moment:pareto}
Soit $X_1, \ldots, X_n$ un échantillon aléatoire issu d'une distribution Pareto type II tel que pour tout $i \in \{ 1, \ldots , n\}$ et $x >0$, 
$$
F(x)=1-\left(\frac{\theta}{x+\theta}\right)^\alpha
\quad \mbox{ et } \quad 
f(x)=\frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}},
$$
avec paramètres inconnus $\alpha>0$ et $\theta>0$. 
\begin{enumerate}
\item Trouver $\ex[X_1]$ et $\vr(X_1)$.
\item On suppose $\alpha>2$. Construire des estimateurs pour les paramètres $\alpha$ et $\theta$ en utilisant la méthode des moments.
\item Écrire la vraisemblance, la log-vraisemblance et les deux équations qui doivent être résolues pour trouver les estimateurs du maximum de vraisemblance pour $\alpha$ et $\theta$.
\end{enumerate}
\begin{rep}
a) $\ex[X_1]= \theta/(\alpha-1)$ et $
\vr(X_1)=\alpha\theta^2/\{(\alpha-1)^2(\alpha-2)\}$ 

b) $\hat\alpha = 2S_n^2/(S_n^2-\bar X_n^2)$ et $\hat\theta=\bar X_n (S_n^2+\bar X_n^2)/(S_n^2-\bar X_n^2)$
\end{rep}
\begin{sol}
\begin{enumerate}
\item On a
\begin{align*}
\ex[X_1]&=\int_0^\infty x \frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}}\d x.
\end{align*}
On pose $t=x+\theta$,  
\begin{align*}
\ex[X_1] &= \int_\theta^\infty (t-\theta) \frac{\alpha\theta^\alpha}{t^{\alpha+1}}\mbox{d} t\\
&=\int_\theta^\infty \frac{\alpha\theta^\alpha}{t^{\alpha}}\d t-\int_\theta^\infty \frac{\alpha\theta^{\alpha+1}}{t^{\alpha+1}}\mbox{d} t\\
&= \left.\frac{-\alpha\theta^\alpha}{(\alpha-1)t^{\alpha-1}}\right|_\theta^\infty+ \left.\frac{\alpha\theta^{\alpha+1}}{\alpha t^{\alpha}}\right|_\theta^\infty, \quad \alpha>1\\
&=\frac{\alpha\theta}{\alpha-1}-\theta = \frac{\theta}{\alpha-1}.
\end{align*}

De même,
\begin{align*}
\ex[X_1^2]&=\int_0^\infty x^2 \frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}}\d x.
\end{align*}
On pose $t=x+\theta$,  
\begin{align*}
\ex[X_1^2]&=\int_\theta^\infty (t-\theta)^2 \frac{\alpha\theta^\alpha}{t^{\alpha+1}}\d t
\end{align*}
En intégrant par parties,
\begin{align*}
\ex[X_1^2]&= \left.\frac{-(t-\theta)^2\theta^\alpha}{t^{\alpha}}\right|_\theta^\infty+ \int_\theta^\infty 2(t-\theta)\frac{\theta^{\alpha}}{ t^{\alpha}}\d t\\
&=\left.\frac{-2(t-\theta)\theta^\alpha}{(\alpha-1)t^{\alpha-1}}\right|_\theta^\infty+ \int_\theta^\infty 2\frac{\theta^{\alpha}}{ (\alpha-1)t^{\alpha-1}}\d t\\
&=\left.\frac{-2\theta^\alpha}{(\alpha-1)(\alpha-2)t^{\alpha-2}}\right|_\theta^\infty, \quad \alpha>2\\
&=\frac{2\theta^\alpha}{(\alpha-1)(\alpha-2)\theta^{\alpha-2}}= \frac{2\theta^2}{(\alpha-1)(\alpha-2)}.
\end{align*}
Ainsi
$$
\vr(X_1)=\frac{2\theta^2}{(\alpha-1)(\alpha-2)}-\frac{\theta^2}{(\alpha-1)^2}=\frac{\alpha\theta^2}{(\alpha-1)^2(\alpha-2)}.
$$

\item On a $\bar X_n=\frac{1}{n}\sum_{i=1}^nX_i$ pour la moyenne échantillonnale et $S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X_n)^2$ pour la variance échantillonnale. Les estimateurs des moments sont tels que
\begin{align}
\bar X_n &= \frac{\hat\theta}{\hat\alpha-1} \label{eq:ajust3}\\
S_n^2 &= \frac{\hat\alpha\hat\theta^2}{(\hat\alpha-1)^2(\hat\alpha-2)}\label{eq:ajust4}
\end{align}
En utilisant \eqref{eq:ajust3}, on a $\hat\theta=\bar X_n (\hat\alpha -1)$. Aussi, on note que
\begin{align*}
S_n^2= \frac{\hat\alpha\hat\theta^2}{(\hat\alpha-1)^2(\hat\alpha-2)}&= \left(\frac{\hat\theta}{\hat\alpha-1}\right)^2\frac{\hat\alpha}{\hat\alpha-2}\\
& = \bar X_n^2\frac{\hat\alpha}{\hat\alpha-2}, \quad \mbox{ avec \eqref{eq:ajust3}}.
\end{align*}
On réarrange pour trouver
$$
\hat\alpha = \frac{2S_n^2}{S_n^2-\bar X_n^2}
$$
et
$$
\hat\theta=\bar X_n \left(\frac{2S_n^2}{S_n^2-\bar X_n^2} -1\right)=\bar X_n \frac{S_n^2+\bar X_n^2}{S_n^2-\bar X_n^2}.
$$

\item La vraisemblance est
$$
\mathcal{L}(\alpha,\theta)=\prod_{i=1}^n\frac{\alpha\theta^\alpha}{(x_i+\theta)^{\alpha+1}}=\frac{\alpha^n\theta^{n\alpha}}{\prod_{i=1}^n(x_i+\theta)^{\alpha+1}}
$$
et la log-vraisemblance est
\begin{align*}
\ell(\alpha,\theta)&=n\ln \alpha+n\alpha\ln \theta-\ln\left\{\prod_{i=1}^n(x_i+\theta)^{\alpha+1}\right\}\\
&=n\ln \alpha+n\alpha\ln \theta-(\alpha+1)\sum_{i=1}^n\ln(x_i+\theta).
\end{align*}
Les dérivées partielles sont
\begin{align*}
\frac{\partial\ell(\alpha,\theta)}{\partial \alpha}&=\frac{n}{ \alpha}+n\ln \theta-\sum_{i=1}^n\ln(x_i+\theta)\\
\frac{\partial\ell(\alpha,\theta)}{\partial \theta}&=\frac{n\alpha}{ \theta}-(\alpha+1)\sum_{i=1}^n\frac{1}{x_i+\theta}.
\end{align*}
Les estimateurs du maximum de vraisemblance sont tels que
\begin{align*}
\frac{n}{\hat\alpha}+n\ln \hat\theta-\sum_{i=1}^n\ln(x_i+\hat\theta)&=0\\
\frac{n\hat\alpha}{ \hat\theta}-(\hat\alpha+1)\sum_{i=1}^n\frac{1}{x_i+\hat\theta}&=0.
\end{align*}
\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire issu d'une loi
  uniforme sur l'intervalle $(a, b)$ où $a$ et $b$ sont des constantes
  inconnues. Calculer l'estimateur du maximum de vraisemblance de $a$
  et $b$.
  \begin{rep}
    $\hat{a} = \min(X_1, \dots, X_n)$ et $\hat{b} = \max(X_1, \dots,
    X_n)$
  \end{rep}
  \begin{sol}
    La fonction de vraisemblance est
    \begin{equation*}
      L(a, b) = \left( \frac{1}{b - a} \right)^n\mathbf{1}(a < x_1, \dots, x_n < b).
    \end{equation*}
    Pour maximiser cette fonction, il
    faut minimiser la quantité $b - a$ en choisissant une valeur de
    $b$ la plus petite possible et une valeur de $a$ la plus grande
    possible. Étant donné le support de la distribution, on choisit
    donc $\hat{a} = \min(X_1, \dots, X_n)$ et $\hat{b} = \max(X_1,
    \dots, X_n)$.
  \end{sol}
\end{exercice}

\begin{exercice}
On suppose que $X_1, \ldots, X_n$ forment un échantillon aléatoire d'une loi uniforme sur l'intervalle $(0, 2\theta +1)$ pour un paramètre inconnu $\theta > -1/2$.
\begin{enumerate}
\item Calculer l'estimateur du maximum de vraisemblance pour $\theta$.
\item Calculer l'EMV de $\vr (X)$, où $X$ suit une distribution $\mathcal{U}(0, 2\theta+1)$.
\end{enumerate}
\begin{rep}
a) $\{\max(X_1,\dots,X_n) -1\}/2$
b) $\{\max(X_1,\dots,X_n)\}^2/12$
\end{rep}

\begin{sol}
\begin{enumerate}
\item La fonction de vraisemblance de $\theta$ pour $x_1,\dots, x_n$ est 
\begin{align*}
L(\theta) &= \left(\frac{1}{2 \theta + 1}\right)^n \boldsymbol{1}(x_1 \le 2 \theta + 1) \times \dots \times\boldsymbol{1}(x_n \le 2 \theta + 1)\\
& = \left(\frac{1}{2 \theta + 1}\right)^n \boldsymbol{1}\{\max(x_1,\dots,x_n) \le 2 \theta + 1\}\\
& = \left(\frac{1}{2 \theta + 1}\right)^n \boldsymbol{1}\left\{  \theta \ge \frac{\max(x_1,\dots,x_n) -1}{2}\right\}.
\end{align*}
Puisque $\{1/(2\theta+1)\}^n$ est décroissante en $\theta$, $L(\theta)$ est maximisée à 
$$
\hat \theta_n = \frac{\max(x_1,\dots,x_n) -1}{2}
$$
L'EMV de $\theta$ est donc
$$
\hat \theta_n = \frac{\max(X_1,\dots,X_n) -1}{2}.
$$

\item La variance peut être calculée comme suit~:
$$
\ex(X) = \int_0^{2\theta  +1} x \, \frac{1}{2\theta + 1} \, \mbox{d} x = \frac{2 \theta  +1}{2} 
$$
et
$$
\ex(X^2) =  \int_0^{2\theta  +1} x^2 \, \frac{1}{2\theta + 1} \, \mbox{d}x = \frac{(2\theta + 1)^2}{3} \, .
$$
Par conséquent,
$$
\vr(X) = \ex(X^2) - \{\ex(X)\}^2 = \frac{(2\theta + 1)^2}{3}- \frac{(2\theta + 1)^2}{4} = \frac{(2\theta + 1)^2}{12} \, .
$$
Puisque l'application $g : (0,\infty) \to (0,\infty)$ donnée par
$$
g(x) = \frac{(2x + 1)^2}{12}
$$
est strictement croissante (et donc un-pour-un), la propriété d'invariance de l'EMV donne que l'EMV de $\vr(X)$ est
\begin{align*}
\frac{(2\hat\theta_n + 1)^2}{12} = \frac{\{\max(X_1,\dots,X_n)\}^2}{12} \, .
\end{align*}

\end{enumerate}
\end{sol}
\end{exercice}

\begin{exercice}
  Soit $X_1, \dots, X_n$ un échantillon aléatoire issu d'une
  distribution dont la loi de probabilité est
  \begin{displaymath}
    \prob{X = x} = \theta^x (1 - \theta)^{1 - x}, \quad x = 0, 1,
    \quad 0 \leq \theta \leq \frac{1}{2}.
  \end{displaymath}
  \begin{enumerate}
  \item Calculer les estimateurs du maximum de vraisemblance et des
    moments de $\theta$.
  \item Calculer l'erreur quadratique moyenne pour les estimateurs
    développés en a).
  \item Lequel des estimateurs obtenus en a) est le meilleur?
    Justifier.
  \end{enumerate}
  \begin{rep}
    \begin{enumerate}
    \item $\tilde{\theta} = \bar{X}$, $\hat{\theta} =
      \min(\bar{X}, 1/2)$
    \item EQM$(\tilde{\theta}) = \theta (1 - \theta)/n$,
      EQM$(\hat{\theta}) =
      \sum_{y = 0}^{[n/2]} (y/n - \theta)^2 \binom{n}{y} \theta^y (1 -
      \theta)^{n-y} + \sum_{y = [n/2] + 1}^n (1/2 - \theta)^2
      \binom{n}{y} \theta^y (1 - \theta)^{n - y}$
    \item EQM$(\hat{\theta}) \leq$ EQM$(\tilde{\theta})$
    \end{enumerate}
  \end{rep}
  \begin{sol}
    \begin{enumerate}
    \item La distribution de $X$ est une Bernoulli avec une
      restriction sur la valeur du paramètre $\theta$. On a donc que
      $\esp{X} = \theta$. L'estimateur des moments de $\theta$ est
      donc $\tilde{\theta} = \bar{X}$.

      Pour l'estimateur du maximum de vraisemblance, on a, en posant
      $y = \sum_{i = 1}^n x_i$,
      \begin{align*}
        L(\theta) &= \theta^{y} (1 - \theta)^{n - y}, \\
        l(\theta) &= y \ln(\theta) + (n - y) \ln(1 - \theta) \\
        \intertext{et}
        l^\prime(\theta) &= \frac{y}{\theta} - \frac{(n - y)}{1 - \theta}\\
        &= \frac{y - n\theta}{\theta (1 - \theta)}\\
        &= \frac{n \bar{x} - n \theta}{\theta (1 - \theta)}.
      \end{align*}
      Ainsi, la log-vraisemblance est croissante pour $\theta \leq
      \bar{x}$ et décroissante pour $\theta > \bar{x}$ (voir la
      figure~\ref{fig:ponctuelle:uniforme}). Le maximum est donc
      atteint en $\bar{x}$. Cependant, puisque $0 \leq \theta \leq
      1/2$ on doit avoir $\hat{\theta} \leq 1/2$. On a donc
      $\hat{\theta} = \min(\bar{X}, 1/2)$.
      \begin{figure}
        \centering
<<echo=FALSE, fig=TRUE, out.width = "0.6\\textwidth">>=
f <- function(x) 3 * log(x) + 7 * log(1 - x)
curve(f, from = 0, to = 0.5,
      xlab = expression(theta), ylab = expression(l(theta)))
segments(0.3, par("usr")[3], 0.3, f(0.3), lty = 2)
@
        \caption{Graphique de la fonction de log-vraisemblance de
          l'exercice
          \ref{chap:ajustement}.\ref{ex:ponctuelle:uniforme} pour $n =
          10$ et $y = 3$.}
        \label{fig:ponctuelle:uniforme}
      \end{figure}
    \item Premièrement, on remarque que $Y = \sum_{i = 1}^n X_i = n
      \bar{X} \sim \text{Binomiale}(n, \theta)$ avec $0 \leq \theta
      \leq 1/2$. Deuxièmement, on sait que $\mbox{EQM}(\hat{\theta}) =
      \var{\hat{\theta}} + b(\hat{\theta})^2$, où $\hat{\theta}$ est
      un estimateur quelconque d'un paramètre $\theta$ et
      $b(\hat{\theta}) = \esp{\hat{\theta}} - \theta$ est le biais de
      l'estimateur.

      Pour l'estimateur des moments $\tilde{\theta} = Y/n$, on a
      \begin{align*}
        \mbox{EQM}(\tilde{\theta}) &= \frac{\Var{Y}}{n^2} +
        b(Y/n)^2 \\
        &= \frac{\theta (1 - \theta)}{n},
      \end{align*}
      puique $\esp{Y/n} = \theta$.

      Pour l'estimateur du maximum de vraisemblance
      \begin{equation*}
        \hat{\theta} =
        \begin{cases}
          \frac{Y}{n}, & Y \leq \frac{n}{2} \\
          \frac{1}{2}, & Y > \frac{n}{2},
        \end{cases}
      \end{equation*}
      il est plus simple de développer l'erreur quadratique moyenne
      ainsi:
      \begin{align*}
        \mbox{EQM}(\hat{\theta}) &= \esp{(\hat{\theta} - \theta)^2} \\
        &= \sum_{y = 0}^n (\hat{\theta} - \theta)^2 \prob{Y = y} \\
        &= \sum_{y = 0}^{[n/2]}
        \left(
          \frac{y}{n} - \theta
        \right)^2 \binom{n}{y} \theta^y (1 - \theta)^{n - y} \\
        &\phantom{=} +
        \sum_{y = [n/2] + 1}^n
        \left(
          \frac{1}{2} - \theta
        \right)^2 \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
      \end{align*}
    \item On compare les erreurs quadratiques moyennes des deux
      estimateurs. Soit
      \begin{displaymath}
        \mbox{EQM}(\tilde{\theta}) = \sum_{y = 0}^n
        \left(
          \frac{y}{n} - \theta
        \right)^2 \binom{n}{y} \theta^y (1 - \theta)^{n - y},
      \end{displaymath}
      d'où
      \begin{align*}
        \mbox{EQM}(\tilde{\theta}) - \mbox{EQM}(\hat{\theta}) &=
        \sum_{y=[n/2] + 1}^n
        \left(
          \frac{y}{n} + \frac{1}{2} - 2 \theta
        \right)
        \left(
          \frac{y}{n} - \frac{1}{2}
        \right) \\
        &\phantom{=} \times
        \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
      \end{align*}
      Étant donné que $y/n > 1/2$ et que $\theta \leq 1/2$, tous les
      termes dans la somme sont positifs. On a donc que
      $\mbox{EQM}(\tilde{\theta}) - \mbox{EQM}(\hat{\theta}) > 0$ ou, de manière
      équivalente, $\mbox{EQM}(\hat{\theta}) < \mbox{EQM}(\tilde{\theta})$. En terme
      d'erreur quadratique moyenne, l'estimateur du maximum de
      vraisemblance est meilleur que l'estimateur des moments selon ce critère.
    \end{enumerate}
  \end{sol}
\end{exercice}

% Qualité de l'ajustement 

\begin{exercice}
Un assureur IARD américain souhaite modéliser les montants des sinistres pour une assurance automobile privée. Les données disponibles contiennent $n=6773$ réclamations, et $X_1,\ldots,X_n$ représentent les montants en dollars US. Les réclamations sont considérées indépendantes. Un sommaire et un histogramme des réclamations sont fournis ci-dessous. On a également
$$
\frac{1}{n}\sum_{i=1}^n x_i=1~853 \quad \mbox{ et }\quad \frac{1}{n}\sum_{i=1}^n x_i^2=10~438~832.
$$

<<message=FALSE,echo=FALSE>>=
library("CASdatasets")
@

<<echo=3,fig.width=6,fig.height=4>>=
data(usprivautoclaim)
attach(usprivautoclaim)
summary(PAID)
hist(PAID,breaks=c(seq(0,5000,2500),seq(10000,60000,10000)),freq=FALSE,main="Histogramme des montants de sinistres",xlab="Montants ($)")
@


\begin{enumerate}
\item En utilisant les informations fournies, argumenter qu'une loi normale ne devrait pas être utilisée pour modéliser les montants de réclamations. Proposer (au moins) une distribution qui semble appropriée et justifier.

\item On suppose que les montants de réclamations suivent une loi log-normale définie à l'exercice~\ref{chap:ajustement}.\ref{ex:moment:distln}. Trouver les estimateurs des moments des paramètres $\mu$ et $\sigma^2$.

\item On suppose que les montants de réclamations sont distribués selon une loi Pareto type II définie à l'exercice~\ref{chap:ajustement}.\ref{ex:moment:pareto}. Trouver les estimateurs des paramètres $\alpha$ et $\theta$ par la méthode des moments.

\item Les estimateurs du maximum de vraisemblance des distributions log-normale et Pareto peuvent être calculés par optimisation numérique en utilisant les estimateurs des moments trouvés en b) et c) comme valeurs de départ. Les résultats sont les suivants:
\begin{description}
\item[Log-normale]: Les estimateurs sont $\hat\mu_{MV}=6,95561$ et $\hat\sigma^2_{MV}=1,14698$ et la valeur de log-vraisemblance est $-57~185,11$.
\item[Pareto]: Les estimateurs sont $\hat\alpha_{MV}=4,71364$ et $\hat\theta_{MV}=6~819,891$ et la valeur de log-vraisemblance est $-57~500,12$.
\end{description}
Calculer l'AIC pour chacune des distributions. Quelle distribution semble être préférable selon ce critère?

\item Les diagrammes quantile-quantile de chacun des modèles sont présentés ci-dessous. Commenter sur l'ajustement des modèles. Quel modèle recommandez-vous à l'assureur? Pourquoi?

<<message=FALSE,echo=FALSE,fig.width=10,fig.height=5>>=
library(MASS)
library(actuar)
bla1 <- fitdistr(usprivautoclaim[,5],dlnorm,start=list(meanlog=mean(log(usprivautoclaim[,5])),sdlog=sd(log(usprivautoclaim[,5]))))
alpha <- 2*var(usprivautoclaim[,5])/(var(usprivautoclaim[,5])-mean(usprivautoclaim[,5])^2)
theta <- mean(usprivautoclaim[,5])*(alpha-1)

funtoop <- function(par)
    {
        shape <- exp(par[1])
        scale <- exp(par[2])
        -sum(dpareto(usprivautoclaim[,5],shape,scale,log=TRUE))
    }

bla3 <- optim(log(c(alpha,theta)),funtoop)
alpha2 <- exp(bla3$par[1])
theta2 <- exp(bla3$par[2])

nn <- length(usprivautoclaim[,5])
par(mfrow=c(1,2))
plot(qlnorm((1:nn)/(nn+1),bla1$estimate[1],bla1$estimate[2]),sort(usprivautoclaim[,5]),ylab="Quantiles empiriques",xlab="Quantiles th\u{E9}oriques de la log-normale")
abline(0,1)
plot(qpareto((1:nn)/(nn+1),alpha2,theta2),sort(usprivautoclaim[,5]),ylab="Quantiles empiriques",xlab="Quantiles th\u{E9}oriques de la Pareto",xlim=c(0,40000))
abline(0,1)
@

\item L'assureur s'intéresse à la queue à droite de la distribution et souhaite savoir les quantiles de niveau 99~\% et 99,5~\% pour les distributions des montants. Donner une estimation de ces quantiles sous les modèles log-normal et Pareto ajustés en d). [\emph{Astuce: Observer qu'il est possible d'inverser explicitement la fonction de répartition de la Pareto, et que les quantiles de la log-normale peuvent être exprimés en termes de quantiles de la loi normale.}]

\end{enumerate}
\begin{rep}
b) $\hat \mu =\Sexpr{round(log(1853^2/sqrt(10438832)),3)}$ et $\hat\sigma^2 =\Sexpr{round(log(10438832/1853^2),3)}$ 
c) $\hat\alpha =\Sexpr{round(2*7006257/(7006257-1853^2),3)}$ et 
$\hat\theta=\Sexpr{round(1853*(2*7006257/(7006257-1853^2)-1),2)}$
d) Log-normale~: $114~374,2$, Pareto~: $115~004,2$
f) 11~296,76,  14~166,68,  12~720,54  et  16~537,1
\end{rep}
\begin{sol}
\begin{enumerate}

\item La loi normale est définie sur les nombres réels, alors que les montants sont positifs seulement. Le domaine d'une loi normale n'est donc pas le même que le domaine des montants de réclamation. La moyenne échantillonnale et la médiane ne sont pas égales, ce qui serait le cas si le modèle normal était approprié. L'histogramme des montants ne ressemblent pas à une cloche symétrique. De bonnes options seraient les distributions Gamma, log-normale ou Pareto puisqu'elles sont toutes définies sur les réels positifs et sont asymétriques.

\item On a $\bar x_n=1~853$ et $m_2=\frac{1}{n}\sum_{i=1}^n x_i^2 = 10~438~832$. Selon l'exercice~\ref{chap:ajustement}.\ref{ex:moment:distln}~b), on a
\begin{align*}
\hat \mu &= \ln\left\{\frac{\bar x_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2}} \right\}=\ln\left\{\frac{1~853^2}{\sqrt{10~438~832}} \right\}=\Sexpr{round(log(1853^2/sqrt(10438832)),3)}\\
\hat\sigma^2 &= \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n x_i^2}{\bar x_n^2} \right\}=  \ln\left\{\frac{10~438~832}{1~853^2} \right\}=\Sexpr{round(log(10438832/1853^2),3)}.
\end{align*}


\item On a $\bar x_n=1~853$ et $m_2=\frac{1}{n}\sum_{i=1}^n x_i^2 = 10~438~832$, donc 
\begin{align*}
s_n^2&=\frac{1}{n-1}\sum_{i=1}^n x_i^2 - \frac{n}{n-1}\bar x_n^2\\
& = \frac{n}{n-1}(m_2-\bar x_n^2)\\
&=\frac{6~773}{6~772}(10~438~832-1~853^2)=7~006~257.
\end{align*}
Selon l'exercice~\ref{chap:ajustement}.\ref{ex:moment:pareto}~b), on trouve
\begin{align*}
\hat\alpha &= \frac{2s_n^2}{s_n^2-\bar x_n^2}=\Sexpr{round(2*7006257/(7006257-1853^2),3)}\\
\hat\theta&=\bar x_n \left(\frac{2s_n^2}{s_n^2-\bar x_n^2} -1\right)=\Sexpr{round(1853*(2*7006257/(7006257-1853^2)-1),2)}.
\end{align*}


\item L'AIC est donné par $2(-\ell(\hat\theta)+k)$, où $\ell(\hat\theta)$ est la valeur maximale de la vraisemblance et $k$ le nombre de paramètres dans le modèle. Il y a deux paramètres dans chaque modèle. Les AIC sont donc
\begin{description}
\item[Log-normale]: $2\times(57~185,11+2)=114~374,2$.
\item[Pareto]: $2\times(57~500,12+2)=115~004,2$
\end{description}
Selon le critère AIC, le meilleur modèle est celui avec la valeur la plus petite, la distribution log-normale est donc préférable.

\item L'ajustement des deux modèles n'est pas parfait parce que les points ne sont pas exactement alignés, plus spécialement pour de grandes valeurs de $x$. Les points sont au-dessus de la ligne dans le graphique Quantiles-Quantiles de la Pareto, ce qui signifie que la queue de la distribution empirique est plus épaisse que celle de la Pareto. Cela devrait inquiéter l'assureur puisque ça signifie qu'il sous-estimerait les réclamations. L'ajustement de la log-normale n'est pas parfait non plus, mais il y a moins de sous-estimation, car plusieurs points se retrouvent sous la ligne et les extrêmes sont moins éloignés. Par conséquent, on recommande l'utilisation du modèle log-normal.

<<message=FALSE,echo=FALSE,fig.width=10,fig.height=5>>=
library(MASS)
library(actuar)
bla1 <- fitdistr(usprivautoclaim[,5],dlnorm,start=list(meanlog=mean(log(usprivautoclaim[,5])),sdlog=sd(log(usprivautoclaim[,5]))))
alpha <- 2*var(usprivautoclaim[,5])/(var(usprivautoclaim[,5])-mean(usprivautoclaim[,5])^2)
theta <- mean(usprivautoclaim[,5])*(alpha-1)

funtoop <- function(par)
    {
        shape <- exp(par[1])
        scale <- exp(par[2])
        -sum(dpareto(usprivautoclaim[,5],shape,scale,log=TRUE))
    }

bla3 <- optim(log(c(alpha,theta)),funtoop)
alpha2 <- exp(bla3$par[1])
theta2 <- exp(bla3$par[2])

nn <- length(usprivautoclaim[,5])
par(mfrow=c(1,2))
plot(qlnorm((1:nn)/(nn+1),bla1$estimate[1],bla1$estimate[2]),sort(usprivautoclaim[,5]),ylab="Quantiles empiriques",xlab="Quantiles théoriques de la log-normale")
abline(0,1)
plot(qpareto((1:nn)/(nn+1),alpha2,theta2),sort(usprivautoclaim[,5]),ylab="Quantiles empiriques",xlab="Quantiles théoriques de la Pareto",xlim=c(0,40000))
abline(0,1)
@

\item Les quantiles de la loi Pareto sont l'inverse de sa fonction de répartition. Si $\omega_\kappa$ est le $(1-\kappa)$e quantile, c'est-à-dire qu'il est tel que $F(\omega_\kappa)=1-\kappa$, alors en utilisant la fonction de répartition fournie à l'exercice~\ref{chap:ajustement}.\ref{ex:moment:pareto}, on a
\begin{align*}
1-\left(\frac{\theta}{\omega_\kappa+\theta}\right)^\alpha &= 1-\kappa\\
\frac{\theta}{\omega_\kappa+\theta} &= \kappa^{1/\alpha}\\
\omega_\kappa= \frac{\theta}{\kappa^{1/\alpha}}-\theta.
\end{align*}

Une estimation du quantile 99~\% est
$$
\hat\omega_{1\%}= \frac{\hat\theta}{0,01^{1/\hat\alpha}}-\hat\theta
$$
et en utilisant l'EMV donné en (d), on a 
$$
\hat\omega_{1\%}= \frac{6~819,891}{0,01^{1/ 4,71364}}-6~819,891=11~296,76
$$
et une estimation du quantile 99,5\% est
$$
\hat\omega_{0,5\%}= \frac{6~819,891}{0,005^{1/4,71364}}-6~819,891= 14~166,68
$$

Si $\nu_\kappa$ représente le $(1-\kappa)$e quantile de la loi log-normale,
$$
F(\nu_\kappa)=1-\kappa \Rightarrow \Phi\left(\frac{\ln\nu_\kappa-\mu}{\sigma}\right)=1-\kappa,
$$
où $\Phi$ est la fonction de répartition de $\mathcal{N}(0,1)$. Cela signifique que le $(1-\kappa)$e quantile de la loi normale centrée réduite est utilisé pour trouver
$$
\frac{\ln\nu_\kappa-\mu}{\sigma}=z_\kappa \Rightarrow \nu_\kappa = \exp(\sigma z_\kappa+\mu).
$$
Une estimation du quantile 99~\% est
$$
\hat\nu_{1\%}= \exp(\hat\sigma z_{1\%}+\hat\mu),
$$
où $z_{1\%}=2,33$. En utilisant l'EMV donné en d), on a 
$$
\hat\nu_{1\%}= \exp(\sqrt{1,14698}\times 2,33+6,95561)=12~720,54
$$
et une estimation du quantile 99,5\% est
$$
\hat\nu_{0,5\%}= \exp(\sqrt{1,14698}\times 2,575+6,95561)=16~537,1
$$
Tel attendu en regardant les diagrammes quantile-quantile, les quantiles estimés par la distribution log-normale sont supérieurs à ceux estimés par la distribution Pareto.
\end{enumerate}

<<echo=FALSE>>=
detach(usprivautoclaim)
@
\end{sol}
\end{exercice}

%
%\begin{exercice}
%Télécharger le fichier \texttt{contents.csv} sur le site de cours. Il contient les pertes de biens, dues à un incendie, de plus d'un million de couronnes danoises provenant de réclamations faites à la compagnie de réassurance Copenhagen Re entre 1980 et 1990. Les données peuvent être importées en \textsf{R} comme suit, après avoir changé l'environnement de travail de \textsf{R} au dossier dans lequel vous avez enregistré le fichier \texttt{contents.csv}:
%
%<<>>=
%ct <- read.csv("contents.csv",header=TRUE,row.names=1)
%attach(ct)
%data <- Contents
%@
%
%Les données peuvent aussi être importées par \textsf{RStudio} en cliquant sur ``Import Dataset''.
%
%\begin{enumerate}
%\item En utilisant la fonction \texttt{fitdistr} de la librairie \texttt{MASS}, ajuster divers modèles paramétriques en utilisant le maximum de vraisemblance: loi exponentielle, Gamma, log-normale et normale.
%
%\item Pour chacun de ces quatre modèles, afficher le diagramme quantile-quantile et commenter sur la qualité de l'ajustement.
%
%\item Sélectionner le meilleur modèle ajusté selon le critère AIC.
%
%\item Basé sur les résultats de a)--c), êtes-vous satisfaits du modèle sélectionné? Pensez-vous qu'il y a place à amélioration? Justifier.
%\end{enumerate}
%
%\begin{sol}
%\begin{enumerate}
%
%\item Premièrement, on importe les données en \texttt{R}:
%<<>>=
%ct <- read.csv("contents.csv",header=TRUE,row.names=1)
%attach(ct)
%data <- Contents
%@
%Ensuite, les divers modèles peuvent être ajustés comme suit:
%<<>>=
%library(MASS)
%m1 <- fitdistr(data,densfun="exponential")
%m2 <- fitdistr(data,densfun="lognormal")
%m3 <- fitdistr(data,densfun="gamma",start=list(shape=0.5,rate=0.2))
%m4 <- fitdistr(data,densfun="normal")
%@
%
%Les paramètres estimés sont:
%\begin{center}
%\begin{tabular}{l|r}
%\hline
%Distribution ajustée & Paramètres \\
%\hline
%Exponentielle & $\hat \beta_n = \Sexpr{round(1/m1$estimate,3)}$\\
%\hline
%Log-normale & $\hat \mu_n = \Sexpr{round(m2$estimate[1],3)}$\\
%& $\hat \sigma^2_n = \Sexpr{round(m2$estimate[2],3)}$\\
%\hline
%Gamma & $\hat \alpha_n = \Sexpr{round(m3$estimate[1],3)}$\\
%&  $\hat \beta_n = \Sexpr{round(1/m3$estimate[2],3)}$\\
%\hline
%Normale & $\hat \mu_n =  \Sexpr{round(m4$estimate[1],3)}$\\
%&  $\hat \sigma^2_n = \Sexpr{round(m4$estimate[2],3)}$\\
%\hline
%\end{tabular}
%\end{center}
%
%\item Les diagrammes quantiles-quantiles peuvent être codés comme suit: 
%<<fig.height=6>>=
%par(mfrow=c(2,2))
%n <- length(data)
%q <- 1:n/(n+1)
%plot(qexp(q,rate=m1$estimate), sort(data),
%     xlab="Quantiles th\u{E9}orique de l'exponentielle",
%     ylab="Quantiles empiriques")
%lines(sort(data), sort(data))
%
%plot(qlnorm(q,meanlog=m2$estimate[1], sdlog=m2$estimate[2]), 
%     sort(data), xlab="Quantiles th\u{E9}orique de la log-normale", 
%     ylab="Quantiles empiriques")
%lines(sort(data), sort(data))
%
%plot(qgamma(q,shape=m3$estimate[1],rate=m3$estimate[2]), sort(data),
%     xlab="Quantiles th\u{E9}orique de la Gamma", 
%     ylab="Quantiles empiriques")
%lines(sort(data),sort(data))
%
%plot(qnorm(q,mean=m4$estimate[1],sd=m4$estimate[2]), sort(data),
%     xlab="Quantiles th\u{E9}orique de la normale",
%     ylab="Quantiles empiriques")
%lines(sort(data),sort(data))
%@
%
%Les diagrammes quantiles-quantiles montrent que le modèle normal ne s'ajuste pas bien du tout. Le modèle exponentiel est légèrement mieux, mais s'ajuste tout de même mal, spécifiquement dans les quantiles élevés. Les modèles Gamma et log-normal sont meilleurs et l'ajustement est similaire pour les deux. Cependant, pour les deux modèles, les quantiles très élevés ne sont pas bien capturés: les quantiles prédits par les modèles sont trop bas. Le modèle log-normal prédit les quantiles élevés légèrement mieux que le modèle Gamma.
%
%\item La sélection de modèle selon l'AIC peut se faire comme suit:
%<<>>=
%AIC(m1,m2,m3,m4)
%@
%Le modèle avec le plus petit AIC est \texttt{m2}, i.e., le modèle log-normal.
%
%\item Le modèle sélectionné, i.e., le modèle log-normal, ne modélise pas adéquatement les quantiles élevés. Cependant, ceux-ci sont particulièrement importants pour une compagnie d'assurance puisque que ce sont eux qui causent les plus grandes pertes et qui doivent être modélisés avec précision. Par conséquent, on devrait tenter de trouver un meilleur modèle.
%\end{enumerate}
%\end{sol}
%\end{exercice}

\Closesolutionfile{solutions}
\Closesolutionfile{reponses}


%\section*{Exercices proposés dans \cite{Wackerly:mathstat:7e:2008}}
%
%\begin{trivlist}
%\item 3.145--3.151, 3.153, 3.155, 3.158, 3.159, 3.161, 3.162, 3.163
%\end{trivlist}


%%%
%%% Insérer les réponses
%%%
\input{reponses-ajustement}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "exercices_analyse_statistique"
%%% coding: utf-8-unix
%%% End:
