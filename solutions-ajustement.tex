\section*{Chapitre \ref{chap:ajustement}}
\addcontentsline{toc}{section}{Chapitre \protect\ref{chap:ajustement}}

\begin{solution}{4.1}
    \begin{enumerate}
    \item On a une une distribution de Poisson de paramètre $\theta$,
      d'où $\esp{X} = \theta$. L'estimateur des moments est donc
      $\hat{\theta} = \bar{X}$.
    \item La densité est celle d'une distribution bêta de paramètres
      $\theta$ et $1$. Ainsi, $\esp{X} = \theta/(\theta + 1)$ et en posant
      \begin{equation*}
        \frac{\theta}{\theta + 1} = \bar{X}
      \end{equation*}
      on trouve que l'estimateur des moments de $\theta$ est
      \begin{equation*}
        \hat{\theta} = \frac{\bar{X}}{1 - \bar{X}}.
      \end{equation*}
    \item On reconnaît la densité d'une distribution Gamma de
      paramètres $1$ et $\theta$. Ainsi, on sait que $\esp{X} =
      \theta$, d'où l'estimateur des moments est $\hat{\theta} =
      \bar{X}$.
    \item Cette densité est celle de la loi de Laplace. On a
      \begin{align*}
        \esp{X} &= \frac{1}{2}
        \left(
          \int_{-\infty}^\theta x e^{x-\theta}\, dx +
          \int_{\theta}^\infty xe^{-x + \theta}\, dx
        \right) \\
        &= \frac{1}{2}(2\theta)\\
        &= \theta.
      \end{align*}
      L'estimateur des moments de $\theta$ est donc $\hat{\theta} =
      \bar{X}$.
    \item On a la densité d'une exponentielle de paramètre 1
      translatée de $\theta$ vers la droite. Par conséquent,
      $\esp{X} = \theta + 1$, un résultat facile à vérifier en
      intégrant. En posant $\theta + 1 = \bar{X}$, on trouve
      facilement que $\hat{\theta} = \bar{X} - 1$.
    \end{enumerate}
  
\end{solution}
\begin{solution}{4.2}
\begin{enumerate}
\item La fonction génératrice des moments de $Z\sim\mathcal{N}(\mu,\sigma^2)$ est
$$
M_{Z}(t)=\ex[e^{tZ}]=\exp(\mu t +\sigma^2 t^2/2).
$$
Ainsi,
\begin{align*}
\ex[Y]&= \ex[e^{Z}]=M_Z(1)=\exp(\mu +\sigma^2/2),\\
\ex[Y^2]&= \ex[e^{2Z}]=M_Z(2)=\exp(2\mu +\sigma^2 2^2/2).
\end{align*}

\item On doit résoudre les deux équations où $m_1=\bar Y_n$ est égal à $\ex[Y]$ et $m_2=\frac{1}{n}\sum_{i=1}^n Y_i^2$ est égal à $\ex[Y^2]$:
\begin{align}
\exp(\hat\mu +\hat\sigma^2/2)&=\bar Y_n \label{eq:ajust1}\\
\exp(2\hat\mu +2\hat\sigma^2 )&=\frac{1}{n}\sum_{i=1}^n Y_i^2.\label{eq:ajust2}
\end{align}
On note que
 \begin{align*}
 \exp(2\hat\mu +2\hat\sigma^2 )&=\left\{\exp(\hat\mu +\hat\sigma^2/2)\right\}^2\exp(\hat\sigma^2)\\
 &=\bar Y_n^2 \exp(\hat\sigma^2),\quad \mbox{ avec \eqref{eq:1}}.
 \end{align*}
 En remplaçant dans \eqref{eq:ajust2}, on obtient que
 $$
 \bar{Y}_n^2 \exp(\hat\sigma^2) = \frac{1}{n}\sum_{i=1}^n Y_i^2
 $$
 est équivalent à
 $$
 \hat\sigma^2 = \ln\left\{\frac{\sum_{i=1}^n Y_i^2/n}{\bar{Y}_n^2} \right\}.
 $$
 En utilisant \eqref{eq:ajust1}, $\hat\mu +\hat\sigma^2/2=\ln(\bar Y_n )$, donc
\begin{align*}
\hat\mu & = \ln(\bar Y_n )-\frac{1}{2}\ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\}\\
& = \ln(\bar Y_n )+\ln\left\{\frac{\bar Y_n}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\}\\
& = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\}.
\end{align*}
Ainsi, les estimateurs des moments sont
$$
\hat \mu = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\} \quad \mbox{ et }\quad \hat\sigma^2 = \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\}.
$$

\item Pour montrer la convergence, on note premièrement que, selon la Loi faible des grands nombres
\begin{align*}
\bar Y &\stackrel{P}{\rightarrow} \ex[Y]=e^{\mu+\sigma^2/2}\\
\frac{1}{n}\sum_{i=1}^n Y_i^2 &\stackrel{P}{\rightarrow} \ex[Y^2]=e^{2\mu+2\sigma^2}\\
\end{align*}
Puisque $\ln$, le carré et la racine carrée sont toutes des fonctions continues, on trouve que, quand $n\to\infty$,
\begin{align*}
\hat\mu = \ln\left\{\frac{\bar Y_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n Y_i^2}} \right\} &\stackrel{P}{\rightarrow} \ln\left\{\frac{ e^{2\mu+\sigma^2}}{\sqrt{e^{2\mu+2\sigma^2}}} \right\}=\ln\left\{ e^{2\mu+\sigma^2-\mu-\sigma^2} \right\}=\mu\\
\hat\sigma^2 = \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n Y_i^2}{\bar Y_n^2} \right\} &\stackrel{P}{\rightarrow} \ln\left\{\frac{e^{2\mu+2\sigma^2}}{e^{2\mu+\sigma^2}} \right\}=\ln\left\{e^{2\mu+2\sigma^2-2\mu-\sigma^2} \right\}=\sigma^2.
\end{align*}
Ainsi, les estimateurs sont convergents.
\end{enumerate}
\end{solution}
\begin{solution}{4.3}
    Pour obtenir l'estimateur des moments de $\theta$, on pose
    \begin{equation*}
      \esp{X} = \frac{1 - \theta}{\theta} = \bar{X},
    \end{equation*}
    d'où
    \begin{equation*}
      \hat{\theta} = \frac{1}{\bar{X} + 1}.
    \end{equation*}
    La moyenne de l'échantillon est $\bar{x} = 64/20 = 3,2$. On a donc
    \begin{equation*}
      \hat{\theta} = \frac{1}{4,2} = 0,2381.
    \end{equation*}
  
\end{solution}
\begin{solution}{4.4}
    \begin{enumerate}
    \item Il s'agit ici de trouver l'estimateur des moments du
      paramètre $\theta$ d'une distribution uniforme discrète sur $1,
      2, \dots, \theta$, c'est-à-dire que $\prob{X = x} = 1/\theta$
      pour $x = 1, \dots, \theta$. En posant
      \begin{align*}
        \esp{X} = \frac{\theta + 1}{2} = \bar{X},
      \end{align*}
      on trouve facilement que l'estimateur des moments de $\theta$
      est $\hat{\theta} = 2 \bar{X} - 1$.
    \item Avec $x_1 = x_2 = x_3 = 3$ et $x_4 = 12$, on a $\hat{\theta}
      = 2 (3 + 3 + 3 + 12)/4 - 1 = 9,5$. Or, cet estimateur est
      absurde puisque, en ayant roulé le résultat 12, on sait qu'il y a au
      moins douze faces sur le dé! En d'autres termes, $9,5$ est
      une valeur de $\theta$ impossible. On constate que l'estimateur
      obtenu à l'aide de la méthode des moments n'est pas toujours un
      estimateur possible.
    \end{enumerate}
  
\end{solution}
\begin{solution}{4.5}
Les équations à résoudre sont
\begin{align*}
0,2 &= F(18,25) = \Phi\left(\frac{\ln(18,25)-\hat{\mu}}{\hat{\sigma}}\right), \\
0,8 &= F(35,8) = \Phi\left(\frac{\ln(35,8)-\hat{\mu}}{\hat{\sigma}}\right).
\end{align*}
Les 20e et 80e quantiles de la loi normale standard sont -0,842 et 0,842, respectivement. Les équations deviennent
\begin{align*}
-0,842 &= \frac{2,904-\hat{\mu}}{\hat{\sigma}}, \\
0,842 &= \frac{3,578-\hat{\mu}}{\hat{\sigma}}.
\end{align*}
Diviser la première équation par la deuxième donne
$$
-1 = \frac{2,904-\hat{\mu}}{3,578-\hat{\mu}}.
$$
La solution est $\hat{\mu}=3,241$ et $\hat{\sigma}=0,4$. La probabilité d'excéder 30 est estimée par
\begin{align*}
\widehat{\Pr(X>30)} &= 1-F(30; \hat\mu,\hat\sigma) = 1-\Phi\left(\frac{\ln(30)-3,241}{0,4}\right) = 1-\Phi(0,4) \\
&= 1-0,6554 = 0,3446.
\end{align*}
\end{solution}
\begin{solution}{4.6}
Les équations à résoudre sont
\begin{align*}
0,2 &= F(100) = \frac{(100/\hat{\theta})^{\hat{\tau}}}{1+(100/\hat{\theta})^{\hat{\tau}}}, \\
0,8 &= F(400) = \frac{(400/\hat{\theta})^{\hat{\tau}}}{1+(400/\hat{\theta})^{\hat{\tau}}}.
\end{align*}
Avec la première équation, on obtient
$$
0,2=0,8(100/\hat{\theta})^{\hat{\tau}} \text{ ou encore } \hat{\theta}^{\hat{\tau}}=4(100)^{\hat{\tau}}.
$$
En insérant le résultat dans la deuxième équation, on obtient
$$
0,8=\frac{4^{\hat{\tau}-1}}{1+4^{\hat{\tau}-1}}.
$$
On résoud pour obtenir $\hat{\tau}=2$ et $\hat{\theta}=200$.
\end{solution}
\begin{solution}{4.7}
On a besoin de la $0,75(21)=15,75$e plus petite observation. On obtient $0,25(13) + 0,75(14)= 13,75$.
\end{solution}
\begin{solution}{4.8}
    Dans tous les cas, la fonction de vraisemblance est $L(\theta) =
    \prod_{i = 1}^n f(x_i; \theta)$ et la fonction de
    log-vraisemblance est $l(\theta) = \ln L(\theta) = \sum_{i = 1}^n
    \ln f(x_i; \theta)$. L'estimateur du maximum de vraisemblance du
    paramètre $\theta$ est la solution de l'équation $l^\prime(\theta)
    = 0$.
    \begin{enumerate}
    \item On a
      \begin{align*}
        L(\theta) &= \frac{e^{-n\theta} \theta^{\sum_{i=1}^n x_i}}{%
          \prod_{i=1}^n x_i!}, \\
        l(\theta) &= -n \theta  + \sum_{i = 1}^n x_i \ln(\theta)
        - \sum_{i=1}^n \ln(x_i!) \\
        \intertext{et}
        l^\prime(\theta) &= -n + \frac{\sum_{i = 1}^n x_i}{\theta}.
      \end{align*}
      En résolvant l'équation $l^\prime(\theta) = 0$ pour $\theta$, on
      trouve que l'estimateur du maximum de vraisemblance est
      $\hat{\theta} = \bar{X}$.
    \item On a
      \begin{align*}
        L(\theta) &= \theta^n
        \left(
          \prod_{i = 1}^n x_i
        \right)^{\theta - 1}, \\
        l(\theta) &= n \ln(\theta) +
        (\theta - 1) \sum_{i = 1}^n \ln(x_i) \\
        \intertext{et}
        l^\prime(\theta) &= \frac{n}{\theta} +
        \sum_{i = 1}^n \ln(x_i).
      \end{align*}
      On trouve donc que
      \begin{equation*}
      \hat{\theta} = -\frac{n}{\sum_{i=1}^n \ln(X_i)} =
      -\frac{n}{\ln(X_1 \cdots X_n)}.
      \end{equation*}
    \item  On a
      \begin{align*}
        L(\theta) &= \theta^{-n} e^{-\sum_{i=1}^n x_i/\theta}, \\
        l(\theta) &= -n \ln(\theta) - \frac{\sum_{i=1}^n x_i}{\theta} \\
        \intertext{et}
        l^\prime(\theta) &= -\frac{n}{\theta} +
        \frac{\sum_{i=1}^n x_i}{\theta^2}.
      \end{align*}
      On obtient que $\hat{\theta} = \bar{X}$.
    \item On a
      \begin{equation*}
        L(\theta) = \left( \frac{1}{2} \right)^n
        e^{-\sum_{i = 1}^n \abs{x_i - \theta}}
      \end{equation*}
      La présence de la valeur absolue rend cette fonction non
      différentiable en $\theta$. On remarque que la fonction de
      vraisemblance sera maximisée lorsque l'expression $\sum_{i=1}^n
      \abs{x_i - \theta}$ sera minimisée. On établit donc que $\hat{\theta} = \text{med}(X_1, \dots, X_n)$, puisqu'on connaît le résultat suivant sur la médiane.

      En général, si $X$ est une variable aléatoire continue et $a$ est une constante, on peut trouver le minimum de
    \begin{align*}
      \esp{\abs{X - a}}
      &= \int_{-\infty}^\infty \abs{x - a} f(x)\,dx \\
      &= \int_{-\infty}^a (a - x)f(x)\, dx
      + \int_a^\infty (x - a)f(x)\,dx.
    \end{align*}
    Or,
    \begin{align*}
      \frac{d}{da} \esp{\abs{X - a}}
      &= \int_{-\infty}^a f(x)\, dx + \int_a^\infty f(x)\, dx \\
      &= F_X(a) - (1 - F_X(a)) \\
      &= 2F_X(a) - 1
    \end{align*}
    Par conséquent, le minimum est atteint au point $a$ tel que
    $2F_X(a) - 1 = 0$, soit $F_X(a) = 1/2$. Par définition, cette
    valeur est la médiane de $X$.

    \item On remarque que le support de la densité dépend du paramètre
      $\theta$. La vraisemblance est
      \begin{equation*}
        L(\theta) = e^{n\theta - \sum_{i=1}^n x_i}\prod_{i=1}^n\mathbf{1}(x_i>\theta)
      \end{equation*}
      S'il y a une indicatrice qui est 0, alors la vraisemblance sera 0, elles doivent donc toutes être égales à 1 simultanément, ce qui est équivalent à
      \begin{equation*}
        L(\theta) = e^{n\theta - \sum_{i=1}^n x_i}\mathbf{1}(\min(x_1,\ldots,x_n)>\theta).
      \end{equation*}
      La fonction $e^{n\theta - \sum_{i=1}^n x_i}$ est strictement
      croissante en fonction de $\theta$, ce qui indique de choisir
      une valeur de $\theta$ la plus grande possible. Par contre, on a
      la contrainte $min(x_1,\ldots,x_n)>\theta$, c'est-à-dire que $\theta$ doit
      être plus inférieur ou égal à la plus petite valeur de
      l'échantillon. Par conséquent, $\hat{\theta} = \min(X_1, \dots,
      X_n) = X_{(1)}$.
    \end{enumerate}
  
\end{solution}
\begin{solution}{4.9}
    \begin{enumerate}
    \item On a $X = Z + \mu$ où $Z \sim
      \text{Exponentielle}(\lambda)$. Alors,
      \begin{align*}
        F_X(x) &= \prob{Z + \mu \leq x}\\
        &= F_Z(x - \mu) \\
        &= 1 - e^{-\lambda (x-\mu)}, \quad x > \mu \\
        \intertext{et}
        f_X(x) &= \lambda e^{-\lambda (x - \mu)}, \quad x > \mu.
      \end{align*}
    \item On a simplement
      \begin{align*}
        \Esp{X} &= \Esp{Z + \mu}\\
        &= \Esp{Z} + \mu\\
        &= \frac{1}{\lambda} + \mu\intertext{et}
        \Var{X} &= \Var{Z + \mu}\\
        &= \Var{Z}\\
        &= \frac{1}{\lambda^2}.
      \end{align*}
    \item On a
      \begin{align*}
        L(\mu,\lambda) &= \lambda^n e^{-\lambda\sum_{i=1}^n(x_i -\mu)}\prod_{i=1}^n\mathbf{1}(x_i\geq \mu)\\
        l(\mu,\lambda) &= n\ln(\lambda) -
        \lambda\sum_{i=1}^n (x_i - \mu) +\log(\mathbf{1}\{\min(x_1,\ldots,x_n)\geq \mu\}) \\
        \intertext{et}
        \frac{\partial l(\mu,\lambda)}{\partial \lambda} &=
        \frac{n}{\lambda} - \sum_{i=1}^n (x_i - \mu), \\
        \intertext{d'où}
        \lambda &= \frac{n}{\sum_{i=1}^n (x_i - \mu)}.
      \end{align*}
      On voit que la fonction $e^{-\lambda\sum_{i=1}^n(x_i -\mu)}$ est strictement
      croissante en fonction de $\mu$. Ainsi, il faut prendre la
      valeur de $\mu$ la plus grande possible telle que $\log(\mathbf{1}\{\min(x_1,\ldots,x_n)\geq \mu\})=0$. On a donc
      \begin{align*}
        \hat{\mu} &= X_{(1)} \\
        \hat{\lambda} &= \frac{n}{\sum_{i=1}^n (x_i - x_{(1)})}.
      \end{align*}
    \item On a, par exemple, les résultats suivants pour une
      exponentielle translatée de paramètres $\mu = \lambda^{-1} =
      \nombre{1000}$:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{x} \hlkwb{<-} \hlkwd{rexp}\hlstd{(}\hlnum{100}\hlstd{,} \hlkwc{rate} \hlstd{=} \hlnum{0.001}\hlstd{)} \hlopt{+} \hlnum{1000}
\hlkwd{min}\hlstd{(x)}
\end{alltt}
\begin{verbatim}
## [1] 1001.475
\end{verbatim}
\begin{alltt}
\hlnum{100} \hlopt{/} \hlkwd{sum}\hlstd{(x} \hlopt{-} \hlkwd{min}\hlstd{(x))}
\end{alltt}
\begin{verbatim}
## [1] 0.001032499
\end{verbatim}
\end{kframe}
\end{knitrout}
      Les estimations obtenues sont près des vraies valeurs des
      paramètres, même pour un relativement petit échantillon de
      taille 100.
    \end{enumerate}
  
\end{solution}
\begin{solution}{4.10}
    Il est clair ici que, comme $f(x;\theta) = 1$, on ne pourra pas
    utiliser la technique habituelle pour calculer l'estimateur du
    maximum de vraisemblance. Il faut d'abord déterminer l'ensemble
    des valeurs de $\theta$ possibles selon l'échantillon obtenu.
    Comme toutes les données de l'échantillon doivent se trouver dans
    l'intervalle $[\theta - 1/2, \theta + 1/2]$, on a $\theta \geq
    X_{(n)} - 1/2$ et $\theta \leq X_{(1)} + 1/2$. De plus, puisque
    $X_{(n)} - X_{(1)} \leq 1$, on a que $X_{(n)} - 1/2 \leq \theta
    \leq X_{(1)} + 1/2$. Ainsi, toute statistique satisfaisant ces
    inégalités est un estimateur du maximum de vraisemblance de
    $\theta$. On a donc que
    \begin{displaymath}
      X_{(n)} - \frac{1}{2} \leq T(X_1, \dots, T_n) \leq X_{(1)} + \frac{1}{2}.
    \end{displaymath}
  
\end{solution}
\begin{solution}{4.11}
    On a
    \begin{equation*}
      L(\mu ,\lambda) =
      \left(
        \frac{\lambda}{2\pi}
      \right)^{n/2}
      \left(
        \prod_{i = 1}^n \frac{1}{x_i^3}
      \right)^{1/2}
      \exp\left\{
        -\frac{\lambda}{2}
        \sum_{i=1}^n \frac{(x_i - \mu)^2}{\mu^2 x_i}
      \right\}.
    \end{equation*}
    Il est plus simple de trouver d'abord l'estimateur du maximum de
    vraisemblance du paramètre $\mu$. On constate qu'il s'agit de la
    valeur qui minimise la somme dans l'exponentielle. Or,
    \begin{equation*}
      \frac{\partial}{\partial \mu}\,
      \sum_{i=1}^n \frac{(x_i - \mu)^2}{\mu^2 x_i} =
      -\sum_{i=1}^n \frac{2}{\mu^2}
      \left( \frac{x_i}{\mu} - 1 \right).
    \end{equation*}
    En posant
    \begin{equation*}
      \sum_{i=1}^n \left( \frac{x_i}{\mu} - 1 \right) = 0,
    \end{equation*}
    on trouve que $\hat{\mu} = \bar{X}$. Pour trouver l'estimateur du
    maximum de vraisemblance de $\lambda$, on établit d'abord que
    \begin{displaymath}
      L(\hat{\mu}, \lambda) \propto \lambda^{n/2} e^{-\lambda H},
    \end{displaymath}
    où
    \begin{align*}
      H &= \sum_{i = 1}^n \frac{(x_i - \bar{x})^2}{2 \bar{x}^2 x_i} \\
      &= \frac{1}{2} \sum_{i = 1}^n
      \left(
        \frac{1}{x_i} - \frac{1}{\bar{x}}
      \right).
    \end{align*}
    On obtient donc
    \begin{align*}
      \hat{\lambda} &= \frac{n}{2H}\\
      &= \frac{n}{\sum_{i = 1}^n X_i^{-1} - \bar{X}^{-1}}.
    \end{align*}
  
\end{solution}
\begin{solution}{4.12}
\begin{enumerate}
\item On a
\begin{align*}
\ex[X_1]&=\int_0^\infty x \frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}}\d x.
\end{align*}
On pose $t=x+\theta$,
\begin{align*}
\ex[X_1] &= \int_\theta^\infty (t-\theta) \frac{\alpha\theta^\alpha}{t^{\alpha+1}}\mbox{d} t\\
&=\int_\theta^\infty \frac{\alpha\theta^\alpha}{t^{\alpha}}\d t-\int_\theta^\infty \frac{\alpha\theta^{\alpha+1}}{t^{\alpha+1}}\mbox{d} t\\
&= \left.\frac{-\alpha\theta^\alpha}{(\alpha-1)t^{\alpha-1}}\right|_\theta^\infty+ \left.\frac{\alpha\theta^{\alpha+1}}{\alpha t^{\alpha}}\right|_\theta^\infty, \quad \alpha>1\\
&=\frac{\alpha\theta}{\alpha-1}-\theta = \frac{\theta}{\alpha-1}.
\end{align*}

De même,
\begin{align*}
\ex[X_1^2]&=\int_0^\infty x^2 \frac{\alpha\theta^\alpha}{(x+\theta)^{\alpha+1}}\d x.
\end{align*}
On pose $t=x+\theta$,
\begin{align*}
\ex[X_1^2]&=\int_\theta^\infty (t-\theta)^2 \frac{\alpha\theta^\alpha}{t^{\alpha+1}}\d t
\end{align*}
En intégrant par parties,
\begin{align*}
\ex[X_1^2]&= \left.\frac{-(t-\theta)^2\theta^\alpha}{t^{\alpha}}\right|_\theta^\infty+ \int_\theta^\infty 2(t-\theta)\frac{\theta^{\alpha}}{ t^{\alpha}}\d t\\
&=\left.\frac{-2(t-\theta)\theta^\alpha}{(\alpha-1)t^{\alpha-1}}\right|_\theta^\infty+ \int_\theta^\infty 2\frac{\theta^{\alpha}}{ (\alpha-1)t^{\alpha-1}}\d t\\
&=\left.\frac{-2\theta^\alpha}{(\alpha-1)(\alpha-2)t^{\alpha-2}}\right|_\theta^\infty, \quad \alpha>2\\
&=\frac{2\theta^\alpha}{(\alpha-1)(\alpha-2)\theta^{\alpha-2}}= \frac{2\theta^2}{(\alpha-1)(\alpha-2)}.
\end{align*}
Ainsi
$$
\vr(X_1)=\frac{2\theta^2}{(\alpha-1)(\alpha-2)}-\frac{\theta^2}{(\alpha-1)^2}=\frac{\alpha\theta^2}{(\alpha-1)^2(\alpha-2)}.
$$

\item On a $\bar X_n=\frac{1}{n}\sum_{i=1}^nX_i$ pour la moyenne échantillonnale et $S_n^2=\frac{1}{n-1}\sum_{i=1}^n(X_i-\bar X_n)^2$ pour la variance échantillonnale. Les estimateurs des moments sont tels que
\begin{align}
\bar X_n &= \frac{\hat\theta}{\hat\alpha-1} \label{eq:ajust3}\\
S_n^2 &= \frac{\hat\alpha\hat\theta^2}{(\hat\alpha-1)^2(\hat\alpha-2)}\label{eq:ajust4}
\end{align}
En utilisant \eqref{eq:ajust3}, on a $\hat\theta=\bar X_n (\hat\alpha -1)$. Aussi, on note que
\begin{align*}
S_n^2= \frac{\hat\alpha\hat\theta^2}{(\hat\alpha-1)^2(\hat\alpha-2)}&= \left(\frac{\hat\theta}{\hat\alpha-1}\right)^2\frac{\hat\alpha}{\hat\alpha-2}\\
& = \bar X_n^2\frac{\hat\alpha}{\hat\alpha-2}, \quad \mbox{ avec \eqref{eq:ajust3}}.
\end{align*}
On réarrange pour trouver
$$
\hat\alpha = \frac{2S_n^2}{S_n^2-\bar X_n^2}
$$
et
$$
\hat\theta=\bar X_n \left(\frac{2S_n^2}{S_n^2-\bar X_n^2} -1\right)=\bar X_n \frac{S_n^2+\bar X_n^2}{S_n^2-\bar X_n^2}.
$$

\item La vraisemblance est
$$
\mathcal{L}(\alpha,\theta)=\prod_{i=1}^n\frac{\alpha\theta^\alpha}{(x_i+\theta)^{\alpha+1}}=\frac{\alpha^n\theta^{n\alpha}}{\prod_{i=1}^n(x_i+\theta)^{\alpha+1}}
$$
et la log-vraisemblance est
\begin{align*}
\ell(\alpha,\theta)&=n\ln \alpha+n\alpha\ln \theta-\ln\left\{\prod_{i=1}^n(x_i+\theta)^{\alpha+1}\right\}\\
&=n\ln \alpha+n\alpha\ln \theta-(\alpha+1)\sum_{i=1}^n\ln(x_i+\theta).
\end{align*}
Les dérivées partielles sont
\begin{align*}
\frac{\partial\ell(\alpha,\theta)}{\partial \alpha}&=\frac{n}{ \alpha}+n\ln \theta-\sum_{i=1}^n\ln(x_i+\theta)\\
\frac{\partial\ell(\alpha,\theta)}{\partial \theta}&=\frac{n\alpha}{ \theta}-(\alpha+1)\sum_{i=1}^n\frac{1}{x_i+\theta}.
\end{align*}
Les estimateurs du maximum de vraisemblance sont tels que
\begin{align*}
\frac{n}{\hat\alpha}+n\ln \hat\theta-\sum_{i=1}^n\ln(x_i+\hat\theta)&=0\\
\frac{n\hat\alpha}{ \hat\theta}-(\hat\alpha+1)\sum_{i=1}^n\frac{1}{x_i+\hat\theta}&=0.
\end{align*}
\end{enumerate}
\end{solution}
\begin{solution}{4.13}
    La fonction de vraisemblance est
    \begin{equation*}
      L(a, b) = \left( \frac{1}{b - a} \right)^n\mathbf{1}(a < x_1, \dots, x_n < b).
    \end{equation*}
    Pour maximiser cette fonction, il
    faut minimiser la quantité $b - a$ en choisissant une valeur de
    $b$ la plus petite possible et une valeur de $a$ la plus grande
    possible. Étant donné le support de la distribution, on choisit
    donc $\hat{a} = \min(X_1, \dots, X_n)$ et $\hat{b} = \max(X_1,
    \dots, X_n)$.
  
\end{solution}
\begin{solution}{4.14}
\begin{enumerate}
\item La fonction de vraisemblance de $\theta$ pour $x_1,\dots, x_n$ est
\begin{align*}
L(\theta) &= \left(\frac{1}{2 \theta + 1}\right)^n \boldsymbol{1}(x_1 \le 2 \theta + 1) \times \dots \times\boldsymbol{1}(x_n \le 2 \theta + 1)\\
& = \left(\frac{1}{2 \theta + 1}\right)^n \boldsymbol{1}\{\max(x_1,\dots,x_n) \le 2 \theta + 1\}\\
& = \left(\frac{1}{2 \theta + 1}\right)^n \boldsymbol{1}\left\{  \theta \ge \frac{\max(x_1,\dots,x_n) -1}{2}\right\}.
\end{align*}
Puisque $\{1/(2\theta+1)\}^n$ est décroissante en $\theta$, $L(\theta)$ est maximisée à
$$
\hat \theta_n = \frac{\max(x_1,\dots,x_n) -1}{2}
$$
L'EMV de $\theta$ est donc
$$
\hat \theta_n = \frac{\max(X_1,\dots,X_n) -1}{2}.
$$

\item La variance peut être calculée comme suit~:
$$
\ex(X) = \int_0^{2\theta  +1} x \, \frac{1}{2\theta + 1} \, \mbox{d} x = \frac{2 \theta  +1}{2}
$$
et
$$
\ex(X^2) =  \int_0^{2\theta  +1} x^2 \, \frac{1}{2\theta + 1} \, \mbox{d}x = \frac{(2\theta + 1)^2}{3} \, .
$$
Par conséquent,
$$
\vr(X) = \ex(X^2) - \{\ex(X)\}^2 = \frac{(2\theta + 1)^2}{3}- \frac{(2\theta + 1)^2}{4} = \frac{(2\theta + 1)^2}{12} \, .
$$
Puisque l'application $g : (0,\infty) \to (0,\infty)$ donnée par
$$
g(x) = \frac{(2x + 1)^2}{12}
$$
est strictement croissante (et donc un-pour-un), la propriété d'invariance de l'EMV donne que l'EMV de $\vr(X)$ est
\begin{align*}
\frac{(2\hat\theta_n + 1)^2}{12} = \frac{\{\max(X_1,\dots,X_n)\}^2}{12} \, .
\end{align*}

\end{enumerate}
\end{solution}
\begin{solution}{4.15}
    \begin{enumerate}
    \item La distribution de $X$ est une Bernoulli avec une
      restriction sur la valeur du paramètre $\theta$. On a donc que
      $\esp{X} = \theta$. L'estimateur des moments de $\theta$ est
      donc $\tilde{\theta} = \bar{X}$.

      Pour l'estimateur du maximum de vraisemblance, on a, en posant
      $y = \sum_{i = 1}^n x_i$,
      \begin{align*}
        L(\theta) &= \theta^{y} (1 - \theta)^{n - y}, \\
        l(\theta) &= y \ln(\theta) + (n - y) \ln(1 - \theta) \\
        \intertext{et}
        l^\prime(\theta) &= \frac{y}{\theta} - \frac{(n - y)}{1 - \theta}\\
        &= \frac{y - n\theta}{\theta (1 - \theta)}\\
        &= \frac{n \bar{x} - n \theta}{\theta (1 - \theta)}.
      \end{align*}
      Ainsi, la log-vraisemblance est croissante pour $\theta \leq
      \bar{x}$ et décroissante pour $\theta > \bar{x}$ (voir la
      figure~\ref{fig:ponctuelle:uniforme}). Le maximum est donc
      atteint en $\bar{x}$. Cependant, puisque $0 \leq \theta \leq
      1/2$ on doit avoir $\hat{\theta} \leq 1/2$. On a donc
      $\hat{\theta} = \min(\bar{X}, 1/2)$.
      \begin{figure}
        \centering
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=0.6\textwidth]{figure/unnamed-chunk-23-1}

\end{knitrout}
        \caption{Graphique de la fonction de log-vraisemblance de
          l'exercice
          \ref{chap:ajustement}.\ref{ex:ponctuelle:uniforme} pour $n =
          10$ et $y = 3$.}
        \label{fig:ponctuelle:uniforme}
      \end{figure}
    \item Premièrement, on remarque que $Y = \sum_{i = 1}^n X_i = n
      \bar{X} \sim \text{Binomiale}(n, \theta)$ avec $0 \leq \theta
      \leq 1/2$. Deuxièmement, on sait que $\mbox{EQM}(\hat{\theta}) =
      \var{\hat{\theta}} + b(\hat{\theta})^2$, où $\hat{\theta}$ est
      un estimateur quelconque d'un paramètre $\theta$ et
      $b(\hat{\theta}) = \esp{\hat{\theta}} - \theta$ est le biais de
      l'estimateur.

      Pour l'estimateur des moments $\tilde{\theta} = Y/n$, on a
      \begin{align*}
        \mbox{EQM}(\tilde{\theta}) &= \frac{\Var{Y}}{n^2} +
        b(Y/n)^2 \\
        &= \frac{\theta (1 - \theta)}{n},
      \end{align*}
      puique $\esp{Y/n} = \theta$.

      Pour l'estimateur du maximum de vraisemblance
      \begin{equation*}
        \hat{\theta} =
        \begin{cases}
          \frac{Y}{n}, & Y \leq \frac{n}{2} \\
          \frac{1}{2}, & Y > \frac{n}{2},
        \end{cases}
      \end{equation*}
      il est plus simple de développer l'erreur quadratique moyenne
      ainsi:
      \begin{align*}
        \mbox{EQM}(\hat{\theta}) &= \esp{(\hat{\theta} - \theta)^2} \\
        &= \sum_{y = 0}^n (\hat{\theta} - \theta)^2 \prob{Y = y} \\
        &= \sum_{y = 0}^{[n/2]}
        \left(
          \frac{y}{n} - \theta
        \right)^2 \binom{n}{y} \theta^y (1 - \theta)^{n - y} \\
        &\phantom{=} +
        \sum_{y = [n/2] + 1}^n
        \left(
          \frac{1}{2} - \theta
        \right)^2 \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
      \end{align*}
    \item On compare les erreurs quadratiques moyennes des deux
      estimateurs. Soit
      \begin{displaymath}
        \mbox{EQM}(\tilde{\theta}) = \sum_{y = 0}^n
        \left(
          \frac{y}{n} - \theta
        \right)^2 \binom{n}{y} \theta^y (1 - \theta)^{n - y},
      \end{displaymath}
      d'où
      \begin{align*}
        \mbox{EQM}(\tilde{\theta}) - \mbox{EQM}(\hat{\theta}) &=
        \sum_{y=[n/2] + 1}^n
        \left(
          \frac{y}{n} + \frac{1}{2} - 2 \theta
        \right)
        \left(
          \frac{y}{n} - \frac{1}{2}
        \right) \\
        &\phantom{=} \times
        \binom{n}{y} \theta^y (1 - \theta)^{n - y}.
      \end{align*}
      Étant donné que $y/n > 1/2$ et que $\theta \leq 1/2$, tous les
      termes dans la somme sont positifs. On a donc que
      $\mbox{EQM}(\tilde{\theta}) - \mbox{EQM}(\hat{\theta}) > 0$ ou, de manière
      équivalente, $\mbox{EQM}(\hat{\theta}) < \mbox{EQM}(\tilde{\theta})$. En terme
      d'erreur quadratique moyenne, l'estimateur du maximum de
      vraisemblance est meilleur que l'estimateur des moments selon ce critère.
    \end{enumerate}
  
\end{solution}
\begin{solution}{4.16}
\begin{enumerate}

\item La loi normale est définie sur les nombres réels, alors que les montants sont positifs seulement. Le domaine d'une loi normale n'est donc pas le même que le domaine des montants de réclamation. La moyenne échantillonnale et la médiane ne sont pas égales, ce qui serait le cas si le modèle normal était approprié. L'histogramme des montants ne ressemblent pas à une cloche symétrique. De bonnes options seraient les distributions Gamma, log-normale ou Pareto puisqu'elles sont toutes définies sur les réels positifs et sont asymétriques.

\item On a $\bar x_n=1~853$ et $m_2=\frac{1}{n}\sum_{i=1}^n x_i^2 = 10~438~832$. Selon l'exercice~\ref{chap:ajustement}.\ref{ex:moment:distln}~b), on a
\begin{align*}
\hat \mu &= \ln\left\{\frac{\bar x_n^2}{\sqrt{\frac{1}{n}\sum_{i=1}^n x_i^2}} \right\}=\ln\left\{\frac{1~853^2}{\sqrt{10~438~832}} \right\}=6.969\\
\hat\sigma^2 &= \ln\left\{\frac{\frac{1}{n}\sum_{i=1}^n x_i^2}{\bar x_n^2} \right\}=  \ln\left\{\frac{10~438~832}{1~853^2} \right\}=1.112.
\end{align*}


\item On a $\bar x_n=1~853$ et $m_2=\frac{1}{n}\sum_{i=1}^n x_i^2 = 10~438~832$, donc
\begin{align*}
s_n^2&=\frac{1}{n-1}\sum_{i=1}^n x_i^2 - \frac{n}{n-1}\bar x_n^2\\
& = \frac{n}{n-1}(m_2-\bar x_n^2)\\
&=\frac{6~773}{6~772}(10~438~832-1~853^2)=7~006~257.
\end{align*}
Selon l'exercice~\ref{chap:ajustement}.\ref{ex:moment:pareto}~b), on trouve
\begin{align*}
\hat\alpha &= \frac{2s_n^2}{s_n^2-\bar x_n^2}=3.922\\
\hat\theta&=\bar x_n \left(\frac{2s_n^2}{s_n^2-\bar x_n^2} -1\right)=5414.77.
\end{align*}


\item L'AIC est donné par $2(-\ell(\hat\theta)+k)$, où $\ell(\hat\theta)$ est la valeur maximale de la vraisemblance et $k$ le nombre de paramètres dans le modèle. Il y a deux paramètres dans chaque modèle. Les AIC sont donc
\begin{description}
\item[Log-normale]: $2\times(57~185,11+2)=114~374,2$.
\item[Pareto]: $2\times(57~500,12+2)=115~004,2$
\end{description}
Selon le critère AIC, le meilleur modèle est celui avec la valeur la plus petite, la distribution log-normale est donc préférable.

\item L'ajustement des deux modèles n'est pas parfait parce que les points ne sont pas exactement alignés, plus spécialement pour de grandes valeurs de $x$. Les points sont au-dessus de la ligne dans le graphique Quantiles-Quantiles de la Pareto, ce qui signifie que la queue de la distribution empirique est plus épaisse que celle de la Pareto. Cela devrait inquiéter l'assureur puisque ça signifie qu'il sous-estimerait les réclamations. L'ajustement de la log-normale n'est pas parfait non plus, mais il y a moins de sous-estimation, car plusieurs points se retrouvent sous la ligne et les extrêmes sont moins éloignés. Par conséquent, on recommande l'utilisation du modèle log-normal.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-27-1}

\end{knitrout}

\item Les quantiles de la loi Pareto sont l'inverse de sa fonction de répartition. Si $\omega_\kappa$ est le $(1-\kappa)$e quantile, c'est-à-dire qu'il est tel que $F(\omega_\kappa)=1-\kappa$, alors en utilisant la fonction de répartition fournie à l'exercice~\ref{chap:ajustement}.\ref{ex:moment:pareto}, on a
\begin{align*}
1-\left(\frac{\theta}{\omega_\kappa+\theta}\right)^\alpha &= 1-\kappa\\
\frac{\theta}{\omega_\kappa+\theta} &= \kappa^{1/\alpha}\\
\omega_\kappa= \frac{\theta}{\kappa^{1/\alpha}}-\theta.
\end{align*}

Une estimation du quantile 99~\% est
$$
\hat\omega_{1\%}= \frac{\hat\theta}{0,01^{1/\hat\alpha}}-\hat\theta
$$
et en utilisant l'EMV donné en (d), on a
$$
\hat\omega_{1\%}= \frac{6~819,891}{0,01^{1/ 4,71364}}-6~819,891=11~296,76
$$
et une estimation du quantile 99,5\% est
$$
\hat\omega_{0,5\%}= \frac{6~819,891}{0,005^{1/4,71364}}-6~819,891= 14~166,68
$$

Si $\nu_\kappa$ représente le $(1-\kappa)$e quantile de la loi log-normale,
$$
F(\nu_\kappa)=1-\kappa \Rightarrow \Phi\left(\frac{\ln\nu_\kappa-\mu}{\sigma}\right)=1-\kappa,
$$
où $\Phi$ est la fonction de répartition de $\mathcal{N}(0,1)$. Cela signifique que le $(1-\kappa)$e quantile de la loi normale centrée réduite est utilisé pour trouver
$$
\frac{\ln\nu_\kappa-\mu}{\sigma}=z_\kappa \Rightarrow \nu_\kappa = \exp(\sigma z_\kappa+\mu).
$$
Une estimation du quantile 99~\% est
$$
\hat\nu_{1\%}= \exp(\hat\sigma z_{1\%}+\hat\mu),
$$
où $z_{1\%}=2,33$. En utilisant l'EMV donné en d), on a
$$
\hat\nu_{1\%}= \exp(\sqrt{1,14698}\times 2,33+6,95561)=12~720,54
$$
et une estimation du quantile 99,5\% est
$$
\hat\nu_{0,5\%}= \exp(\sqrt{1,14698}\times 2,575+6,95561)=16~537,1
$$
Tel attendu en regardant les diagrammes quantile-quantile, les quantiles estimés par la distribution log-normale sont supérieurs à ceux estimés par la distribution Pareto.
\end{enumerate}


\end{solution}
